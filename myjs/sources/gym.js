var sources = {
    "keys": [
        "./wrappers/test_rescale_action.py",
        "./wrappers/resize_observation.py",
        "./wrappers/gray_scale_observation.py",
        "./wrappers/test_clip_action.py",
        "./wrappers/test_pixel_observation.py",
        "./wrappers/flatten_observation.py",
        "./wrappers/monitor.py",
        "./wrappers/filter_observation.py",
        "./wrappers/test_transform_reward.py",
        "./wrappers/clip_action.py",
        "./wrappers/transform_observation.py",
        "./wrappers/tests/__init__.py",
        "./wrappers/__init__.py",
        "./wrappers/test_record_episode_statistics.py",
        "./wrappers/test_flatten_observation.py",
        "./wrappers/test_frame_stack.py",
        "./wrappers/test_transform_observation.py",
        "./wrappers/time_limit.py",
        "./wrappers/transform_reward.py",
        "./wrappers/test_gray_scale_observation.py",
        "./wrappers/pixel_observation.py",
        "./wrappers/atari_preprocessing.py",
        "./wrappers/frame_stack.py",
        "./wrappers/test_resize_observation.py",
        "./wrappers/rescale_action.py",
        "./wrappers/monitoring/tests/test_video_recorder.py",
        "./wrappers/monitoring/tests/__init__.py",
        "./wrappers/monitoring/tests/helpers.py",
        "./wrappers/monitoring/__init__.py",
        "./wrappers/monitoring/video_recorder.py",
        "./wrappers/monitoring/stats_recorder.py",
        "./wrappers/record_episode_statistics.py",
        "./wrappers/test_filter_observation.py",
        "./wrappers/test_atari_preprocessing.py",
        "./version.py",
        "./error.py",
        "./spaces/tuple.py",
        "./spaces/box.py",
        "./spaces/multi_discrete.py",
        "./spaces/tests/test_utils.py",
        "./spaces/tests/__init__.py",
        "./spaces/tests/test_spaces.py",
        "./spaces/discrete.py",
        "./spaces/__init__.py",
        "./spaces/utils.py",
        "./spaces/dict.py",
        "./spaces/space.py",
        "./spaces/multi_binary.py",
        "./__init__.py",
        "./core.py",
        "./utils/seeding.py",
        "./utils/__init__.py",
        "./utils/play.py",
        "./utils/ezpickle.py",
        "./utils/closer.py",
        "./utils/colorize.py",
        "./utils/json_utils.py",
        "./utils/atomic_write.py",
        "./logger.py",
        "./envs/registration.py",
        "./envs/toy_text/cliffwalking.py",
        "./envs/toy_text/taxi.py",
        "./envs/toy_text/nchain.py",
        "./envs/toy_text/frozen_lake.py",
        "./envs/toy_text/guessing_game.py",
        "./envs/toy_text/discrete.py",
        "./envs/toy_text/__init__.py",
        "./envs/toy_text/hotter_colder.py",
        "./envs/toy_text/blackjack.py",
        "./envs/toy_text/roulette.py",
        "./envs/toy_text/kellycoinflip.py",
        "./envs/unittest/__init__.py",
        "./envs/unittest/memorize_digits.py",
        "./envs/unittest/cube_crash.py",
        "./envs/atari/atari_env.py",
        "./envs/atari/__init__.py",
        "./envs/robotics/hand/reach.py",
        "./envs/robotics/hand/__init__.py",
        "./envs/robotics/hand/manipulate.py",
        "./envs/robotics/hand/manipulate_touch_sensors.py",
        "./envs/robotics/hand_env.py",
        "./envs/robotics/__init__.py",
        "./envs/robotics/rotations.py",
        "./envs/robotics/fetch/reach.py",
        "./envs/robotics/fetch/slide.py",
        "./envs/robotics/fetch/__init__.py",
        "./envs/robotics/fetch/pick_and_place.py",
        "./envs/robotics/fetch/push.py",
        "./envs/robotics/utils.py",
        "./envs/robotics/fetch_env.py",
        "./envs/robotics/robot_env.py",
        "./envs/tests/test_determinism.py",
        "./envs/tests/test_envs_semantics.py",
        "./envs/tests/test_envs.py",
        "./envs/tests/test_mujoco_v2_to_v3_conversion.py",
        "./envs/tests/__init__.py",
        "./envs/tests/test_kellycoinflip.py",
        "./envs/tests/spec_list.py",
        "./envs/tests/test_registration.py",
        "./envs/tests/test_frozenlake_dfs.py",
        "./envs/mujoco/pusher.py",
        "./envs/mujoco/humanoid_v3.py",
        "./envs/mujoco/humanoid.py",
        "./envs/mujoco/swimmer_v3.py",
        "./envs/mujoco/humanoidstandup.py",
        "./envs/mujoco/reacher.py",
        "./envs/mujoco/ant_v3.py",
        "./envs/mujoco/striker.py",
        "./envs/mujoco/ant.py",
        "./envs/mujoco/__init__.py",
        "./envs/mujoco/half_cheetah.py",
        "./envs/mujoco/mujoco_env.py",
        "./envs/mujoco/hopper.py",
        "./envs/mujoco/thrower.py",
        "./envs/mujoco/hopper_v3.py",
        "./envs/mujoco/inverted_pendulum.py",
        "./envs/mujoco/walker2d_v3.py",
        "./envs/mujoco/walker2d.py",
        "./envs/mujoco/half_cheetah_v3.py",
        "./envs/mujoco/inverted_double_pendulum.py",
        "./envs/mujoco/swimmer.py",
        "./envs/__init__.py",
        "./envs/classic_control/continuous_mountain_car.py",
        "./envs/classic_control/rendering.py",
        "./envs/classic_control/__init__.py",
        "./envs/classic_control/cartpole.py",
        "./envs/classic_control/mountain_car.py",
        "./envs/classic_control/acrobot.py",
        "./envs/classic_control/pendulum.py",
        "./envs/algorithmic/copy_.py",
        "./envs/algorithmic/reversed_addition.py",
        "./envs/algorithmic/duplicated_input.py",
        "./envs/algorithmic/reverse.py",
        "./envs/algorithmic/tests/__init__.py",
        "./envs/algorithmic/tests/test_algorithmic.py",
        "./envs/algorithmic/__init__.py",
        "./envs/algorithmic/algorithmic_env.py",
        "./envs/algorithmic/repeat_copy.py",
        "./envs/box2d/test_lunar_lander.py",
        "./envs/box2d/__init__.py",
        "./envs/box2d/car_racing.py",
        "./envs/box2d/bipedal_walker.py",
        "./envs/box2d/lunar_lander.py",
        "./envs/box2d/car_dynamics.py",
        "./vector/sync_vector_env.py",
        "./vector/vector_env.py",
        "./vector/tests/test_numpy_utils.py",
        "./vector/tests/test_shared_memory.py",
        "./vector/tests/test_async_vector_env.py",
        "./vector/tests/__init__.py",
        "./vector/tests/test_sync_vector_env.py",
        "./vector/tests/utils.py",
        "./vector/tests/test_vector_env.py",
        "./vector/tests/test_spaces.py",
        "./vector/__init__.py",
        "./vector/utils/misc.py",
        "./vector/utils/shared_memory.py",
        "./vector/utils/numpy_utils.py",
        "./vector/utils/__init__.py",
        "./vector/utils/spaces.py",
        "./vector/async_vector_env.py"
    ],
    "values": [
        "import pytest\n\nimport numpy as np\n\nimport gym\nfrom gym.wrappers import RescaleAction\n\n\ndef test_rescale_action():\n    env = gym.make('CartPole-v1')\n    with pytest.raises(AssertionError):\n        env = RescaleAction(env, -1, 1)\n    del env\n\n    env = gym.make('Pendulum-v0')\n    wrapped_env = RescaleAction(gym.make('Pendulum-v0'), -1, 1)\n\n    seed = 0\n    env.seed(seed)\n    wrapped_env.seed(seed)\n\n    obs = env.reset()\n    wrapped_obs = wrapped_env.reset()\n    assert np.allclose(obs, wrapped_obs)\n\n    obs, reward, _, _ = env.step([1.5])\n    with pytest.raises(AssertionError):\n        wrapped_env.step([1.5])\n    wrapped_obs, wrapped_reward, _, _ = wrapped_env.step([0.75])\n\n    assert np.allclose(obs, wrapped_obs)\n    assert np.allclose(reward, wrapped_reward)",
        "import numpy as np\n\nfrom gym.spaces import Box\nfrom gym import ObservationWrapper\n\n\nclass ResizeObservation(ObservationWrapper):\n    r\"\"\"Downsample the image observation to a square image. \"\"\"\n    def __init__(self, env, shape):\n        super(ResizeObservation, self).__init__(env)\n        if isinstance(shape, int):\n            shape = (shape, shape)\n        assert all(x > 0 for x in shape), shape\n        self.shape = tuple(shape)\n\n        obs_shape = self.shape + self.observation_space.shape[2:]\n        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n\n    def observation(self, observation):\n        import cv2\n        observation = cv2.resize(observation, self.shape[::-1], interpolation=cv2.INTER_AREA)\n        if observation.ndim == 2:\n            observation = np.expand_dims(observation, -1)\n        return observation",
        "import numpy as np\n\nfrom gym.spaces import Box\nfrom gym import ObservationWrapper\n\n\nclass GrayScaleObservation(ObservationWrapper):\n    r\"\"\"Convert the image observation from RGB to gray scale. \"\"\"\n    def __init__(self, env, keep_dim=False):\n        super(GrayScaleObservation, self).__init__(env)\n        self.keep_dim = keep_dim\n\n        assert len(env.observation_space.shape) == 3 and env.observation_space.shape[-1] == 3\n        obs_shape = self.observation_space.shape[:2]\n        if self.keep_dim:\n            self.observation_space = Box(low=0, high=255, shape=(obs_shape[0], obs_shape[1], 1), dtype=np.uint8)\n        else:\n            self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n\n    def observation(self, observation):\n        import cv2\n        observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n        if self.keep_dim:\n            observation = np.expand_dims(observation, -1)\n        return observation",
        "import numpy as np\n\nimport gym\nfrom gym.wrappers import ClipAction\n\n\ndef test_clip_action():\n    # mountaincar: action-based rewards\n    make_env = lambda: gym.make('MountainCarContinuous-v0')\n    env = make_env()\n    wrapped_env = ClipAction(make_env())\n\n    seed = 0\n    env.seed(seed)\n    wrapped_env.seed(seed)\n\n    env.reset()\n    wrapped_env.reset()\n\n    actions = [[.4], [1.2], [-0.3], [0.0], [-2.5]]\n    for action in actions:\n        obs1, r1, d1, _ = env.step(np.clip(action, env.action_space.low, env.action_space.high))\n        obs2, r2, d2, _ = wrapped_env.step(action)\n        assert np.allclose(r1, r2)\n        assert np.allclose(obs1, obs2)\n        assert d1 == d2",
        "\"\"\"Tests for the pixel observation wrapper.\"\"\"\n\n\nimport pytest\nimport numpy as np\n\nimport gym\nfrom gym import spaces\nfrom gym.wrappers.pixel_observation import PixelObservationWrapper, STATE_KEY\n\n\nclass FakeEnvironment(gym.Env):\n    def __init__(self):\n        self.action_space = spaces.Box(\n            shape=(1, ), low=-1, high=1, dtype=np.float32)\n\n    def render(self, width=32, height=32, *args, **kwargs):\n        del args\n        del kwargs\n        image_shape = (height, width, 3)\n        return np.zeros(image_shape, dtype=np.uint8)\n\n    def reset(self):\n        observation = self.observation_space.sample()\n        return observation\n\n    def step(self, action):\n        del action\n        observation = self.observation_space.sample()\n        reward, terminal, info = 0.0, False, {}\n        return observation, reward, terminal, info\n\n\nclass FakeArrayObservationEnvironment(FakeEnvironment):\n    def __init__(self, *args, **kwargs):\n        self.observation_space = spaces.Box(\n            shape=(2, ), low=-1, high=1, dtype=np.float32)\n        super(FakeArrayObservationEnvironment, self).__init__(*args, **kwargs)\n\n\nclass FakeDictObservationEnvironment(FakeEnvironment):\n    def __init__(self, *args, **kwargs):\n        self.observation_space = spaces.Dict({\n            'state': spaces.Box(shape=(2, ), low=-1, high=1, dtype=np.float32),\n        })\n        super(FakeDictObservationEnvironment, self).__init__(*args, **kwargs)\n\n\nclass TestPixelObservationWrapper(object):\n    @pytest.mark.parametrize(\"pixels_only\", (True, False))\n    def test_dict_observation(self, pixels_only):\n        pixel_key = 'rgb'\n\n        env = FakeDictObservationEnvironment()\n\n        # Make sure we are testing the right environment for the test.\n        observation_space = env.observation_space\n        assert isinstance(observation_space, spaces.Dict)\n\n        width, height = (320, 240)\n\n        # The wrapper should only add one observation.\n        wrapped_env = PixelObservationWrapper(\n            env,\n            pixel_keys=(pixel_key, ),\n            pixels_only=pixels_only,\n            render_kwargs={pixel_key: {'width': width, 'height': height}})\n\n        assert isinstance(wrapped_env.observation_space, spaces.Dict)\n\n        if pixels_only:\n            assert len(wrapped_env.observation_space.spaces) == 1\n            assert (list(wrapped_env.observation_space.spaces.keys())\n                    == [pixel_key])\n        else:\n            assert (len(wrapped_env.observation_space.spaces)\n                    == len(observation_space.spaces) + 1)\n            expected_keys = list(observation_space.spaces.keys()) + [pixel_key]\n            assert (list(wrapped_env.observation_space.spaces.keys())\n                    == expected_keys)\n\n        # Check that the added space item is consistent with the added observation.\n        observation = wrapped_env.reset()\n        rgb_observation = observation[pixel_key]\n\n        assert rgb_observation.shape == (height, width, 3)\n        assert rgb_observation.dtype == np.uint8\n\n    @pytest.mark.parametrize(\"pixels_only\", (True, False))\n    def test_single_array_observation(self, pixels_only):\n        pixel_key = 'depth'\n\n        env = FakeArrayObservationEnvironment()\n        observation_space = env.observation_space\n        assert isinstance(observation_space, spaces.Box)\n\n        wrapped_env = PixelObservationWrapper(\n            env,\n            pixel_keys=(pixel_key, ),\n            pixels_only=pixels_only)\n        wrapped_env.observation_space = wrapped_env.observation_space\n        assert isinstance(wrapped_env.observation_space, spaces.Dict)\n\n        if pixels_only:\n            assert len(wrapped_env.observation_space.spaces) == 1\n            assert (list(wrapped_env.observation_space.spaces.keys())\n                    == [pixel_key])\n        else:\n            assert len(wrapped_env.observation_space.spaces) == 2\n            assert (\n                list(wrapped_env.observation_space.spaces.keys())\n                == [STATE_KEY, pixel_key])\n\n        observation = wrapped_env.reset()\n        depth_observation = observation[pixel_key]\n\n        assert depth_observation.shape == (32, 32, 3)\n        assert depth_observation.dtype == np.uint8",
        "import gym.spaces as spaces\nfrom gym import ObservationWrapper\n\n\nclass FlattenObservation(ObservationWrapper):\n    r\"\"\"Observation wrapper that flattens the observation.\"\"\"\n    def __init__(self, env):\n        super(FlattenObservation, self).__init__(env)\n        self.observation_space = spaces.flatten_space(env.observation_space)\n\n    def observation(self, observation):\n        return spaces.flatten(self.env.observation_space, observation)",
        "import gym\nfrom gym import Wrapper\nfrom gym import error, version, logger\nimport os, json, numpy as np\nfrom gym.wrappers.monitoring import stats_recorder, video_recorder\nfrom gym.utils import atomic_write, closer\nfrom gym.utils.json_utils import json_encode_np\n\nFILE_PREFIX = 'openaigym'\nMANIFEST_PREFIX = FILE_PREFIX + '.manifest'\n\nclass Monitor(Wrapper):\n    def __init__(self, env, directory, video_callable=None, force=False, resume=False,\n                 write_upon_reset=False, uid=None, mode=None):\n        super(Monitor, self).__init__(env)\n\n        self.videos = []\n\n        self.stats_recorder = None\n        self.video_recorder = None\n        self.enabled = False\n        self.episode_id = 0\n        self._monitor_id = None\n        self.env_semantics_autoreset = env.metadata.get('semantics.autoreset')\n\n        self._start(directory, video_callable, force, resume,\n                            write_upon_reset, uid, mode)\n\n    def step(self, action):\n        self._before_step(action)\n        observation, reward, done, info = self.env.step(action)\n        done = self._after_step(observation, reward, done, info)\n\n        return observation, reward, done, info\n\n    def reset(self, **kwargs):\n        self._before_reset()\n        observation = self.env.reset(**kwargs)\n        self._after_reset(observation)\n\n        return observation\n\n    def set_monitor_mode(self, mode):\n        logger.info(\"Setting the monitor mode is deprecated and will be removed soon\")\n        self._set_mode(mode)\n\n\n    def _start(self, directory, video_callable=None, force=False, resume=False,\n              write_upon_reset=False, uid=None, mode=None):\n        \"\"\"Start monitoring.\n\n        Args:\n            directory (str): A per-training run directory where to record stats.\n            video_callable (Optional[function, False]): function that takes in the index of the episode and outputs a boolean, indicating whether we should record a video on this episode. The default (for video_callable is None) is to take perfect cubes, capped at 1000. False disables video recording.\n            force (bool): Clear out existing training data from this directory (by deleting every file prefixed with \"openaigym.\").\n            resume (bool): Retain the training data already in this directory, which will be merged with our new data\n            write_upon_reset (bool): Write the manifest file on each reset. (This is currently a JSON file, so writing it is somewhat expensive.)\n            uid (Optional[str]): A unique id used as part of the suffix for the file. By default, uses os.getpid().\n            mode (['evaluation', 'training']): Whether this is an evaluation or training episode.\n        \"\"\"\n        if self.env.spec is None:\n            logger.warn(\"Trying to monitor an environment which has no 'spec' set. This usually means you did not create it via 'gym.make', and is recommended only for advanced users.\")\n            env_id = '(unknown)'\n        else:\n            env_id = self.env.spec.id\n\n        if not os.path.exists(directory):\n            logger.info('Creating monitor directory %s', directory)\n            os.makedirs(directory, exist_ok=True)\n\n        if video_callable is None:\n            video_callable = capped_cubic_video_schedule\n        elif video_callable == False:\n            video_callable = disable_videos\n        elif not callable(video_callable):\n            raise error.Error('You must provide a function, None, or False for video_callable, not {}: {}'.format(type(video_callable), video_callable))\n        self.video_callable = video_callable\n\n        # Check on whether we need to clear anything\n        if force:\n            clear_monitor_files(directory)\n        elif not resume:\n            training_manifests = detect_training_manifests(directory)\n            if len(training_manifests) > 0:\n                raise error.Error('''Trying to write to monitor directory {} with existing monitor files: {}.\n\n You should use a unique directory for each training run, or use 'force=True' to automatically clear previous monitor files.'''.format(directory, ', '.join(training_manifests[:5])))\n\n        self._monitor_id = monitor_closer.register(self)\n\n        self.enabled = True\n        self.directory = os.path.abspath(directory)\n        # We use the 'openai-gym' prefix to determine if a file is\n        # ours\n        self.file_prefix = FILE_PREFIX\n        self.file_infix = '{}.{}'.format(self._monitor_id, uid if uid else os.getpid())\n\n        self.stats_recorder = stats_recorder.StatsRecorder(directory, '{}.episode_batch.{}'.format(self.file_prefix, self.file_infix), autoreset=self.env_semantics_autoreset, env_id=env_id)\n\n        if not os.path.exists(directory): os.mkdir(directory)\n        self.write_upon_reset = write_upon_reset\n\n        if mode is not None:\n            self._set_mode(mode)\n\n    def _flush(self, force=False):\n        \"\"\"Flush all relevant monitor information to disk.\"\"\"\n        if not self.write_upon_reset and not force:\n            return\n\n        self.stats_recorder.flush()\n\n        # Give it a very distiguished name, since we need to pick it\n        # up from the filesystem later.\n        path = os.path.join(self.directory, '{}.manifest.{}.manifest.json'.format(self.file_prefix, self.file_infix))\n        logger.debug('Writing training manifest file to %s', path)\n        with atomic_write.atomic_write(path) as f:\n            # We need to write relative paths here since people may\n            # move the training_dir around. It would be cleaner to\n            # already have the basenames rather than basename'ing\n            # manually, but this works for now.\n            json.dump({\n                'stats': os.path.basename(self.stats_recorder.path),\n                'videos': [(os.path.basename(v), os.path.basename(m))\n                           for v, m in self.videos],\n                'env_info': self._env_info(),\n            }, f, default=json_encode_np)\n\n    def close(self):\n        \"\"\"Flush all monitor data to disk and close any open rending windows.\"\"\"\n        super(Monitor, self).close()\n\n        if not self.enabled:\n            return\n        self.stats_recorder.close()\n        if self.video_recorder is not None:\n            self._close_video_recorder()\n        self._flush(force=True)\n\n        # Stop tracking this for autoclose\n        monitor_closer.unregister(self._monitor_id)\n        self.enabled = False\n\n        logger.info('''Finished writing results. You can upload them to the scoreboard via gym.upload(%r)''', self.directory)\n\n    def _set_mode(self, mode):\n        if mode == 'evaluation':\n            type = 'e'\n        elif mode == 'training':\n            type = 't'\n        else:\n            raise error.Error('Invalid mode {}: must be \"training\" or \"evaluation\"', mode)\n        self.stats_recorder.type = type\n\n    def _before_step(self, action):\n        if not self.enabled: return\n        self.stats_recorder.before_step(action)\n\n    def _after_step(self, observation, reward, done, info):\n        if not self.enabled: return done\n\n        if done and self.env_semantics_autoreset:\n            # For envs with BlockingReset wrapping VNCEnv, this observation will be the first one of the new episode\n            self.reset_video_recorder()\n            self.episode_id += 1\n            self._flush()\n\n        # Record stats\n        self.stats_recorder.after_step(observation, reward, done, info)\n        # Record video\n        self.video_recorder.capture_frame()\n\n        return done\n\n    def _before_reset(self):\n        if not self.enabled: return\n        self.stats_recorder.before_reset()\n\n    def _after_reset(self, observation):\n        if not self.enabled: return\n\n        # Reset the stat count\n        self.stats_recorder.after_reset(observation)\n\n        self.reset_video_recorder()\n\n        # Bump *after* all reset activity has finished\n        self.episode_id += 1\n\n        self._flush()\n\n    def reset_video_recorder(self):\n        # Close any existing video recorder\n        if self.video_recorder:\n            self._close_video_recorder()\n\n        # Start recording the next video.\n        #\n        # TODO: calculate a more correct 'episode_id' upon merge\n        self.video_recorder = video_recorder.VideoRecorder(\n            env=self.env,\n            base_path=os.path.join(self.directory, '{}.video.{}.video{:06}'.format(self.file_prefix, self.file_infix, self.episode_id)),\n            metadata={'episode_id': self.episode_id},\n            enabled=self._video_enabled(),\n        )\n        self.video_recorder.capture_frame()\n\n    def _close_video_recorder(self):\n        self.video_recorder.close()\n        if self.video_recorder.functional:\n            self.videos.append((self.video_recorder.path, self.video_recorder.metadata_path))\n\n    def _video_enabled(self):\n        return self.video_callable(self.episode_id)\n\n    def _env_info(self):\n        env_info = {\n            'gym_version': version.VERSION,\n        }\n        if self.env.spec:\n            env_info['env_id'] = self.env.spec.id\n        return env_info\n\n    def __del__(self):\n        # Make sure we've closed up shop when garbage collecting\n        self.close()\n\n    def get_total_steps(self):\n        return self.stats_recorder.total_steps        \n\n    def get_episode_rewards(self):\n        return self.stats_recorder.episode_rewards\n\n    def get_episode_lengths(self):\n        return self.stats_recorder.episode_lengths\n\ndef detect_training_manifests(training_dir, files=None):\n    if files is None:\n        files = os.listdir(training_dir)\n    return [os.path.join(training_dir, f) for f in files if f.startswith(MANIFEST_PREFIX + '.')]\n\ndef detect_monitor_files(training_dir):\n    return [os.path.join(training_dir, f) for f in os.listdir(training_dir) if f.startswith(FILE_PREFIX + '.')]\n\ndef clear_monitor_files(training_dir):\n    files = detect_monitor_files(training_dir)\n    if len(files) == 0:\n        return\n\n    logger.info('Clearing %d monitor files from previous run (because force=True was provided)', len(files))\n    for file in files:\n        os.unlink(file)\n\ndef capped_cubic_video_schedule(episode_id):\n    if episode_id < 1000:\n        return int(round(episode_id ** (1. / 3))) ** 3 == episode_id\n    else:\n        return episode_id % 1000 == 0\n\ndef disable_videos(episode_id):\n    return False\n\nmonitor_closer = closer.Closer()\n\n# This method gets used for a sanity check in scoreboard/api.py. It's\n# not intended for use outside of the gym codebase.\ndef _open_monitors():\n    return list(monitor_closer.closeables.values())\n\ndef load_env_info_from_manifests(manifests, training_dir):\n    env_infos = []\n    for manifest in manifests:\n        with open(manifest) as f:\n            contents = json.load(f)\n            env_infos.append(contents['env_info'])\n\n    env_info = collapse_env_infos(env_infos, training_dir)\n    return env_info\n\ndef load_results(training_dir):\n    if not os.path.exists(training_dir):\n        logger.error('Training directory %s not found', training_dir)\n        return\n\n    manifests = detect_training_manifests(training_dir)\n    if not manifests:\n        logger.error('No manifests found in training directory %s', training_dir)\n        return\n\n    logger.debug('Uploading data from manifest %s', ', '.join(manifests))\n\n    # Load up stats + video files\n    stats_files = []\n    videos = []\n    env_infos = []\n\n    for manifest in manifests:\n        with open(manifest) as f:\n            contents = json.load(f)\n            # Make these paths absolute again\n            stats_files.append(os.path.join(training_dir, contents['stats']))\n            videos += [(os.path.join(training_dir, v), os.path.join(training_dir, m))\n                       for v, m in contents['videos']]\n            env_infos.append(contents['env_info'])\n\n    env_info = collapse_env_infos(env_infos, training_dir)\n    data_sources, initial_reset_timestamps, timestamps, episode_lengths, episode_rewards, episode_types, initial_reset_timestamp = merge_stats_files(stats_files)\n\n    return {\n        'manifests': manifests,\n        'env_info': env_info,\n        'data_sources': data_sources,\n        'timestamps': timestamps,\n        'episode_lengths': episode_lengths,\n        'episode_rewards': episode_rewards,\n        'episode_types': episode_types,\n        'initial_reset_timestamps': initial_reset_timestamps,\n        'initial_reset_timestamp': initial_reset_timestamp,\n        'videos': videos,\n    }\n\ndef merge_stats_files(stats_files):\n    timestamps = []\n    episode_lengths = []\n    episode_rewards = []\n    episode_types = []\n    initial_reset_timestamps = []\n    data_sources = []\n\n    for i, path in enumerate(stats_files):\n        with open(path) as f:\n            content = json.load(f)\n            if len(content['timestamps'])==0: continue # so empty file doesn't mess up results, due to null initial_reset_timestamp\n            data_sources += [i] * len(content['timestamps'])\n            timestamps += content['timestamps']\n            episode_lengths += content['episode_lengths']\n            episode_rewards += content['episode_rewards']\n            # Recent addition\n            episode_types += content.get('episode_types', [])\n            # Keep track of where each episode came from.\n            initial_reset_timestamps.append(content['initial_reset_timestamp'])\n\n    idxs = np.argsort(timestamps)\n    timestamps = np.array(timestamps)[idxs].tolist()\n    episode_lengths = np.array(episode_lengths)[idxs].tolist()\n    episode_rewards = np.array(episode_rewards)[idxs].tolist()\n    data_sources = np.array(data_sources)[idxs].tolist()\n\n    if episode_types:\n        episode_types = np.array(episode_types)[idxs].tolist()\n    else:\n        episode_types = None\n\n    if len(initial_reset_timestamps) > 0:\n        initial_reset_timestamp = min(initial_reset_timestamps)\n    else:\n        initial_reset_timestamp = 0\n\n    return data_sources, initial_reset_timestamps, timestamps, episode_lengths, episode_rewards, episode_types, initial_reset_timestamp\n\n# TODO training_dir isn't used except for error messages, clean up the layering\ndef collapse_env_infos(env_infos, training_dir):\n    assert len(env_infos) > 0\n\n    first = env_infos[0]\n    for other in env_infos[1:]:\n        if first != other:\n            raise error.Error('Found two unequal env_infos: {} and {}. This usually indicates that your training directory {} has commingled results from multiple runs.'.format(first, other, training_dir))\n\n    for key in ['env_id', 'gym_version']:\n        if key not in first:\n            raise error.Error(\"env_info {} from training directory {} is missing expected key {}. This is unexpected and likely indicates a bug in gym.\".format(first, training_dir, key))\n    return first",
        "import copy\n\nfrom gym import spaces\nfrom gym import ObservationWrapper\n\n\nclass FilterObservation(ObservationWrapper):\n    \"\"\"Filter dictionary observations by their keys.\n    \n    Args:\n        env: The environment to wrap.\n        filter_keys: List of keys to be included in the observations.\n\n    Raises:\n        ValueError: If observation keys in not instance of None or\n            iterable.\n        ValueError: If any of the `filter_keys` are not included in\n            the original `env`'s observation space\n    \n    \"\"\"\n    def __init__(self, env, filter_keys=None):\n        super(FilterObservation, self).__init__(env)\n\n        wrapped_observation_space = env.observation_space\n        assert isinstance(wrapped_observation_space, spaces.Dict), (\n            \"FilterObservationWrapper is only usable with dict observations.\")\n\n        observation_keys = wrapped_observation_space.spaces.keys()\n\n        if filter_keys is None:\n            filter_keys = tuple(observation_keys)\n\n        missing_keys = set(\n            key for key in filter_keys if key not in observation_keys)\n\n        if missing_keys:\n            raise ValueError(\n                \"All the filter_keys must be included in the \"\n                \"original obsrevation space.\\n\"\n                \"Filter keys: {filter_keys}\\n\"\n                \"Observation keys: {observation_keys}\\n\"\n                \"Missing keys: {missing_keys}\".format(\n                    filter_keys=filter_keys,\n                    observation_keys=observation_keys,\n                    missing_keys=missing_keys,\n                ))\n\n        self.observation_space = type(wrapped_observation_space)([\n            (name, copy.deepcopy(space))\n            for name, space in wrapped_observation_space.spaces.items()\n            if name in filter_keys\n        ])\n\n        self._env = env\n        self._filter_keys = tuple(filter_keys)\n\n    def observation(self, observation):\n        filter_observation = self._filter_observation(observation)\n        return filter_observation\n\n    def _filter_observation(self, observation):\n        observation = type(observation)([\n            (name, value)\n            for name, value in observation.items()\n            if name in self._filter_keys\n        ])\n        return observation",
        "import pytest\n\nimport numpy as np\n\nimport gym\nfrom gym.wrappers import TransformReward\n\n\n@pytest.mark.parametrize('env_id', ['CartPole-v1', 'Pendulum-v0'])\ndef test_transform_reward(env_id):\n    # use case #1: scale\n    scales = [0.1, 200]\n    for scale in scales:\n        env = gym.make(env_id)\n        wrapped_env = TransformReward(gym.make(env_id), lambda r: scale*r)\n        action = env.action_space.sample()\n\n        env.seed(0)\n        env.reset()\n        wrapped_env.seed(0)\n        wrapped_env.reset()\n\n        _, reward, _, _ = env.step(action)\n        _, wrapped_reward, _, _ = wrapped_env.step(action)\n\n        assert wrapped_reward == scale*reward\n    del env, wrapped_env\n\n    # use case #2: clip\n    min_r = -0.0005\n    max_r = 0.0002\n    env = gym.make(env_id)\n    wrapped_env = TransformReward(gym.make(env_id), lambda r: np.clip(r, min_r, max_r))\n    action = env.action_space.sample()\n\n    env.seed(0)\n    env.reset()\n    wrapped_env.seed(0)\n    wrapped_env.reset()\n\n    _, reward, _, _ = env.step(action)\n    _, wrapped_reward, _, _ = wrapped_env.step(action)\n\n    assert abs(wrapped_reward) < abs(reward)\n    assert wrapped_reward == -0.0005 or wrapped_reward == 0.0002\n    del env, wrapped_env\n\n    # use case #3: sign\n    env = gym.make(env_id)\n    wrapped_env = TransformReward(gym.make(env_id), lambda r: np.sign(r))\n\n    env.seed(0)\n    env.reset()\n    wrapped_env.seed(0)\n    wrapped_env.reset()\n\n    for _ in range(1000):\n        action = env.action_space.sample()\n        _, wrapped_reward, done, _ = wrapped_env.step(action)\n        assert wrapped_reward in [-1.0, 0.0, 1.0]\n        if done:\n            break\n    del env, wrapped_env",
        "import numpy as np\n\nfrom gym import ActionWrapper\nfrom gym.spaces import Box\n\n\nclass ClipAction(ActionWrapper):\n    r\"\"\"Clip the continuous action within the valid bound. \"\"\"\n    def __init__(self, env):\n        assert isinstance(env.action_space, Box)\n        super(ClipAction, self).__init__(env)\n\n    def action(self, action):\n        return np.clip(action, self.action_space.low, self.action_space.high)",
        "from gym import ObservationWrapper\n\n\nclass TransformObservation(ObservationWrapper):\n    r\"\"\"Transform the observation via an arbitrary function. \n\n    Example::\n\n        >>> import gym\n        >>> env = gym.make('CartPole-v1')\n        >>> env = TransformObservation(env, lambda obs: obs + 0.1*np.random.randn(*obs.shape))\n        >>> env.reset()\n        array([-0.08319338,  0.04635121, -0.07394746,  0.20877492])\n\n    Args:\n        env (Env): environment\n        f (callable): a function that transforms the observation\n\n    \"\"\"\n    def __init__(self, env, f):\n        super(TransformObservation, self).__init__(env)\n        assert callable(f)\n        self.f = f\n\n    def observation(self, observation):\n        return self.f(observation)",
        "",
        "from gym import error\nfrom gym.wrappers.monitor import Monitor\nfrom gym.wrappers.time_limit import TimeLimit\nfrom gym.wrappers.filter_observation import FilterObservation\nfrom gym.wrappers.atari_preprocessing import AtariPreprocessing\nfrom gym.wrappers.rescale_action import RescaleAction\nfrom gym.wrappers.flatten_observation import FlattenObservation\nfrom gym.wrappers.gray_scale_observation import GrayScaleObservation\nfrom gym.wrappers.frame_stack import LazyFrames\nfrom gym.wrappers.frame_stack import FrameStack\nfrom gym.wrappers.transform_observation import TransformObservation\nfrom gym.wrappers.transform_reward import TransformReward\nfrom gym.wrappers.resize_observation import ResizeObservation\nfrom gym.wrappers.clip_action import ClipAction\nfrom gym.wrappers.record_episode_statistics import RecordEpisodeStatistics",
        "import pytest\n\nimport gym\nfrom gym.wrappers import RecordEpisodeStatistics\n\n\n@pytest.mark.parametrize('env_id', ['CartPole-v0', 'Pendulum-v0'])\n@pytest.mark.parametrize('deque_size', [2, 5])\ndef test_record_episode_statistics(env_id, deque_size):\n    env = gym.make(env_id)\n    env = RecordEpisodeStatistics(env, deque_size)\n\n    for n in range(5):\n        env.reset()\n        assert env.episode_return == 0.0\n        assert env.episode_length == 0\n        for t in range(env.spec.max_episode_steps):\n            _, _, done, info = env.step(env.action_space.sample())\n            if done:\n                assert 'episode' in info\n                assert all([item in info['episode'] for item in ['r', 'l', 't']])\n                break\n    assert len(env.return_queue) == deque_size\n    assert len(env.length_queue) == deque_size",
        "import pytest\n\nimport numpy as np\n\nimport gym\nfrom gym.wrappers import FlattenObservation\nfrom gym import spaces\n\n\n@pytest.mark.parametrize('env_id', ['Blackjack-v0', 'KellyCoinflip-v0'])\ndef test_flatten_observation(env_id):\n    env = gym.make(env_id)\n    wrapped_env = FlattenObservation(env)\n\n    obs = env.reset()\n    wrapped_obs = wrapped_env.reset()\n\n    if env_id == 'Blackjack-v0':\n        space = spaces.Tuple((\n            spaces.Discrete(32),\n            spaces.Discrete(11),\n            spaces.Discrete(2)))\n        wrapped_space = spaces.Box(-np.inf, np.inf,\n                                   [32 + 11 + 2], dtype=np.float32)\n    elif env_id == 'KellyCoinflip-v0':\n        space = spaces.Tuple((\n            spaces.Box(0, 250.0, [1], dtype=np.float32),\n            spaces.Discrete(300 + 1)))\n        wrapped_space = spaces.Box(-np.inf, np.inf,\n                                   [1 + (300 + 1)], dtype=np.float32)\n\n    assert space.contains(obs)\n    assert wrapped_space.contains(wrapped_obs)",
        "import pytest\npytest.importorskip(\"atari_py\")\n\nimport numpy as np\nimport gym\nfrom gym.wrappers import FrameStack\ntry:\n    import lz4\nexcept ImportError:\n    lz4 = None\n\n\n@pytest.mark.parametrize('env_id', ['CartPole-v1', 'Pendulum-v0', 'Pong-v0'])\n@pytest.mark.parametrize('num_stack', [2, 3, 4])\n@pytest.mark.parametrize('lz4_compress', [\n    pytest.param(True, marks=pytest.mark.skipif(lz4 is None, reason=\"Need lz4 to run tests with compression\")),\n    False\n])\ndef test_frame_stack(env_id, num_stack, lz4_compress):\n    env = gym.make(env_id)\n    shape = env.observation_space.shape\n    env = FrameStack(env, num_stack, lz4_compress)\n    assert env.observation_space.shape == (num_stack,) + shape\n\n    obs = env.reset()\n    obs = np.asarray(obs)\n    assert obs.shape == (num_stack,) + shape\n    for i in range(1, num_stack):\n        assert np.allclose(obs[i - 1], obs[i])\n\n    obs, _, _, _ = env.step(env.action_space.sample())\n    obs = np.asarray(obs)\n    assert obs.shape == (num_stack,) + shape\n    for i in range(1, num_stack - 1):\n        assert np.allclose(obs[i - 1], obs[i])\n    assert not np.allclose(obs[-1], obs[-2])",
        "import pytest\n\nimport numpy as np\n\nimport gym\nfrom gym.wrappers import TransformObservation\n\n\n@pytest.mark.parametrize('env_id', ['CartPole-v1', 'Pendulum-v0'])\ndef test_transform_observation(env_id):\n    affine_transform = lambda x: 3*x + 2\n    env = gym.make(env_id)\n    wrapped_env = TransformObservation(gym.make(env_id), lambda obs: affine_transform(obs))\n\n    env.seed(0)\n    wrapped_env.seed(0)\n\n    obs = env.reset()\n    wrapped_obs = wrapped_env.reset()\n    assert np.allclose(wrapped_obs, affine_transform(obs))\n\n    action = env.action_space.sample()\n    obs, reward, done, _ = env.step(action)\n    wrapped_obs, wrapped_reward, wrapped_done, _ = wrapped_env.step(action)\n    assert np.allclose(wrapped_obs, affine_transform(obs))\n    assert np.allclose(wrapped_reward, reward)\n    assert wrapped_done == done",
        "import gym\n\n\nclass TimeLimit(gym.Wrapper):\n    def __init__(self, env, max_episode_steps=None):\n        super(TimeLimit, self).__init__(env)\n        if max_episode_steps is None and self.env.spec is not None:\n            max_episode_steps = env.spec.max_episode_steps\n        if self.env.spec is not None:\n            self.env.spec.max_episode_steps = max_episode_steps\n        self._max_episode_steps = max_episode_steps\n        self._elapsed_steps = None\n\n    def step(self, action):\n        assert self._elapsed_steps is not None, \"Cannot call env.step() before calling reset()\"\n        observation, reward, done, info = self.env.step(action)\n        self._elapsed_steps += 1\n        if self._elapsed_steps >= self._max_episode_steps:\n            info['TimeLimit.truncated'] = not done\n            done = True\n        return observation, reward, done, info\n\n    def reset(self, **kwargs):\n        self._elapsed_steps = 0\n        return self.env.reset(**kwargs)",
        "from gym import RewardWrapper\n\n\nclass TransformReward(RewardWrapper):\n    r\"\"\"Transform the reward via an arbitrary function.\n\n    Example::\n\n        >>> import gym\n        >>> env = gym.make('CartPole-v1')\n        >>> env = TransformReward(env, lambda r: 0.01*r)\n        >>> env.reset()\n        >>> observation, reward, done, info = env.step(env.action_space.sample())\n        >>> reward\n        0.01\n\n    Args:\n        env (Env): environment\n        f (callable): a function that transforms the reward\n\n    \"\"\"\n    def __init__(self, env, f):\n        super(TransformReward, self).__init__(env)\n        assert callable(f)\n        self.f = f\n\n    def reward(self, reward):\n        return self.f(reward)",
        "import pytest\n\nimport numpy as np\n\nimport gym\nfrom gym.wrappers import GrayScaleObservation\nfrom gym.wrappers import AtariPreprocessing\npytest.importorskip('atari_py')\npytest.importorskip('cv2')\n\n@pytest.mark.parametrize('env_id', ['PongNoFrameskip-v0', 'SpaceInvadersNoFrameskip-v0'])\n@pytest.mark.parametrize('keep_dim', [True, False])\ndef test_gray_scale_observation(env_id, keep_dim):\n    gray_env = AtariPreprocessing(gym.make(env_id), screen_size=84, grayscale_obs=True)\n    rgb_env = AtariPreprocessing(gym.make(env_id), screen_size=84, grayscale_obs=False)\n    wrapped_env = GrayScaleObservation(rgb_env, keep_dim=keep_dim)\n    assert rgb_env.observation_space.shape[-1] == 3\n\n    seed = 0\n    gray_env.seed(seed)\n    wrapped_env.seed(seed)\n\n    gray_obs = gray_env.reset()\n    wrapped_obs = wrapped_env.reset()\n\n    if keep_dim:\n        assert wrapped_env.observation_space.shape[-1] == 1\n        assert len(wrapped_obs.shape) == 3\n        wrapped_obs = wrapped_obs.squeeze(-1)\n    else:\n        assert len(wrapped_env.observation_space.shape) == 2\n        assert len(wrapped_obs.shape) == 2\n\n    # ALE gray scale is slightly different, but no more than by one shade\n    assert np.allclose(gray_obs.astype('int32'), wrapped_obs.astype('int32'), atol=1)",
        "\"\"\"An observation wrapper that augments observations by pixel values.\"\"\"\n\nimport collections\nimport copy\n\nimport numpy as np\n\nfrom gym import spaces\nfrom gym import ObservationWrapper\n\nSTATE_KEY = 'state'\n\n\nclass PixelObservationWrapper(ObservationWrapper):\n    \"\"\"Augment observations by pixel values.\"\"\"\n\n    def __init__(self,\n                 env,\n                 pixels_only=True,\n                 render_kwargs=None,\n                 pixel_keys=('pixels', )):\n        \"\"\"Initializes a new pixel Wrapper.\n\n        Args:\n            env: The environment to wrap.\n            pixels_only: If `True` (default), the original observation returned\n                by the wrapped environment will be discarded, and a dictionary\n                observation will only include pixels. If `False`, the\n                observation dictionary will contain both the original\n                observations and the pixel observations.\n            render_kwargs: Optional `dict` containing keyword arguments passed\n                to the `self.render` method.\n            pixel_keys: Optional custom string specifying the pixel\n                observation's key in the `OrderedDict` of observations.\n                Defaults to 'pixels'.\n\n        Raises:\n            ValueError: If `env`'s observation spec is not compatible with the\n                wrapper. Supported formats are a single array, or a dict of\n                arrays.\n            ValueError: If `env`'s observation already contains any of the\n                specified `pixel_keys`.\n        \"\"\"\n\n        super(PixelObservationWrapper, self).__init__(env)\n\n        if render_kwargs is None:\n            render_kwargs = {}\n\n        for key in pixel_keys:\n            render_kwargs.setdefault(key, {})\n\n            render_mode = render_kwargs[key].pop('mode', 'rgb_array')\n            assert render_mode == 'rgb_array', render_mode\n            render_kwargs[key]['mode'] = 'rgb_array'\n\n        wrapped_observation_space = env.observation_space\n\n        if isinstance(wrapped_observation_space, spaces.Box):\n            self._observation_is_dict = False\n            invalid_keys = set([STATE_KEY])\n        elif isinstance(wrapped_observation_space,\n                        (spaces.Dict, collections.MutableMapping)):\n            self._observation_is_dict = True\n            invalid_keys = set(wrapped_observation_space.spaces.keys())\n        else:\n            raise ValueError(\"Unsupported observation space structure.\")\n\n        if not pixels_only:\n            # Make sure that now keys in the `pixel_keys` overlap with\n            # `observation_keys`\n            overlapping_keys = set(pixel_keys) & set(invalid_keys)\n            if overlapping_keys:\n                raise ValueError(\"Duplicate or reserved pixel keys {!r}.\"\n                                 .format(overlapping_keys))\n\n        if pixels_only:\n            self.observation_space = spaces.Dict()\n        elif self._observation_is_dict:\n            self.observation_space = copy.deepcopy(wrapped_observation_space)\n        else:\n            self.observation_space = spaces.Dict()\n            self.observation_space.spaces[STATE_KEY] = wrapped_observation_space\n\n        # Extend observation space with pixels.\n\n        pixels_spaces = {}\n        for pixel_key in pixel_keys:\n            pixels = self.env.render(**render_kwargs[pixel_key])\n\n            if np.issubdtype(pixels.dtype, np.integer):\n                low, high = (0, 255)\n            elif np.issubdtype(pixels.dtype, np.float):\n                low, high = (-float('inf'), float('inf'))\n            else:\n                raise TypeError(pixels.dtype)\n\n            pixels_space = spaces.Box(\n                shape=pixels.shape, low=low, high=high, dtype=pixels.dtype)\n            pixels_spaces[pixel_key] = pixels_space\n\n        self.observation_space.spaces.update(pixels_spaces)\n\n        self._env = env\n        self._pixels_only = pixels_only\n        self._render_kwargs = render_kwargs\n        self._pixel_keys = pixel_keys\n\n    def observation(self, observation):\n        pixel_observation = self._add_pixel_observation(observation)\n        return pixel_observation\n\n    def _add_pixel_observation(self, observation):\n        if self._pixels_only:\n            observation = collections.OrderedDict()\n        elif self._observation_is_dict:\n            observation = type(observation)(observation)\n        else:\n            observation = collections.OrderedDict()\n            observation[STATE_KEY] = observation\n\n        pixel_observations = {\n            pixel_key: self.env.render(**self._render_kwargs[pixel_key])\n            for pixel_key in self._pixel_keys\n        }\n\n        observation.update(pixel_observations)\n\n        return observation",
        "import numpy as np\n\nimport gym\nfrom gym.spaces import Box\nfrom gym.wrappers import TimeLimit\ntry:\n    import cv2\nexcept ImportError:\n    cv2 = None\n\n\nclass AtariPreprocessing(gym.Wrapper):\n    r\"\"\"Atari 2600 preprocessings. \n\n    This class follows the guidelines in \n    Machado et al. (2018), \"Revisiting the Arcade Learning Environment: \n    Evaluation Protocols and Open Problems for General Agents\".\n\n    Specifically:\n\n    * NoopReset: obtain initial state by taking random number of no-ops on reset. \n    * Frame skipping: 4 by default\n    * Max-pooling: most recent two observations\n    * Termination signal when a life is lost: turned off by default. Not recommended by Machado et al. (2018).\n    * Resize to a square image: 84x84 by default\n    * Grayscale observation: optional\n    * Scale observation: optional\n\n    Args:\n        env (Env): environment\n        noop_max (int): max number of no-ops\n        frame_skip (int): the frequency at which the agent experiences the game. \n        screen_size (int): resize Atari frame\n        terminal_on_life_loss (bool): if True, then step() returns done=True whenever a\n            life is lost. \n        grayscale_obs (bool): if True, then gray scale observation is returned, otherwise, RGB observation\n            is returned.\n        scale_obs (bool): if True, then observation normalized in range [0,1] is returned. It also limits memory\n            optimization benefits of FrameStack Wrapper.\n    \"\"\"\n\n    def __init__(self, env, noop_max=30, frame_skip=4, screen_size=84, terminal_on_life_loss=False, grayscale_obs=True,\n                 scale_obs=False):\n        super().__init__(env)\n        assert cv2 is not None, \\\n            \"opencv-python package not installed! Try running pip install gym[atari] to get dependencies  for atari\"\n        assert frame_skip > 0\n        assert screen_size > 0\n        assert noop_max >= 0\n        if frame_skip > 1:\n            assert 'NoFrameskip' in env.spec.id, 'disable frame-skipping in the original env. for more than one' \\\n                                                 ' frame-skip as it will be done by the wrapper'\n        self.noop_max = noop_max\n        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n\n        self.frame_skip = frame_skip\n        self.screen_size = screen_size\n        self.terminal_on_life_loss = terminal_on_life_loss\n        self.grayscale_obs = grayscale_obs\n        self.scale_obs = scale_obs\n\n        # buffer of most recent two observations for max pooling\n        if grayscale_obs:\n            self.obs_buffer = [np.empty(env.observation_space.shape[:2], dtype=np.uint8),\n                               np.empty(env.observation_space.shape[:2], dtype=np.uint8)]\n        else:\n            self.obs_buffer = [np.empty(env.observation_space.shape, dtype=np.uint8),\n                               np.empty(env.observation_space.shape, dtype=np.uint8)]\n\n        self.ale = env.unwrapped.ale\n        self.lives = 0\n        self.game_over = False\n\n        _low, _high, _obs_dtype = (0, 255, np.uint8) if not scale_obs else (0, 1, np.float32)\n        if grayscale_obs:\n            self.observation_space = Box(low=_low, high=_high, shape=(screen_size, screen_size), dtype=_obs_dtype)\n        else:\n            self.observation_space = Box(low=_low, high=_high, shape=(screen_size, screen_size, 3), dtype=_obs_dtype)\n\n    def step(self, action):\n        R = 0.0\n\n        for t in range(self.frame_skip):\n            _, reward, done, info = self.env.step(action)\n            R += reward\n            self.game_over = done\n\n            if self.terminal_on_life_loss:\n                new_lives = self.ale.lives()\n                done = done or new_lives < self.lives\n                self.lives = new_lives\n\n            if done:\n                break\n            if t == self.frame_skip - 2:\n                if self.grayscale_obs:\n                    self.ale.getScreenGrayscale(self.obs_buffer[1])\n                else:\n                    self.ale.getScreenRGB2(self.obs_buffer[1])\n            elif t == self.frame_skip - 1:\n                if self.grayscale_obs:\n                    self.ale.getScreenGrayscale(self.obs_buffer[0])\n                else:\n                    self.ale.getScreenRGB2(self.obs_buffer[0])\n        return self._get_obs(), R, done, info\n\n    def reset(self, **kwargs):\n        # NoopReset\n        self.env.reset(**kwargs)\n        noops = self.env.unwrapped.np_random.randint(1, self.noop_max + 1) if self.noop_max > 0 else 0\n        for _ in range(noops):\n            _, _, done, _ = self.env.step(0)\n            if done:\n                self.env.reset(**kwargs)\n\n        self.lives = self.ale.lives()\n        if self.grayscale_obs:\n            self.ale.getScreenGrayscale(self.obs_buffer[0])\n        else:\n            self.ale.getScreenRGB2(self.obs_buffer[0])\n        self.obs_buffer[1].fill(0)\n        return self._get_obs()\n\n    def _get_obs(self):\n        if self.frame_skip > 1:  # more efficient in-place pooling\n            np.maximum(self.obs_buffer[0], self.obs_buffer[1], out=self.obs_buffer[0])\n        obs = cv2.resize(self.obs_buffer[0], (self.screen_size, self.screen_size), interpolation=cv2.INTER_AREA)\n\n        if self.scale_obs:\n            obs = np.asarray(obs, dtype=np.float32) / 255.0\n        else:\n            obs = np.asarray(obs, dtype=np.uint8)\n        return obs",
        "from collections import deque\nimport numpy as np\n\nfrom gym.spaces import Box\nfrom gym import Wrapper\n\n\nclass LazyFrames(object):\n    r\"\"\"Ensures common frames are only stored once to optimize memory use. \n\n    To further reduce the memory use, it is optionally to turn on lz4 to \n    compress the observations.\n\n    .. note::\n\n        This object should only be converted to numpy array just before forward pass. \n\n    \"\"\"\n    def __init__(self, frames, lz4_compress=False):\n        if lz4_compress:\n            from lz4.block import compress\n            self.frame_shape = frames[0].shape\n            self.dtype = frames[0].dtype\n            frames = [compress(frame) for frame in frames]\n        self._frames = frames\n        self.lz4_compress = lz4_compress\n\n    def __array__(self, dtype=None):\n        if self.lz4_compress:\n            from lz4.block import decompress\n            frames = [np.frombuffer(decompress(frame), dtype=self.dtype).reshape(self.frame_shape) for frame in self._frames]\n        else:\n            frames = self._frames\n        out = np.stack(frames, axis=0)\n        if dtype is not None:\n            out = out.astype(dtype)\n        return out\n\n    def __len__(self):\n        return len(self.__array__())\n\n    def __getitem__(self, i):\n        return self.__array__()[i]\n\n    def __eq__(self, other):\n        return self.__array__() == other\n\n    @property\n    def shape(self):\n        return self.__array__().shape\n\n\nclass FrameStack(Wrapper):\n    r\"\"\"Observation wrapper that stacks the observations in a rolling manner. \n\n    For example, if the number of stacks is 4, then the returned observation contains\n    the most recent 4 observations. For environment 'Pendulum-v0', the original observation\n    is an array with shape [3], so if we stack 4 observations, the processed observation\n    has shape [3, 4]. \n\n    .. note::\n\n        To be memory efficient, the stacked observations are wrapped by :class:`LazyFrame`.\n\n    .. note::\n\n        The observation space must be `Box` type. If one uses `Dict`\n        as observation space, it should apply `FlattenDictWrapper` at first. \n\n    Example::\n\n        >>> import gym\n        >>> env = gym.make('PongNoFrameskip-v0')\n        >>> env = FrameStack(env, 4)\n        >>> env.observation_space\n        Box(4, 210, 160, 3)\n\n    Args:\n        env (Env): environment object\n        num_stack (int): number of stacks\n\n    \"\"\"\n    def __init__(self, env, num_stack, lz4_compress=False):\n        super(FrameStack, self).__init__(env)\n        self.num_stack = num_stack\n        self.lz4_compress = lz4_compress\n\n        self.frames = deque(maxlen=num_stack)\n\n        low = np.repeat(self.observation_space.low[np.newaxis, ...], num_stack, axis=0)\n        high = np.repeat(self.observation_space.high[np.newaxis, ...], num_stack, axis=0)\n        self.observation_space = Box(low=low, high=high, dtype=self.observation_space.dtype)\n\n    def _get_observation(self):\n        assert len(self.frames) == self.num_stack, (len(self.frames), self.num_stack)\n        return LazyFrames(list(self.frames), self.lz4_compress)\n\n    def step(self, action):\n        observation, reward, done, info = self.env.step(action)\n        self.frames.append(observation)\n        return self._get_observation(), reward, done, info\n\n    def reset(self, **kwargs):\n        observation = self.env.reset(**kwargs)\n        [self.frames.append(observation) for _ in range(self.num_stack)]\n        return self._get_observation()",
        "import pytest\n\nimport gym\nfrom gym.wrappers import ResizeObservation\ntry:\n    import atari_py\nexcept ImportError:\n    atari_py = None\n\n\n@pytest.mark.skipif(atari_py is None, reason='Only run this test when atari_py is installed')\n@pytest.mark.parametrize('env_id', ['PongNoFrameskip-v0', 'SpaceInvadersNoFrameskip-v0'])\n@pytest.mark.parametrize('shape', [16, 32, (8, 5), [10, 7]])\ndef test_resize_observation(env_id, shape):\n    env = gym.make(env_id)\n    env = ResizeObservation(env, shape)\n\n\n    assert env.observation_space.shape[-1] == 3\n    obs = env.reset()\n    if isinstance(shape, int):\n        assert env.observation_space.shape[:2] == (shape, shape)\n        assert obs.shape == (shape, shape, 3)\n    else:\n        assert env.observation_space.shape[:2] == tuple(shape)\n        assert obs.shape == tuple(shape) + (3,)",
        "import numpy as np\n\nimport gym\nfrom gym import spaces\n\n\nclass RescaleAction(gym.ActionWrapper):\n    r\"\"\"Rescales the continuous action space of the environment to a range [a,b].\n\n    Example::\n\n        >>> RescaleAction(env, a, b).action_space == Box(a,b)\n        True\n\n    \"\"\"\n    def __init__(self, env, a, b):\n        assert isinstance(env.action_space, spaces.Box), (\n            \"expected Box action space, got {}\".format(type(env.action_space)))\n        assert np.less_equal(a, b).all(), (a, b)\n        super(RescaleAction, self).__init__(env)\n        self.a = np.zeros(env.action_space.shape, dtype=env.action_space.dtype) + a\n        self.b = np.zeros(env.action_space.shape, dtype=env.action_space.dtype) + b\n        self.action_space = spaces.Box(low=a, high=b, shape=env.action_space.shape, dtype=env.action_space.dtype)\n\n    def action(self, action):\n        assert np.all(np.greater_equal(action, self.a)), (action, self.a)\n        assert np.all(np.less_equal(action, self.b)), (action, self.b)\n        low = self.env.action_space.low\n        high = self.env.action_space.high\n        action = low + (high - low)*((action - self.a)/(self.b - self.a))\n        action = np.clip(action, low, high)\n        return action",
        "import json\nimport os\nimport shutil\nimport tempfile\nimport numpy as np\n\nimport gym\nfrom gym.wrappers.monitoring.video_recorder import VideoRecorder\n\nclass BrokenRecordableEnv(object):\n    metadata = {'render.modes': [None, 'rgb_array']}\n\n    def render(self, mode=None):\n        pass\n\nclass UnrecordableEnv(object):\n    metadata = {'render.modes': [None]}\n\n    def render(self, mode=None):\n        pass\n\ndef test_record_simple():\n    env = gym.make(\"CartPole-v1\")\n    rec = VideoRecorder(env)\n    env.reset()\n    rec.capture_frame()\n    rec.close()\n    assert not rec.empty\n    assert not rec.broken\n    assert os.path.exists(rec.path)\n    f = open(rec.path)\n    assert os.fstat(f.fileno()).st_size > 100\n\ndef test_no_frames():\n    env = BrokenRecordableEnv()\n    rec = VideoRecorder(env)\n    rec.close()\n    assert rec.empty\n    assert rec.functional\n    assert not os.path.exists(rec.path)\n\ndef test_record_unrecordable_method():\n    env = UnrecordableEnv()\n    rec = VideoRecorder(env)\n    assert not rec.enabled\n    rec.close()\n\ndef test_record_breaking_render_method():\n    env = BrokenRecordableEnv()\n    rec = VideoRecorder(env)\n    rec.capture_frame()\n    rec.close()\n    assert rec.empty\n    assert rec.broken\n    assert not os.path.exists(rec.path)\n\ndef test_text_envs():\n    env = gym.make('FrozenLake-v0')\n    video = VideoRecorder(env)\n    try:\n        env.reset()\n        video.capture_frame()\n        video.close()\n    finally:\n        os.remove(video.path)",
        "",
        "import contextlib\nimport shutil\nimport tempfile\n\n@contextlib.contextmanager\ndef tempdir():\n    temp = tempfile.mkdtemp()\n    yield temp\n    shutil.rmtree(temp)",
        "",
        "import json\nimport os\nimport subprocess\nimport tempfile\nimport os.path\nimport distutils.spawn, distutils.version\nimport numpy as np\nfrom io import StringIO\nfrom gym import error, logger\n\ndef touch(path):\n    open(path, 'a').close()\n\nclass VideoRecorder(object):\n    \"\"\"VideoRecorder renders a nice movie of a rollout, frame by frame. It\n    comes with an `enabled` option so you can still use the same code\n    on episodes where you don't want to record video.\n\n    Note:\n        You are responsible for calling `close` on a created\n        VideoRecorder, or else you may leak an encoder process.\n\n    Args:\n        env (Env): Environment to take video of.\n        path (Optional[str]): Path to the video file; will be randomly chosen if omitted.\n        base_path (Optional[str]): Alternatively, path to the video file without extension, which will be added.\n        metadata (Optional[dict]): Contents to save to the metadata file.\n        enabled (bool): Whether to actually record video, or just no-op (for convenience)\n    \"\"\"\n\n    def __init__(self, env, path=None, metadata=None, enabled=True, base_path=None):\n        modes = env.metadata.get('render.modes', [])\n        self._async = env.metadata.get('semantics.async')\n        self.enabled = enabled\n\n        # Don't bother setting anything else if not enabled\n        if not self.enabled:\n            return\n\n        self.ansi_mode = False\n        if 'rgb_array' not in modes:\n            if 'ansi' in modes:\n                self.ansi_mode = True\n            else:\n                logger.info('Disabling video recorder because {} neither supports video mode \"rgb_array\" nor \"ansi\".'.format(env))\n                # Whoops, turns out we shouldn't be enabled after all\n                self.enabled = False\n                return\n\n        if path is not None and base_path is not None:\n            raise error.Error(\"You can pass at most one of `path` or `base_path`.\")\n\n        self.last_frame = None\n        self.env = env\n\n        required_ext = '.json' if self.ansi_mode else '.mp4'\n        if path is None:\n            if base_path is not None:\n                # Base path given, append ext\n                path = base_path + required_ext\n            else:\n                # Otherwise, just generate a unique filename\n                with tempfile.NamedTemporaryFile(suffix=required_ext, delete=False) as f:\n                    path = f.name\n        self.path = path\n\n        path_base, actual_ext = os.path.splitext(self.path)\n\n        if actual_ext != required_ext:\n            hint = \" HINT: The environment is text-only, therefore we're recording its text output in a structured JSON format.\" if self.ansi_mode else ''\n            raise error.Error(\"Invalid path given: {} -- must have file extension {}.{}\".format(self.path, required_ext, hint))\n        # Touch the file in any case, so we know it's present. (This\n        # corrects for platform platform differences. Using ffmpeg on\n        # OS X, the file is precreated, but not on Linux.\n        touch(path)\n\n        self.frames_per_sec = env.metadata.get('video.frames_per_second', 30)\n        self.output_frames_per_sec = env.metadata.get('video.output_frames_per_second', self.frames_per_sec)\n        self.encoder = None # lazily start the process\n        self.broken = False\n\n        # Dump metadata\n        self.metadata = metadata or {}\n        self.metadata['content_type'] = 'video/vnd.openai.ansivid' if self.ansi_mode else 'video/mp4'\n        self.metadata_path = '{}.meta.json'.format(path_base)\n        self.write_metadata()\n\n        logger.info('Starting new video recorder writing to %s', self.path)\n        self.empty = True\n\n    @property\n    def functional(self):\n        return self.enabled and not self.broken\n\n    def capture_frame(self):\n        \"\"\"Render the given `env` and add the resulting frame to the video.\"\"\"\n        if not self.functional: return\n        logger.debug('Capturing video frame: path=%s', self.path)\n\n        render_mode = 'ansi' if self.ansi_mode else 'rgb_array'\n        frame = self.env.render(mode=render_mode)\n\n        if frame is None:\n            if self._async:\n                return\n            else:\n                # Indicates a bug in the environment: don't want to raise\n                # an error here.\n                logger.warn('Env returned None on render(). Disabling further rendering for video recorder by marking as disabled: path=%s metadata_path=%s', self.path, self.metadata_path)\n                self.broken = True\n        else:\n            self.last_frame = frame\n            if self.ansi_mode:\n                self._encode_ansi_frame(frame)\n            else:\n                self._encode_image_frame(frame)\n\n    def close(self):\n        \"\"\"Make sure to manually close, or else you'll leak the encoder process\"\"\"\n        if not self.enabled:\n            return\n\n        if self.encoder:\n            logger.debug('Closing video encoder: path=%s', self.path)\n            self.encoder.close()\n            self.encoder = None\n        else:\n            # No frames captured. Set metadata, and remove the empty output file.\n            os.remove(self.path)\n\n            if self.metadata is None:\n                self.metadata = {}\n            self.metadata['empty'] = True\n\n        # If broken, get rid of the output file, otherwise we'd leak it.\n        if self.broken:\n            logger.info('Cleaning up paths for broken video recorder: path=%s metadata_path=%s', self.path, self.metadata_path)\n\n            # Might have crashed before even starting the output file, don't try to remove in that case.\n            if os.path.exists(self.path):\n                os.remove(self.path)\n\n            if self.metadata is None:\n                self.metadata = {}\n            self.metadata['broken'] = True\n\n        self.write_metadata()\n\n    def write_metadata(self):\n        with open(self.metadata_path, 'w') as f:\n            json.dump(self.metadata, f)\n\n    def _encode_ansi_frame(self, frame):\n        if not self.encoder:\n            self.encoder = TextEncoder(self.path, self.frames_per_sec)\n            self.metadata['encoder_version'] = self.encoder.version_info\n        self.encoder.capture_frame(frame)\n        self.empty = False\n\n    def _encode_image_frame(self, frame):\n        if not self.encoder:\n            self.encoder = ImageEncoder(self.path, frame.shape, self.frames_per_sec, self.output_frames_per_sec)\n            self.metadata['encoder_version'] = self.encoder.version_info\n\n        try:\n            self.encoder.capture_frame(frame)\n        except error.InvalidFrame as e:\n            logger.warn('Tried to pass invalid video frame, marking as broken: %s', e)\n            self.broken = True\n        else:\n            self.empty = False\n\n\nclass TextEncoder(object):\n    \"\"\"Store a moving picture made out of ANSI frames. Format adapted from\n    https://github.com/asciinema/asciinema/blob/master/doc/asciicast-v1.md\"\"\"\n\n    def __init__(self, output_path, frames_per_sec):\n        self.output_path = output_path\n        self.frames_per_sec = frames_per_sec\n        self.frames = []\n\n    def capture_frame(self, frame):\n        string = None\n        if isinstance(frame, str):\n            string = frame\n        elif isinstance(frame, StringIO):\n            string = frame.getvalue()\n        else:\n            raise error.InvalidFrame('Wrong type {} for {}: text frame must be a string or StringIO'.format(type(frame), frame))\n\n        frame_bytes = string.encode('utf-8')\n\n        if frame_bytes[-1:] != b'\\n':\n            raise error.InvalidFrame('Frame must end with a newline: \"\"\"{}\"\"\"'.format(string))\n\n        if b'\\r' in frame_bytes:\n            raise error.InvalidFrame('Frame contains carriage returns (only newlines are allowed: \"\"\"{}\"\"\"'.format(string))\n\n        self.frames.append(frame_bytes)\n\n    def close(self):\n        #frame_duration = float(1) / self.frames_per_sec\n        frame_duration = .5\n\n        # Turn frames into events: clear screen beforehand\n        # https://rosettacode.org/wiki/Terminal_control/Clear_the_screen#Python\n        # https://rosettacode.org/wiki/Terminal_control/Cursor_positioning#Python\n        clear_code = b\"%c[2J\\033[1;1H\" % (27)\n        # Decode the bytes as UTF-8 since JSON may only contain UTF-8\n        events = [ (frame_duration, (clear_code+frame.replace(b'\\n', b'\\r\\n')).decode('utf-8'))  for frame in self.frames ]\n\n        # Calculate frame size from the largest frames.\n        # Add some padding since we'll get cut off otherwise.\n        height = max([frame.count(b'\\n') for frame in self.frames]) + 1\n        width = max([max([len(line) for line in frame.split(b'\\n')]) for frame in self.frames]) + 2\n\n        data = {\n            \"version\": 1,\n            \"width\": width,\n            \"height\": height,\n            \"duration\": len(self.frames)*frame_duration,\n            \"command\": \"-\",\n            \"title\": \"gym VideoRecorder episode\",\n            \"env\": {}, # could add some env metadata here\n            \"stdout\": events,\n        }\n\n        with open(self.output_path, 'w') as f:\n            json.dump(data, f)\n\n    @property\n    def version_info(self):\n        return {'backend':'TextEncoder','version':1}\n\nclass ImageEncoder(object):\n    def __init__(self, output_path, frame_shape, frames_per_sec, output_frames_per_sec):\n        self.proc = None\n        self.output_path = output_path\n        # Frame shape should be lines-first, so w and h are swapped\n        h, w, pixfmt = frame_shape\n        if pixfmt != 3 and pixfmt != 4:\n            raise error.InvalidFrame(\"Your frame has shape {}, but we require (w,h,3) or (w,h,4), i.e., RGB values for a w-by-h image, with an optional alpha channel.\".format(frame_shape))\n        self.wh = (w,h)\n        self.includes_alpha = (pixfmt == 4)\n        self.frame_shape = frame_shape\n        self.frames_per_sec = frames_per_sec\n        self.output_frames_per_sec = output_frames_per_sec\n\n        if distutils.spawn.find_executable('avconv') is not None:\n            self.backend = 'avconv'\n        elif distutils.spawn.find_executable('ffmpeg') is not None:\n            self.backend = 'ffmpeg'\n        else:\n            raise error.DependencyNotInstalled(\"\"\"Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\"\"\")\n\n        self.start()\n\n    @property\n    def version_info(self):\n        return {\n            'backend':self.backend,\n            'version':str(subprocess.check_output([self.backend, '-version'],\n                                                  stderr=subprocess.STDOUT)),\n            'cmdline':self.cmdline\n        }\n\n    def start(self):\n        self.cmdline = (self.backend,\n                     '-nostats',\n                     '-loglevel', 'error', # suppress warnings\n                     '-y',\n\n                     # input\n                     '-f', 'rawvideo',\n                     '-s:v', '{}x{}'.format(*self.wh),\n                     '-pix_fmt',('rgb32' if self.includes_alpha else 'rgb24'),\n                     '-framerate', '%d' % self.frames_per_sec,\n                     '-i', '-', # this used to be /dev/stdin, which is not Windows-friendly\n\n                     # output\n                     '-vf', 'scale=trunc(iw/2)*2:trunc(ih/2)*2',\n                     '-vcodec', 'libx264',\n                     '-pix_fmt', 'yuv420p',\n                     '-r', '%d' % self.output_frames_per_sec,\n                     self.output_path\n                     )\n\n        logger.debug('Starting ffmpeg with \"%s\"', ' '.join(self.cmdline))\n        if hasattr(os,'setsid'): #setsid not present on Windows\n            self.proc = subprocess.Popen(self.cmdline, stdin=subprocess.PIPE, preexec_fn=os.setsid)\n        else:\n            self.proc = subprocess.Popen(self.cmdline, stdin=subprocess.PIPE)\n\n    def capture_frame(self, frame):\n        if not isinstance(frame, (np.ndarray, np.generic)):\n            raise error.InvalidFrame('Wrong type {} for {} (must be np.ndarray or np.generic)'.format(type(frame), frame))\n        if frame.shape != self.frame_shape:\n            raise error.InvalidFrame(\"Your frame has shape {}, but the VideoRecorder is configured for shape {}.\".format(frame.shape, self.frame_shape))\n        if frame.dtype != np.uint8:\n            raise error.InvalidFrame(\"Your frame has data type {}, but we require uint8 (i.e. RGB values from 0-255).\".format(frame.dtype))\n\n        if distutils.version.LooseVersion(np.__version__) >= distutils.version.LooseVersion('1.9.0'):\n            self.proc.stdin.write(frame.tobytes())\n        else:\n            self.proc.stdin.write(frame.tostring())\n\n    def close(self):\n        self.proc.stdin.close()\n        ret = self.proc.wait()\n        if ret != 0:\n            logger.error(\"VideoRecorder encoder exited with status {}\".format(ret))",
        "import json\nimport os\nimport time\n\nfrom gym import error\nfrom gym.utils import atomic_write\nfrom gym.utils.json_utils import json_encode_np\n\nclass StatsRecorder(object):\n    def __init__(self, directory, file_prefix, autoreset=False, env_id=None):\n        self.autoreset = autoreset\n        self.env_id = env_id\n\n        self.initial_reset_timestamp = None\n        self.directory = directory\n        self.file_prefix = file_prefix\n        self.episode_lengths = []\n        self.episode_rewards = []\n        self.episode_types = [] # experimental addition\n        self._type = 't'\n        self.timestamps = []\n        self.steps = None\n        self.total_steps = 0\n        self.rewards = None\n\n        self.done = None\n        self.closed = False\n\n        filename = '{}.stats.json'.format(self.file_prefix)\n        self.path = os.path.join(self.directory, filename)\n\n    @property\n    def type(self):\n        return self._type\n\n    @type.setter\n    def type(self, type):\n        if type not in ['t', 'e']:\n            raise error.Error('Invalid episode type {}: must be t for training or e for evaluation', type)\n        self._type = type\n\n    def before_step(self, action):\n        assert not self.closed\n\n        if self.done:\n            raise error.ResetNeeded(\"Trying to step environment which is currently done. While the monitor is active for {}, you cannot step beyond the end of an episode. Call 'env.reset()' to start the next episode.\".format(self.env_id))\n        elif self.steps is None:\n            raise error.ResetNeeded(\"Trying to step an environment before reset. While the monitor is active for {}, you must call 'env.reset()' before taking an initial step.\".format(self.env_id))\n\n    def after_step(self, observation, reward, done, info):\n        self.steps += 1\n        self.total_steps += 1\n        self.rewards += reward\n        self.done = done\n\n        if done:\n            self.save_complete()\n\n        if done:\n            if self.autoreset:\n                self.before_reset()\n                self.after_reset(observation)\n\n    def before_reset(self):\n        assert not self.closed\n\n        if self.done is not None and not self.done and self.steps > 0:\n            raise error.Error(\"Tried to reset environment which is not done. While the monitor is active for {}, you cannot call reset() unless the episode is over.\".format(self.env_id))\n\n        self.done = False\n        if self.initial_reset_timestamp is None:\n            self.initial_reset_timestamp = time.time()\n\n    def after_reset(self, observation):\n        self.steps = 0\n        self.rewards = 0\n        # We write the type at the beginning of the episode. If a user\n        # changes the type, it's more natural for it to apply next\n        # time the user calls reset().\n        self.episode_types.append(self._type)\n\n    def save_complete(self):\n        if self.steps is not None:\n            self.episode_lengths.append(self.steps)\n            self.episode_rewards.append(float(self.rewards))\n            self.timestamps.append(time.time())\n\n    def close(self):\n        self.flush()\n        self.closed = True\n\n    def flush(self):\n        if self.closed:\n            return\n\n        with atomic_write.atomic_write(self.path) as f:\n            json.dump({\n                'initial_reset_timestamp': self.initial_reset_timestamp,\n                'timestamps': self.timestamps,\n                'episode_lengths': self.episode_lengths,\n                'episode_rewards': self.episode_rewards,\n                'episode_types': self.episode_types,\n            }, f, default=json_encode_np)",
        "import time\nfrom collections import deque\n\nimport gym\n\n\nclass RecordEpisodeStatistics(gym.Wrapper):\n    def __init__(self, env, deque_size=100):\n        super(RecordEpisodeStatistics, self).__init__(env)\n        self.t0 = time.time()  # TODO: use perf_counter when gym removes Python 2 support\n        self.episode_return = 0.0\n        self.episode_length = 0\n        self.return_queue = deque(maxlen=deque_size)\n        self.length_queue = deque(maxlen=deque_size)\n\n    def reset(self, **kwargs):\n        observation = super(RecordEpisodeStatistics, self).reset(**kwargs)\n        self.episode_return = 0.0\n        self.episode_length = 0\n        return observation\n\n    def step(self, action):\n        observation, reward, done, info = super(RecordEpisodeStatistics, self).step(action)\n        self.episode_return += reward\n        self.episode_length += 1\n        if done:\n            info['episode'] = {'r': self.episode_return, \n                               'l': self.episode_length, \n                               't': round(time.time() - self.t0, 6)}\n            self.return_queue.append(self.episode_return)\n            self.length_queue.append(self.episode_length)\n            self.episode_return = 0.0\n            self.episode_length = 0\n        return observation, reward, done, info",
        "import pytest\nimport numpy as np\n\nimport gym\nfrom gym import spaces\nfrom gym.wrappers.filter_observation import FilterObservation\n\n\nclass FakeEnvironment(gym.Env):\n    def __init__(self, observation_keys=('state')):\n        self.observation_space = spaces.Dict({\n            name: spaces.Box(shape=(2, ), low=-1, high=1, dtype=np.float32)\n            for name in observation_keys\n        })\n        self.action_space = spaces.Box(\n            shape=(1, ), low=-1, high=1, dtype=np.float32)\n\n    def render(self, width=32, height=32, *args, **kwargs):\n        del args\n        del kwargs\n        image_shape = (height, width, 3)\n        return np.zeros(image_shape, dtype=np.uint8)\n\n    def reset(self):\n        observation = self.observation_space.sample()\n        return observation\n\n    def step(self, action):\n        del action\n        observation = self.observation_space.sample()\n        reward, terminal, info = 0.0, False, {}\n        return observation, reward, terminal, info\n\n\nFILTER_OBSERVATION_TEST_CASES = (\n    (('key1', 'key2'), ('key1', )),\n    (('key1', 'key2'), ('key1', 'key2')),\n    (('key1', ), None),\n    (('key1', ), ('key1', )),\n)\n\nERROR_TEST_CASES = (\n    ('key', ValueError, \"All the filter_keys must be included..*\"),\n    (False, TypeError, \"'bool' object is not iterable\"),\n    (1, TypeError, \"'int' object is not iterable\"),\n)\n\n\nclass TestFilterObservation(object):\n    @pytest.mark.parametrize(\"observation_keys,filter_keys\",\n                             FILTER_OBSERVATION_TEST_CASES)\n    def test_filter_observation(self, observation_keys, filter_keys):\n        env = FakeEnvironment(observation_keys=observation_keys)\n\n        # Make sure we are testing the right environment for the test.\n        observation_space = env.observation_space\n        assert isinstance(observation_space, spaces.Dict)\n\n        wrapped_env = FilterObservation(env, filter_keys=filter_keys)\n\n        assert isinstance(wrapped_env.observation_space, spaces.Dict)\n\n        if filter_keys is None:\n            filter_keys = tuple(observation_keys)\n\n        assert len(wrapped_env.observation_space.spaces) == len(filter_keys)\n        assert (tuple(wrapped_env.observation_space.spaces.keys())\n                == tuple(filter_keys))\n\n        # Check that the added space item is consistent with the added observation.\n        observation = wrapped_env.reset()\n        assert (len(observation) == len(filter_keys))\n\n    @pytest.mark.parametrize(\"filter_keys,error_type,error_match\",\n                             ERROR_TEST_CASES)\n    def test_raises_with_incorrect_arguments(self,\n                                             filter_keys,\n                                             error_type,\n                                             error_match):\n        env = FakeEnvironment(observation_keys=('key1', 'key2'))\n\n        ValueError\n\n        with pytest.raises(error_type, match=error_match):\n            FilterObservation(env, filter_keys=filter_keys)",
        "import numpy as np\nimport gym\nfrom gym.wrappers import AtariPreprocessing\nimport pytest\n\npytest.importorskip('atari_py')\n\n\n@pytest.fixture(scope='module')\ndef env_fn():\n    return lambda: gym.make('PongNoFrameskip-v4')\n\n\ndef test_atari_preprocessing_grayscale(env_fn):\n    import cv2\n    env1 = env_fn()\n    env2 = AtariPreprocessing(env_fn(), screen_size=84, grayscale_obs=True, frame_skip=1, noop_max=0)\n    env3 = AtariPreprocessing(env_fn(), screen_size=84, grayscale_obs=False, frame_skip=1, noop_max=0)\n    env1.seed(0)\n    env2.seed(0)\n    env3.seed(0)\n    obs1 = env1.reset()\n    obs2 = env2.reset()\n    obs3 = env3.reset()\n    assert obs1.shape == (210, 160, 3)\n    assert obs2.shape == (84, 84)\n    assert obs3.shape == (84, 84, 3)\n    assert np.allclose(obs3, cv2.resize(obs1, (84, 84), interpolation=cv2.INTER_AREA))\n    obs3_gray = cv2.cvtColor(obs3, cv2.COLOR_RGB2GRAY)\n    # the edges of the numbers do not render quite the same in the grayscale, so we ignore them\n    assert np.allclose(obs2[10:38], obs3_gray[10:38])\n    # the paddle also do not render quite the same\n    assert np.allclose(obs2[44:], obs3_gray[44:])\n\n    env1.close()\n    env2.close()\n    env3.close()\n\n\ndef test_atari_preprocessing_scale(env_fn):\n    # arbitrarily chosen number for stepping into env. and ensuring all observations are in the required range\n    max_test_steps = 10\n\n    for grayscale in [True, False]:\n        for scaled in [True, False]:\n            env = AtariPreprocessing(env_fn(), screen_size=84, grayscale_obs=grayscale, scale_obs=scaled,\n                                     frame_skip=1, noop_max=0)\n            obs = env.reset().flatten()\n            done, step_i = False, 0\n            max_obs = 1 if scaled else 255\n            assert (0 <= obs).all() and (obs <= max_obs).all(), 'Obs. must be in range [0,{}]'.format(max_obs)\n            while not done or step_i <= max_test_steps:\n                obs, _, done, _ = env.step(env.action_space.sample())\n                obs = obs.flatten()\n                assert (0 <= obs).all() and (obs <= max_obs).all(), 'Obs. must be in range [0,{}]'.format(max_obs)\n                step_i += 1\n\n            env.close()",
        "VERSION = '0.17.2'",
        "import sys\n\nclass Error(Exception):\n    pass\n\n# Local errors\n\nclass Unregistered(Error):\n    \"\"\"Raised when the user requests an item from the registry that does\n    not actually exist.\n    \"\"\"\n    pass\n\nclass UnregisteredEnv(Unregistered):\n    \"\"\"Raised when the user requests an env from the registry that does\n    not actually exist.\n    \"\"\"\n    pass\n\nclass UnregisteredBenchmark(Unregistered):\n    \"\"\"Raised when the user requests an env from the registry that does\n    not actually exist.\n    \"\"\"\n    pass\n\nclass DeprecatedEnv(Error):\n    \"\"\"Raised when the user requests an env from the registry with an\n    older version number than the latest env with the same name.\n    \"\"\"\n    pass\n\nclass UnseedableEnv(Error):\n    \"\"\"Raised when the user tries to seed an env that does not support\n    seeding.\n    \"\"\"\n    pass\n\nclass DependencyNotInstalled(Error):\n    pass\n\nclass UnsupportedMode(Exception):\n    \"\"\"Raised when the user requests a rendering mode not supported by the\n    environment.\n    \"\"\"\n    pass\n\nclass ResetNeeded(Exception):\n    \"\"\"When the monitor is active, raised when the user tries to step an\n    environment that's already done.\n    \"\"\"\n    pass\n\nclass ResetNotAllowed(Exception):\n    \"\"\"When the monitor is active, raised when the user tries to step an\n    environment that's not yet done.\n    \"\"\"\n    pass\n\nclass InvalidAction(Exception):\n    \"\"\"Raised when the user performs an action not contained within the\n    action space\n    \"\"\"\n    pass\n\n# API errors\n\nclass APIError(Error):\n    def __init__(self, message=None, http_body=None, http_status=None,\n                 json_body=None, headers=None):\n        super(APIError, self).__init__(message)\n\n        if http_body and hasattr(http_body, 'decode'):\n            try:\n                http_body = http_body.decode('utf-8')\n            except:\n                http_body = ('<Could not decode body as utf-8. '\n                             'Please report to gym@openai.com>')\n\n        self._message = message\n        self.http_body = http_body\n        self.http_status = http_status\n        self.json_body = json_body\n        self.headers = headers or {}\n        self.request_id = self.headers.get('request-id', None)\n\n    def __unicode__(self):\n        if self.request_id is not None:\n            msg = self._message or \"<empty message>\"\n            return u\"Request {0}: {1}\".format(self.request_id, msg)\n        else:\n            return self._message\n\n    def __str__(self):\n        try:               # Python 2\n            return unicode(self).encode('utf-8')\n        except NameError:  # Python 3\n            return self.__unicode__()\n\n\nclass APIConnectionError(APIError):\n    pass\n\n\nclass InvalidRequestError(APIError):\n\n    def __init__(self, message, param, http_body=None,\n                 http_status=None, json_body=None, headers=None):\n        super(InvalidRequestError, self).__init__(\n            message, http_body, http_status, json_body,\n            headers)\n        self.param = param\n\n\nclass AuthenticationError(APIError):\n    pass\n\nclass RateLimitError(APIError):\n    pass\n\n# Video errors\n\nclass VideoRecorderError(Error):\n    pass\n\nclass InvalidFrame(Error):\n    pass\n\n# Wrapper errors\n\nclass DoubleWrapperError(Error):\n    pass\n\n\nclass WrapAfterConfigureError(Error):\n    pass\n\n\nclass RetriesExceededError(Error):\n    pass\n\n# Vectorized environments errors\n\nclass AlreadyPendingCallError(Exception):\n    \"\"\"\n    Raised when `reset`, or `step` is called asynchronously (e.g. with\n    `reset_async`, or `step_async` respectively), and `reset_async`, or\n    `step_async` (respectively) is called again (without a complete call to\n    `reset_wait`, or `step_wait` respectively).\n    \"\"\"\n    def __init__(self, message, name):\n        super(AlreadyPendingCallError, self).__init__(message)\n        self.name = name\n\nclass NoAsyncCallError(Exception):\n    \"\"\"\n    Raised when an asynchronous `reset`, or `step` is not running, but\n    `reset_wait`, or `step_wait` (respectively) is called.\n    \"\"\"\n    def __init__(self, message, name):\n        super(NoAsyncCallError, self).__init__(message)\n        self.name = name\n\nclass ClosedEnvironmentError(Exception):\n    \"\"\"\n    Trying to call `reset`, or `step`, while the environment is closed.\n    \"\"\"\n    pass",
        "import numpy as np\nfrom .space import Space\n\n\nclass Tuple(Space):\n    \"\"\"\n    A tuple (i.e., product) of simpler spaces\n\n    Example usage:\n    self.observation_space = spaces.Tuple((spaces.Discrete(2), spaces.Discrete(3)))\n    \"\"\"\n    def __init__(self, spaces):\n        self.spaces = spaces\n        for space in spaces:\n            assert isinstance(space, Space), \"Elements of the tuple must be instances of gym.Space\"\n        super(Tuple, self).__init__(None, None)\n\n    def seed(self, seed=None):\n        [space.seed(seed) for space in self.spaces]\n\n    def sample(self):\n        return tuple([space.sample() for space in self.spaces])\n\n    def contains(self, x):\n        if isinstance(x, list):\n            x = tuple(x)  # Promote list to tuple for contains check\n        return isinstance(x, tuple) and len(x) == len(self.spaces) and all(\n            space.contains(part) for (space,part) in zip(self.spaces,x))\n\n    def __repr__(self):\n        return \"Tuple(\" + \", \". join([str(s) for s in self.spaces]) + \")\"\n\n    def to_jsonable(self, sample_n):\n        # serialize as list-repr of tuple of vectors\n        return [space.to_jsonable([sample[i] for sample in sample_n]) \\\n                for i, space in enumerate(self.spaces)]\n\n    def from_jsonable(self, sample_n):\n        return [sample for sample in zip(*[space.from_jsonable(sample_n[i]) for i, space in enumerate(self.spaces)])]\n\n    def __getitem__(self, index):\n        return self.spaces[index]\n\n    def __len__(self):\n        return len(self.spaces)\n      \n    def __eq__(self, other):\n        return isinstance(other, Tuple) and self.spaces == other.spaces",
        "import numpy as np\n\nfrom .space import Space\nfrom gym import logger\n\n\nclass Box(Space):\n    \"\"\"\n    A (possibly unbounded) box in R^n. Specifically, a Box represents the\n    Cartesian product of n closed intervals. Each interval has the form of one\n    of [a, b], (-oo, b], [a, oo), or (-oo, oo).\n\n    There are two common use cases:\n\n    * Identical bound for each dimension::\n        >>> Box(low=-1.0, high=2.0, shape=(3, 4), dtype=np.float32)\n        Box(3, 4)\n\n    * Independent bound for each dimension::\n        >>> Box(low=np.array([-1.0, -2.0]), high=np.array([2.0, 4.0]), dtype=np.float32)\n        Box(2,)\n\n    \"\"\"\n    def __init__(self, low, high, shape=None, dtype=np.float32):\n        assert dtype is not None, 'dtype must be explicitly provided. '\n        self.dtype = np.dtype(dtype)\n\n        # determine shape if it isn't provided directly\n        if shape is not None:\n            shape = tuple(shape)\n            assert np.isscalar(low) or low.shape == shape, \"low.shape doesn't match provided shape\"\n            assert np.isscalar(high) or high.shape == shape, \"high.shape doesn't match provided shape\"\n        elif not np.isscalar(low):\n            shape = low.shape\n            assert np.isscalar(high) or high.shape == shape, \"high.shape doesn't match low.shape\"\n        elif not np.isscalar(high):\n            shape = high.shape\n            assert np.isscalar(low) or low.shape == shape, \"low.shape doesn't match high.shape\"\n        else:\n            raise ValueError(\"shape must be provided or inferred from the shapes of low or high\")\n\n        if np.isscalar(low):\n            low = np.full(shape, low, dtype=dtype)\n\n        if np.isscalar(high):\n            high = np.full(shape, high, dtype=dtype)\n\n        self.shape = shape\n        self.low = low\n        self.high = high\n\n        def _get_precision(dtype):\n            if np.issubdtype(dtype, np.floating):\n                return np.finfo(dtype).precision\n            else:\n                return np.inf\n        low_precision = _get_precision(self.low.dtype)\n        high_precision = _get_precision(self.high.dtype)\n        dtype_precision = _get_precision(self.dtype)\n        if min(low_precision, high_precision) > dtype_precision:\n            logger.warn(\"Box bound precision lowered by casting to {}\".format(self.dtype))\n        self.low = self.low.astype(self.dtype)\n        self.high = self.high.astype(self.dtype)\n\n        # Boolean arrays which indicate the interval type for each coordinate\n        self.bounded_below = -np.inf < self.low\n        self.bounded_above = np.inf > self.high\n\n        super(Box, self).__init__(self.shape, self.dtype)\n\n    def is_bounded(self, manner=\"both\"):\n        below = np.all(self.bounded_below)\n        above = np.all(self.bounded_above)\n        if manner == \"both\":\n            return below and above\n        elif manner == \"below\":\n            return below\n        elif manner == \"above\":\n            return above\n        else:\n            raise ValueError(\"manner is not in {'below', 'above', 'both'}\")\n\n    def sample(self):\n        \"\"\"\n        Generates a single random sample inside of the Box.\n\n        In creating a sample of the box, each coordinate is sampled according to\n        the form of the interval:\n\n        * [a, b] : uniform distribution\n        * [a, oo) : shifted exponential distribution\n        * (-oo, b] : shifted negative exponential distribution\n        * (-oo, oo) : normal distribution\n        \"\"\"\n        high = self.high if self.dtype.kind == 'f' \\\n                else self.high.astype('int64') + 1\n        sample = np.empty(self.shape)\n\n        # Masking arrays which classify the coordinates according to interval\n        # type\n        unbounded   = ~self.bounded_below & ~self.bounded_above\n        upp_bounded = ~self.bounded_below &  self.bounded_above\n        low_bounded =  self.bounded_below & ~self.bounded_above\n        bounded     =  self.bounded_below &  self.bounded_above\n\n\n        # Vectorized sampling by interval type\n        sample[unbounded] = self.np_random.normal(\n                size=unbounded[unbounded].shape)\n\n        sample[low_bounded] = self.np_random.exponential(\n            size=low_bounded[low_bounded].shape) + self.low[low_bounded]\n\n        sample[upp_bounded] = -self.np_random.exponential(\n            size=upp_bounded[upp_bounded].shape) + self.high[upp_bounded]\n\n        sample[bounded] = self.np_random.uniform(low=self.low[bounded],\n                                            high=high[bounded],\n                                            size=bounded[bounded].shape)\n        if self.dtype.kind == 'i':\n            sample = np.floor(sample)\n\n        return sample.astype(self.dtype)\n\n    def contains(self, x):\n        if isinstance(x, list):\n            x = np.array(x)  # Promote list to array for contains check\n        return x.shape == self.shape and np.all(x >= self.low) and np.all(x <= self.high)\n\n    def to_jsonable(self, sample_n):\n        return np.array(sample_n).tolist()\n\n    def from_jsonable(self, sample_n):\n        return [np.asarray(sample) for sample in sample_n]\n\n    def __repr__(self):\n        return \"Box\" + str(self.shape)\n\n    def __eq__(self, other):\n        return isinstance(other, Box) and (self.shape == other.shape) and np.allclose(self.low, other.low) and np.allclose(self.high, other.high)",
        "import numpy as np\nfrom .space import Space\n\n\nclass MultiDiscrete(Space):\n    \"\"\"\n    - The multi-discrete action space consists of a series of discrete action spaces with different number of actions in eachs\n    - It is useful to represent game controllers or keyboards where each key can be represented as a discrete action space\n    - It is parametrized by passing an array of positive integers specifying number of actions for each discrete action space\n\n    Note: Some environment wrappers assume a value of 0 always represents the NOOP action.\n\n    e.g. Nintendo Game Controller\n    - Can be conceptualized as 3 discrete action spaces:\n\n        1) Arrow Keys: Discrete 5  - NOOP[0], UP[1], RIGHT[2], DOWN[3], LEFT[4]  - params: min: 0, max: 4\n        2) Button A:   Discrete 2  - NOOP[0], Pressed[1] - params: min: 0, max: 1\n        3) Button B:   Discrete 2  - NOOP[0], Pressed[1] - params: min: 0, max: 1\n\n    - Can be initialized as\n\n        MultiDiscrete([ 5, 2, 2 ])\n\n    \"\"\"\n    def __init__(self, nvec):\n\n        \"\"\"\n        nvec: vector of counts of each categorical variable\n        \"\"\"\n        assert (np.array(nvec) > 0).all(), 'nvec (counts) have to be positive'\n        self.nvec = np.asarray(nvec, dtype=np.int64)\n\n        super(MultiDiscrete, self).__init__(self.nvec.shape, np.int64)\n\n    def sample(self):\n        return (self.np_random.random_sample(self.nvec.shape)*self.nvec).astype(self.dtype)\n\n    def contains(self, x):\n        if isinstance(x, list):\n            x = np.array(x)  # Promote list to array for contains check\n        # if nvec is uint32 and space dtype is uint32, then 0 <= x < self.nvec guarantees that x\n        # is within correct bounds for space dtype (even though x does not have to be unsigned)\n        return x.shape == self.shape and (0 <= x).all() and (x < self.nvec).all()\n\n    def to_jsonable(self, sample_n):\n        return [sample.tolist() for sample in sample_n]\n\n    def from_jsonable(self, sample_n):\n        return np.array(sample_n)\n\n    def __repr__(self):\n        return \"MultiDiscrete({})\".format(self.nvec)\n\n    def __eq__(self, other):\n        return isinstance(other, MultiDiscrete) and np.all(self.nvec == other.nvec)",
        "from collections import OrderedDict\nimport numpy as np\nimport pytest\n\nfrom gym.spaces import utils\nfrom gym.spaces import Tuple, Box, Discrete, MultiDiscrete, MultiBinary, Dict\n\n\n@pytest.mark.parametrize([\"space\", \"flatdim\"], [\n    (Discrete(3), 3),\n    (Box(low=0., high=np.inf, shape=(2, 2)), 4),\n    (Tuple([Discrete(5), Discrete(10)]), 15),\n    (Tuple([Discrete(5), Box(low=np.array([0, 0]), high=np.array([1, 5]), dtype=np.float32)]), 7),\n    (Tuple((Discrete(5), Discrete(2), Discrete(2))), 9),\n    (MultiDiscrete([2, 2, 100]), 3),\n    (MultiBinary(10), 10),\n    (Dict({\"position\": Discrete(5),\n           \"velocity\": Box(low=np.array([0, 0]), high=np.array([1, 5]), dtype=np.float32)}), 7),\n])\ndef test_flatdim(space, flatdim):\n    dim = utils.flatdim(space)\n    assert dim == flatdim, \"Expected {} to equal {}\".format(dim, flatdim)\n\n\n@pytest.mark.parametrize(\"space\", [\n    Discrete(3),\n    Box(low=0., high=np.inf, shape=(2, 2)),\n    Tuple([Discrete(5), Discrete(10)]),\n    Tuple([Discrete(5), Box(low=np.array([0, 0]), high=np.array([1, 5]), dtype=np.float32)]),\n    Tuple((Discrete(5), Discrete(2), Discrete(2))),\n    MultiDiscrete([2, 2, 100]),\n    MultiBinary(10),\n    Dict({\"position\": Discrete(5),\n          \"velocity\": Box(low=np.array([0, 0]), high=np.array([1, 5]), dtype=np.float32)}),\n    ])\ndef test_flatten_space_boxes(space):\n    flat_space = utils.flatten_space(space)\n    assert isinstance(flat_space, Box), \"Expected {} to equal {}\".format(type(flat_space), Box)\n    flatdim = utils.flatdim(space)\n    (single_dim, ) = flat_space.shape\n    assert single_dim == flatdim, \"Expected {} to equal {}\".format(single_dim, flatdim)\n\n\n@pytest.mark.parametrize(\"space\", [\n    Discrete(3),\n    Box(low=0., high=np.inf, shape=(2, 2)),\n    Tuple([Discrete(5), Discrete(10)]),\n    Tuple([Discrete(5), Box(low=np.array([0, 0]), high=np.array([1, 5]), dtype=np.float32)]),\n    Tuple((Discrete(5), Discrete(2), Discrete(2))),\n    MultiDiscrete([2, 2, 100]),\n    MultiBinary(10),\n    Dict({\"position\": Discrete(5),\n          \"velocity\": Box(low=np.array([0, 0]), high=np.array([1, 5]), dtype=np.float32)}),\n    ])\ndef test_flat_space_contains_flat_points(space):\n    some_samples = [space.sample() for _ in range(10)]\n    flattened_samples = [utils.flatten(space, sample) for sample in some_samples]\n    flat_space = utils.flatten_space(space)\n    for i, flat_sample in enumerate(flattened_samples):\n        assert flat_sample in flat_space,\\\n            'Expected sample #{} {} to be in {}'.format(i, flat_sample, flat_space)\n\n\n@pytest.mark.parametrize(\"space\", [\n    Discrete(3),\n    Box(low=0., high=np.inf, shape=(2, 2)),\n    Tuple([Discrete(5), Discrete(10)]),\n    Tuple([Discrete(5), Box(low=np.array([0, 0]), high=np.array([1, 5]), dtype=np.float32)]),\n    Tuple((Discrete(5), Discrete(2), Discrete(2))),\n    MultiDiscrete([2, 2, 100]),\n    MultiBinary(10),\n    Dict({\"position\": Discrete(5),\n          \"velocity\": Box(low=np.array([0, 0]), high=np.array([1, 5]), dtype=np.float32)}),\n    ])\ndef test_flatten_dim(space):\n    sample = utils.flatten(space, space.sample())\n    (single_dim, ) = sample.shape\n    flatdim = utils.flatdim(space)\n    assert single_dim == flatdim, \"Expected {} to equal {}\".format(single_dim, flatdim)\n\n\n@pytest.mark.parametrize(\"space\", [\n    Discrete(3),\n    Box(low=0., high=np.inf, shape=(2, 2)),\n    Tuple([Discrete(5), Discrete(10)]),\n    Tuple([Discrete(5), Box(low=np.array([0, 0]), high=np.array([1, 5]), dtype=np.float32)]),\n    Tuple((Discrete(5), Discrete(2), Discrete(2))),\n    MultiDiscrete([2, 2, 100]),\n    MultiBinary(10),\n    Dict({\"position\": Discrete(5),\n          \"velocity\": Box(low=np.array([0, 0]), high=np.array([1, 5]), dtype=np.float32)}),\n])\ndef test_flatten_roundtripping(space):\n    some_samples = [space.sample() for _ in range(10)]\n    flattened_samples = [utils.flatten(space, sample) for sample in some_samples]\n    roundtripped_samples = [utils.unflatten(space, sample) for sample in flattened_samples]\n    for i, (original, roundtripped) in enumerate(zip(some_samples, roundtripped_samples)):\n        assert compare_nested(original, roundtripped), \\\n            'Expected sample #{} {} to equal {}'.format(i, original, roundtripped)\n\n\ndef compare_nested(left, right):\n    if isinstance(left, np.ndarray) and isinstance(right, np.ndarray):\n        return np.allclose(left, right)\n    elif isinstance(left, OrderedDict) and isinstance(right, OrderedDict):\n        res = len(left) == len(right)\n        for ((left_key, left_value), (right_key, right_value)) in zip(left.items(), right.items()):\n            if not res:\n                return False\n            res = left_key == right_key and compare_nested(left_value, right_value)\n        return res\n    elif isinstance(left, (tuple, list)) and isinstance(right, (tuple, list)):\n        res = len(left) == len(right)\n        for (x, y) in zip(left, right):\n            if not res:\n                return False\n            res = compare_nested(x, y)\n        return res\n    else:\n        return left == right",
        "",
        "import json  # note: ujson fails this test due to float equality\nfrom copy import copy\n\nimport numpy as np\nimport pytest\n\nfrom gym.spaces import Tuple, Box, Discrete, MultiDiscrete, MultiBinary, Dict\n\n\n@pytest.mark.parametrize(\"space\", [\n    Discrete(3),\n    Box(low=0., high=np.inf, shape=(2,2)),\n    Tuple([Discrete(5), Discrete(10)]),\n    Tuple([Discrete(5), Box(low=np.array([0, 0]), high=np.array([1, 5]), dtype=np.float32)]),\n    Tuple((Discrete(5), Discrete(2), Discrete(2))),\n    MultiDiscrete([2, 2, 100]),\n    MultiBinary(10),\n    Dict({\"position\": Discrete(5),\n          \"velocity\": Box(low=np.array([0, 0]), high=np.array([1, 5]), dtype=np.float32)}),\n])\ndef test_roundtripping(space):\n    sample_1 = space.sample()\n    sample_2 = space.sample()\n    assert space.contains(sample_1)\n    assert space.contains(sample_2)\n    json_rep = space.to_jsonable([sample_1, sample_2])\n\n    json_roundtripped = json.loads(json.dumps(json_rep))\n\n    samples_after_roundtrip = space.from_jsonable(json_roundtripped)\n    sample_1_prime, sample_2_prime = samples_after_roundtrip\n\n    s1 = space.to_jsonable([sample_1])\n    s1p = space.to_jsonable([sample_1_prime])\n    s2 = space.to_jsonable([sample_2])\n    s2p = space.to_jsonable([sample_2_prime])\n    assert s1 == s1p, \"Expected {} to equal {}\".format(s1, s1p)\n    assert s2 == s2p, \"Expected {} to equal {}\".format(s2, s2p)\n\n\n@pytest.mark.parametrize(\"space\", [\n    Discrete(3),\n    Box(low=np.array([-10, 0]), high=np.array([10, 10]), dtype=np.float32),\n    Box(low=-np.inf, high=np.inf, shape=(1,3)),\n    Tuple([Discrete(5), Discrete(10)]),\n    Tuple([Discrete(5), Box(low=np.array([0, 0]), high=np.array([1, 5]), dtype=np.float32)]),\n    Tuple((Discrete(5), Discrete(2), Discrete(2))),\n    MultiDiscrete([2, 2, 100]),\n    MultiBinary(6),\n    Dict({\"position\": Discrete(5),\n          \"velocity\": Box(low=np.array([0, 0]), high=np.array([1, 5]), dtype=np.float32)}),\n])\ndef test_equality(space):\n    space1 = space\n    space2 = copy(space)\n    assert space1 == space2, \"Expected {} to equal {}\".format(space1, space2)\n\n\n@pytest.mark.parametrize(\"spaces\", [\n    (Discrete(3), Discrete(4)),\n    (MultiDiscrete([2, 2, 100]), MultiDiscrete([2, 2, 8])),\n    (MultiBinary(8), MultiBinary(7)),\n    (Box(low=np.array([-10, 0]), high=np.array([10, 10]), dtype=np.float32),\n     Box(low=np.array([-10, 0]), high=np.array([10, 9]), dtype=np.float32)),\n    (Box(low=-np.inf,high=0., shape=(2,1)), \n        Box(low=0., high=np.inf, shape=(2,1))),\n    (Tuple([Discrete(5), Discrete(10)]), Tuple([Discrete(1), Discrete(10)])),\n    (Dict({\"position\": Discrete(5)}), Dict({\"position\": Discrete(4)})),\n    (Dict({\"position\": Discrete(5)}), Dict({\"speed\": Discrete(5)})),\n])\ndef test_inequality(spaces):\n    space1, space2 = spaces\n    assert space1 != space2, \"Expected {} != {}\".format(space1, space2)\n\n\n@pytest.mark.parametrize(\"space\", [\n    Discrete(5),\n    Box(low=0, high=255, shape=(2,), dtype='uint8'),\n    Box(low=-np.inf, high=np.inf, shape=(3,3)),\n    Box(low=1., high=np.inf, shape=(3,3)),\n    Box(low=-np.inf, high=2., shape=(3,3)),\n])\ndef test_sample(space):\n    space.seed(0)\n    n_trials = 100\n    samples = np.array([space.sample() for _ in range(n_trials)])\n    expected_mean = 0.\n    if isinstance(space, Box):\n        if space.is_bounded():\n            expected_mean = (space.high + space.low) / 2\n        elif space.is_bounded(\"below\"):\n            expected_mean = 1 + space.low\n        elif space.is_bounded(\"above\"):\n            expected_mean = -1 + space.high\n        else:\n            expected_mean = 0.\n    elif isinstance(space, Discrete):\n        expected_mean = space.n / 2\n    else:\n        raise NotImplementedError\n    np.testing.assert_allclose(expected_mean, samples.mean(), atol=3.0 * samples.std())\n\n@pytest.mark.parametrize(\"spaces\", [\n    (Discrete(5), MultiBinary(5)),\n    (Box(low=np.array([-10, 0]), high=np.array([10,10]), dtype=np.float32), MultiDiscrete([2, 2, 8])),\n    (Box(low=0, high=255, shape=(64, 64, 3), dtype=np.uint8), Box(low=0, high=255, shape=(32, 32, 3), dtype=np.uint8)),\n    (Dict({\"position\": Discrete(5)}), Tuple([Discrete(5)])),\n    (Dict({\"position\": Discrete(5)}), Discrete(5)),\n    (Tuple((Discrete(5),)), Discrete(5)),\n    (Box(low=np.array([-np.inf,0.]), high=np.array([0., np.inf])),\n        Box(low=np.array([-np.inf, 1.]), high=np.array([0., np.inf])))\n])\ndef test_class_inequality(spaces):\n    assert spaces[0] == spaces[0]\n    assert spaces[1] == spaces[1]\n    assert spaces[0] != spaces[1]\n    assert spaces[1] != spaces[0]\n\n\n@pytest.mark.parametrize(\"space_fn\", [\n    lambda: Dict(space1='abc'),\n    lambda: Dict({'space1': 'abc'}),\n    lambda: Tuple(['abc'])\n])\ndef test_bad_space_calls(space_fn):\n    with pytest.raises(AssertionError):\n        space_fn()",
        "import numpy as np\nfrom .space import Space\n\n\nclass Discrete(Space):\n    r\"\"\"A discrete space in :math:`\\{ 0, 1, \\\\dots, n-1 \\}`. \n\n    Example::\n\n        >>> Discrete(2)\n\n    \"\"\"\n    def __init__(self, n):\n        assert n >= 0\n        self.n = n\n        super(Discrete, self).__init__((), np.int64)\n\n    def sample(self):\n        return self.np_random.randint(self.n)\n\n    def contains(self, x):\n        if isinstance(x, int):\n            as_int = x\n        elif isinstance(x, (np.generic, np.ndarray)) and (x.dtype.char in np.typecodes['AllInteger'] and x.shape == ()):\n            as_int = int(x)\n        else:\n            return False\n        return as_int >= 0 and as_int < self.n\n\n    def __repr__(self):\n        return \"Discrete(%d)\" % self.n\n\n    def __eq__(self, other):\n        return isinstance(other, Discrete) and self.n == other.n",
        "from gym.spaces.space import Space\nfrom gym.spaces.box import Box\nfrom gym.spaces.discrete import Discrete\nfrom gym.spaces.multi_discrete import MultiDiscrete\nfrom gym.spaces.multi_binary import MultiBinary\nfrom gym.spaces.tuple import Tuple\nfrom gym.spaces.dict import Dict\n\nfrom gym.spaces.utils import flatdim\nfrom gym.spaces.utils import flatten_space\nfrom gym.spaces.utils import flatten\nfrom gym.spaces.utils import unflatten\n\n__all__ = [\"Space\", \"Box\", \"Discrete\", \"MultiDiscrete\", \"MultiBinary\", \"Tuple\", \"Dict\", \"flatdim\", \"flatten_space\", \"flatten\", \"unflatten\"]",
        "from collections import OrderedDict\nimport numpy as np\n\nfrom gym.spaces import Box\nfrom gym.spaces import Discrete\nfrom gym.spaces import MultiDiscrete\nfrom gym.spaces import MultiBinary\nfrom gym.spaces import Tuple\nfrom gym.spaces import Dict\n\n\ndef flatdim(space):\n    \"\"\"Return the number of dimensions a flattened equivalent of this space\n    would have.\n\n    Accepts a space and returns an integer. Raises ``NotImplementedError`` if\n    the space is not defined in ``gym.spaces``.\n    \"\"\"\n    if isinstance(space, Box):\n        return int(np.prod(space.shape))\n    elif isinstance(space, Discrete):\n        return int(space.n)\n    elif isinstance(space, Tuple):\n        return int(sum([flatdim(s) for s in space.spaces]))\n    elif isinstance(space, Dict):\n        return int(sum([flatdim(s) for s in space.spaces.values()]))\n    elif isinstance(space, MultiBinary):\n        return int(space.n)\n    elif isinstance(space, MultiDiscrete):\n        return int(np.prod(space.shape))\n    else:\n        raise NotImplementedError\n\n\ndef flatten(space, x):\n    \"\"\"Flatten a data point from a space.\n\n    This is useful when e.g. points from spaces must be passed to a neural\n    network, which only understands flat arrays of floats.\n\n    Accepts a space and a point from that space. Always returns a 1D array.\n    Raises ``NotImplementedError`` if the space is not defined in\n    ``gym.spaces``.\n    \"\"\"\n    if isinstance(space, Box):\n        return np.asarray(x, dtype=np.float32).flatten()\n    elif isinstance(space, Discrete):\n        onehot = np.zeros(space.n, dtype=np.float32)\n        onehot[x] = 1.0\n        return onehot\n    elif isinstance(space, Tuple):\n        return np.concatenate(\n            [flatten(s, x_part) for x_part, s in zip(x, space.spaces)])\n    elif isinstance(space, Dict):\n        return np.concatenate(\n            [flatten(s, x[key]) for key, s in space.spaces.items()])\n    elif isinstance(space, MultiBinary):\n        return np.asarray(x).flatten()\n    elif isinstance(space, MultiDiscrete):\n        return np.asarray(x).flatten()\n    else:\n        raise NotImplementedError\n\n\ndef unflatten(space, x):\n    \"\"\"Unflatten a data point from a space.\n\n    This reverses the transformation applied by ``flatten()``. You must ensure\n    that the ``space`` argument is the same as for the ``flatten()`` call.\n\n    Accepts a space and a flattened point. Returns a point with a structure\n    that matches the space. Raises ``NotImplementedError`` if the space is not\n    defined in ``gym.spaces``.\n    \"\"\"\n    if isinstance(space, Box):\n        return np.asarray(x, dtype=np.float32).reshape(space.shape)\n    elif isinstance(space, Discrete):\n        return int(np.nonzero(x)[0][0])\n    elif isinstance(space, Tuple):\n        dims = [flatdim(s) for s in space.spaces]\n        list_flattened = np.split(x, np.cumsum(dims)[:-1])\n        list_unflattened = [\n            unflatten(s, flattened)\n            for flattened, s in zip(list_flattened, space.spaces)\n        ]\n        return tuple(list_unflattened)\n    elif isinstance(space, Dict):\n        dims = [flatdim(s) for s in space.spaces.values()]\n        list_flattened = np.split(x, np.cumsum(dims)[:-1])\n        list_unflattened = [\n            (key, unflatten(s, flattened))\n            for flattened, (key,\n                            s) in zip(list_flattened, space.spaces.items())\n        ]\n        return OrderedDict(list_unflattened)\n    elif isinstance(space, MultiBinary):\n        return np.asarray(x).reshape(space.shape)\n    elif isinstance(space, MultiDiscrete):\n        return np.asarray(x).reshape(space.shape)\n    else:\n        raise NotImplementedError\n\n\ndef flatten_space(space):\n    \"\"\"Flatten a space into a single ``Box``.\n\n    This is equivalent to ``flatten()``, but operates on the space itself. The\n    result always is a `Box` with flat boundaries. The box has exactly\n    ``flatdim(space)`` dimensions. Flattening a sample of the original space\n    has the same effect as taking a sample of the flattenend space.\n\n    Raises ``NotImplementedError`` if the space is not defined in\n    ``gym.spaces``.\n\n    Example::\n\n        >>> box = Box(0.0, 1.0, shape=(3, 4, 5))\n        >>> box\n        Box(3, 4, 5)\n        >>> flatten_space(box)\n        Box(60,)\n        >>> flatten(box, box.sample()) in flatten_space(box)\n        True\n\n    Example that flattens a discrete space::\n\n        >>> discrete = Discrete(5)\n        >>> flatten_space(discrete)\n        Box(5,)\n        >>> flatten(box, box.sample()) in flatten_space(box)\n        True\n\n    Example that recursively flattens a dict::\n\n        >>> space = Dict({\"position\": Discrete(2),\n        ...               \"velocity\": Box(0, 1, shape=(2, 2))})\n        >>> flatten_space(space)\n        Box(6,)\n        >>> flatten(space, space.sample()) in flatten_space(space)\n        True\n    \"\"\"\n    if isinstance(space, Box):\n        return Box(space.low.flatten(), space.high.flatten())\n    if isinstance(space, Discrete):\n        return Box(low=0, high=1, shape=(space.n, ))\n    if isinstance(space, Tuple):\n        space = [flatten_space(s) for s in space.spaces]\n        return Box(\n            low=np.concatenate([s.low for s in space]),\n            high=np.concatenate([s.high for s in space]),\n        )\n    if isinstance(space, Dict):\n        space = [flatten_space(s) for s in space.spaces.values()]\n        return Box(\n            low=np.concatenate([s.low for s in space]),\n            high=np.concatenate([s.high for s in space]),\n        )\n    if isinstance(space, MultiBinary):\n        return Box(low=0, high=1, shape=(space.n, ))\n    if isinstance(space, MultiDiscrete):\n        return Box(\n            low=np.zeros_like(space.nvec),\n            high=space.nvec,\n        )\n    raise NotImplementedError",
        "from collections import OrderedDict\nfrom .space import Space\n\n\nclass Dict(Space):\n    \"\"\"\n    A dictionary of simpler spaces.\n\n    Example usage:\n    self.observation_space = spaces.Dict({\"position\": spaces.Discrete(2), \"velocity\": spaces.Discrete(3)})\n\n    Example usage [nested]:\n    self.nested_observation_space = spaces.Dict({\n        'sensors':  spaces.Dict({\n            'position': spaces.Box(low=-100, high=100, shape=(3,)),\n            'velocity': spaces.Box(low=-1, high=1, shape=(3,)),\n            'front_cam': spaces.Tuple((\n                spaces.Box(low=0, high=1, shape=(10, 10, 3)),\n                spaces.Box(low=0, high=1, shape=(10, 10, 3))\n            )),\n            'rear_cam': spaces.Box(low=0, high=1, shape=(10, 10, 3)),\n        }),\n        'ext_controller': spaces.MultiDiscrete((5, 2, 2)),\n        'inner_state':spaces.Dict({\n            'charge': spaces.Discrete(100),\n            'system_checks': spaces.MultiBinary(10),\n            'job_status': spaces.Dict({\n                'task': spaces.Discrete(5),\n                'progress': spaces.Box(low=0, high=100, shape=()),\n            })\n        })\n    })\n    \"\"\"\n    def __init__(self, spaces=None, **spaces_kwargs):\n        assert (spaces is None) or (not spaces_kwargs), 'Use either Dict(spaces=dict(...)) or Dict(foo=x, bar=z)'\n        if spaces is None:\n            spaces = spaces_kwargs\n        if isinstance(spaces, dict) and not isinstance(spaces, OrderedDict):\n            spaces = OrderedDict(sorted(list(spaces.items())))\n        if isinstance(spaces, list):\n            spaces = OrderedDict(spaces)\n        self.spaces = spaces\n        for space in spaces.values():\n            assert isinstance(space, Space), 'Values of the dict should be instances of gym.Space'\n        super(Dict, self).__init__(None, None) # None for shape and dtype, since it'll require special handling\n\n    def seed(self, seed=None):\n        [space.seed(seed) for space in self.spaces.values()]\n\n    def sample(self):\n        return OrderedDict([(k, space.sample()) for k, space in self.spaces.items()])\n\n    def contains(self, x):\n        if not isinstance(x, dict) or len(x) != len(self.spaces):\n            return False\n        for k, space in self.spaces.items():\n            if k not in x:\n                return False\n            if not space.contains(x[k]):\n                return False\n        return True\n\n    def __getitem__(self, key):\n        return self.spaces[key]\n\n    def __repr__(self):\n        return \"Dict(\" + \", \". join([str(k) + \":\" + str(s) for k, s in self.spaces.items()]) + \")\"\n\n    def to_jsonable(self, sample_n):\n        # serialize as dict-repr of vectors\n        return {key: space.to_jsonable([sample[key] for sample in sample_n]) \\\n                for key, space in self.spaces.items()}\n\n    def from_jsonable(self, sample_n):\n        dict_of_list = {}\n        for key, space in self.spaces.items():\n            dict_of_list[key] = space.from_jsonable(sample_n[key])\n        ret = []\n        for i, _ in enumerate(dict_of_list[key]):\n            entry = {}\n            for key, value in dict_of_list.items():\n                entry[key] = value[i]\n            ret.append(entry)\n        return ret\n\n    def __eq__(self, other):\n        return isinstance(other, Dict) and self.spaces == other.spaces",
        "from gym.utils import seeding\n\n\nclass Space(object):\n    \"\"\"Defines the observation and action spaces, so you can write generic\n    code that applies to any Env. For example, you can choose a random\n    action.\n    \"\"\"\n    def __init__(self, shape=None, dtype=None):\n        import numpy as np  # takes about 300-400ms to import, so we load lazily\n        self.shape = None if shape is None else tuple(shape)\n        self.dtype = None if dtype is None else np.dtype(dtype)\n        self.np_random = None\n        self.seed()\n\n    def sample(self):\n        \"\"\"Randomly sample an element of this space. Can be \n        uniform or non-uniform sampling based on boundedness of space.\"\"\"\n        raise NotImplementedError\n\n    def seed(self, seed=None):\n        \"\"\"Seed the PRNG of this space. \"\"\"\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def contains(self, x):\n        \"\"\"\n        Return boolean specifying if x is a valid\n        member of this space\n        \"\"\"\n        raise NotImplementedError\n\n    def __contains__(self, x):\n        return self.contains(x)\n\n    def to_jsonable(self, sample_n):\n        \"\"\"Convert a batch of samples from this space to a JSONable data type.\"\"\"\n        # By default, assume identity is JSONable\n        return sample_n\n\n    def from_jsonable(self, sample_n):\n        \"\"\"Convert a JSONable data type to a batch of samples from this space.\"\"\"\n        # By default, assume identity is JSONable\n        return sample_n",
        "import numpy as np\nfrom .space import Space\n\n\nclass MultiBinary(Space):\n    '''\n    An n-dimensional binary space. \n\n    The argument to MultiBinary defines n.\n    \n    Example Usage:\n    \n    >> self.observation_space = spaces.MultiBinary(5)\n\n    >> self.observation_space.sample()\n\n        array([0,1,0,1,0], dtype =int8)\n\n    '''\n    \n    def __init__(self, n):\n        self.n = n\n        super(MultiBinary, self).__init__((self.n,), np.int8)\n\n    def sample(self):\n        return self.np_random.randint(low=0, high=2, size=self.n, dtype=self.dtype)\n\n    def contains(self, x):\n        if isinstance(x, list):\n            x = np.array(x)  # Promote list to array for contains check\n        return ((x==0) | (x==1)).all()\n\n    def to_jsonable(self, sample_n):\n        return np.array(sample_n).tolist()\n\n    def from_jsonable(self, sample_n):\n        return [np.asarray(sample) for sample in sample_n]\n\n    def __repr__(self):\n        return \"MultiBinary({})\".format(self.n)\n\n    def __eq__(self, other):\n        return isinstance(other, MultiBinary) and self.n == other.n",
        "import distutils.version\nimport os\nimport sys\nimport warnings\n\nfrom gym import error\nfrom gym.version import VERSION as __version__\n\nfrom gym.core import Env, GoalEnv, Wrapper, ObservationWrapper, ActionWrapper, RewardWrapper\nfrom gym.spaces import Space\nfrom gym.envs import make, spec, register\nfrom gym import logger\nfrom gym import vector\n\n__all__ = [\"Env\", \"Space\", \"Wrapper\", \"make\", \"spec\", \"register\"]",
        "import gym\nfrom gym import error\nfrom gym.utils import closer\n\nenv_closer = closer.Closer()\n\n\nclass Env(object):\n    \"\"\"The main OpenAI Gym class. It encapsulates an environment with\n    arbitrary behind-the-scenes dynamics. An environment can be\n    partially or fully observed.\n\n    The main API methods that users of this class need to know are:\n\n        step\n        reset\n        render\n        close\n        seed\n\n    And set the following attributes:\n\n        action_space: The Space object corresponding to valid actions\n        observation_space: The Space object corresponding to valid observations\n        reward_range: A tuple corresponding to the min and max possible rewards\n\n    Note: a default reward range set to [-inf,+inf] already exists. Set it if you want a narrower range.\n\n    The methods are accessed publicly as \"step\", \"reset\", etc...\n    \"\"\"\n    # Set this in SOME subclasses\n    metadata = {'render.modes': []}\n    reward_range = (-float('inf'), float('inf'))\n    spec = None\n\n    # Set these in ALL subclasses\n    action_space = None\n    observation_space = None\n\n    def step(self, action):\n        \"\"\"Run one timestep of the environment's dynamics. When end of\n        episode is reached, you are responsible for calling `reset()`\n        to reset this environment's state.\n\n        Accepts an action and returns a tuple (observation, reward, done, info).\n\n        Args:\n            action (object): an action provided by the agent\n\n        Returns:\n            observation (object): agent's observation of the current environment\n            reward (float) : amount of reward returned after previous action\n            done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n        \"\"\"\n        raise NotImplementedError\n\n    def reset(self):\n        \"\"\"Resets the state of the environment and returns an initial observation.\n\n        Returns:\n            observation (object): the initial observation.\n        \"\"\"\n        raise NotImplementedError\n\n    def render(self, mode='human'):\n        \"\"\"Renders the environment.\n\n        The set of supported modes varies per environment. (And some\n        environments do not support rendering at all.) By convention,\n        if mode is:\n\n        - human: render to the current display or terminal and\n          return nothing. Usually for human consumption.\n        - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n          representing RGB values for an x-by-y pixel image, suitable\n          for turning into a video.\n        - ansi: Return a string (str) or StringIO.StringIO containing a\n          terminal-style text representation. The text can include newlines\n          and ANSI escape sequences (e.g. for colors).\n\n        Note:\n            Make sure that your class's metadata 'render.modes' key includes\n              the list of supported modes. It's recommended to call super()\n              in implementations to use the functionality of this method.\n\n        Args:\n            mode (str): the mode to render with\n\n        Example:\n\n        class MyEnv(Env):\n            metadata = {'render.modes': ['human', 'rgb_array']}\n\n            def render(self, mode='human'):\n                if mode == 'rgb_array':\n                    return np.array(...) # return RGB frame suitable for video\n                elif mode == 'human':\n                    ... # pop up a window and render\n                else:\n                    super(MyEnv, self).render(mode=mode) # just raise an exception\n        \"\"\"\n        raise NotImplementedError\n\n    def close(self):\n        \"\"\"Override close in your subclass to perform any necessary cleanup.\n\n        Environments will automatically close() themselves when\n        garbage collected or when the program exits.\n        \"\"\"\n        pass\n\n    def seed(self, seed=None):\n        \"\"\"Sets the seed for this env's random number generator(s).\n\n        Note:\n            Some environments use multiple pseudorandom number generators.\n            We want to capture all such seeds used in order to ensure that\n            there aren't accidental correlations between multiple generators.\n\n        Returns:\n            list<bigint>: Returns the list of seeds used in this env's random\n              number generators. The first value in the list should be the\n              \"main\" seed, or the value which a reproducer should pass to\n              'seed'. Often, the main seed equals the provided 'seed', but\n              this won't be true if seed=None, for example.\n        \"\"\"\n        return\n\n    @property\n    def unwrapped(self):\n        \"\"\"Completely unwrap this env.\n\n        Returns:\n            gym.Env: The base non-wrapped gym.Env instance\n        \"\"\"\n        return self\n\n    def __str__(self):\n        if self.spec is None:\n            return '<{} instance>'.format(type(self).__name__)\n        else:\n            return '<{}<{}>>'.format(type(self).__name__, self.spec.id)\n\n    def __enter__(self):\n        \"\"\"Support with-statement for the environment. \"\"\"\n        return self\n\n    def __exit__(self, *args):\n        \"\"\"Support with-statement for the environment. \"\"\"\n        self.close()\n        # propagate exception\n        return False\n\n\nclass GoalEnv(Env):\n    \"\"\"A goal-based environment. It functions just as any regular OpenAI Gym environment but it\n    imposes a required structure on the observation_space. More concretely, the observation\n    space is required to contain at least three elements, namely `observation`, `desired_goal`, and\n    `achieved_goal`. Here, `desired_goal` specifies the goal that the agent should attempt to achieve.\n    `achieved_goal` is the goal that it currently achieved instead. `observation` contains the\n    actual observations of the environment as per usual.\n    \"\"\"\n\n    def reset(self):\n        # Enforce that each GoalEnv uses a Goal-compatible observation space.\n        if not isinstance(self.observation_space, gym.spaces.Dict):\n            raise error.Error('GoalEnv requires an observation space of type gym.spaces.Dict')\n        for key in ['observation', 'achieved_goal', 'desired_goal']:\n            if key not in self.observation_space.spaces:\n                raise error.Error('GoalEnv requires the \"{}\" key to be part of the observation dictionary.'.format(key))\n\n    def compute_reward(self, achieved_goal, desired_goal, info):\n        \"\"\"Compute the step reward. This externalizes the reward function and makes\n        it dependent on a desired goal and the one that was achieved. If you wish to include\n        additional rewards that are independent of the goal, you can include the necessary values\n        to derive it in 'info' and compute it accordingly.\n\n        Args:\n            achieved_goal (object): the goal that was achieved during execution\n            desired_goal (object): the desired goal that we asked the agent to attempt to achieve\n            info (dict): an info dictionary with additional information\n\n        Returns:\n            float: The reward that corresponds to the provided achieved goal w.r.t. to the desired\n            goal. Note that the following should always hold true:\n\n                ob, reward, done, info = env.step()\n                assert reward == env.compute_reward(ob['achieved_goal'], ob['goal'], info)\n        \"\"\"\n        raise NotImplementedError\n\n\nclass Wrapper(Env):\n    \"\"\"Wraps the environment to allow a modular transformation.\n\n    This class is the base class for all wrappers. The subclass could override\n    some methods to change the behavior of the original environment without touching the\n    original code.\n\n    .. note::\n\n        Don't forget to call ``super().__init__(env)`` if the subclass overrides :meth:`__init__`.\n\n    \"\"\"\n    def __init__(self, env):\n        self.env = env\n        self.action_space = self.env.action_space\n        self.observation_space = self.env.observation_space\n        self.reward_range = self.env.reward_range\n        self.metadata = self.env.metadata\n\n    def __getattr__(self, name):\n        if name.startswith('_'):\n            raise AttributeError(\"attempted to get missing private attribute '{}'\".format(name))\n        return getattr(self.env, name)\n\n    @property\n    def spec(self):\n        return self.env.spec\n\n    @classmethod\n    def class_name(cls):\n        return cls.__name__\n\n    def step(self, action):\n        return self.env.step(action)\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\n    def render(self, mode='human', **kwargs):\n        return self.env.render(mode, **kwargs)\n\n    def close(self):\n        return self.env.close()\n\n    def seed(self, seed=None):\n        return self.env.seed(seed)\n\n    def compute_reward(self, achieved_goal, desired_goal, info):\n        return self.env.compute_reward(achieved_goal, desired_goal, info)\n\n    def __str__(self):\n        return '<{}{}>'.format(type(self).__name__, self.env)\n\n    def __repr__(self):\n        return str(self)\n\n    @property\n    def unwrapped(self):\n        return self.env.unwrapped\n\n\nclass ObservationWrapper(Wrapper):\n    def reset(self, **kwargs):\n        observation = self.env.reset(**kwargs)\n        return self.observation(observation)\n\n    def step(self, action):\n        observation, reward, done, info = self.env.step(action)\n        return self.observation(observation), reward, done, info\n\n    def observation(self, observation):\n        raise NotImplementedError\n\n\nclass RewardWrapper(Wrapper):\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\n    def step(self, action):\n        observation, reward, done, info = self.env.step(action)\n        return observation, self.reward(reward), done, info\n\n    def reward(self, reward):\n        raise NotImplementedError\n\n\nclass ActionWrapper(Wrapper):\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\n    def step(self, action):\n        return self.env.step(self.action(action))\n\n    def action(self, action):\n        raise NotImplementedError\n\n    def reverse_action(self, action):\n        raise NotImplementedError",
        "import hashlib\nimport numpy as np\nimport os\nimport random as _random\nimport struct\nimport sys\n\nfrom gym import error\n\ndef np_random(seed=None):\n    if seed is not None and not (isinstance(seed, int) and 0 <= seed):\n        raise error.Error('Seed must be a non-negative integer or omitted, not {}'.format(seed))\n\n    seed = create_seed(seed)\n\n    rng = np.random.RandomState()\n    rng.seed(_int_list_from_bigint(hash_seed(seed)))\n    return rng, seed\n\ndef hash_seed(seed=None, max_bytes=8):\n    \"\"\"Any given evaluation is likely to have many PRNG's active at\n    once. (Most commonly, because the environment is running in\n    multiple processes.) There's literature indicating that having\n    linear correlations between seeds of multiple PRNG's can correlate\n    the outputs:\n\n    http://blogs.unity3d.com/2015/01/07/a-primer-on-repeatable-random-numbers/\n    http://stackoverflow.com/questions/1554958/how-different-do-random-seeds-need-to-be\n    http://dl.acm.org/citation.cfm?id=1276928\n\n    Thus, for sanity we hash the seeds before using them. (This scheme\n    is likely not crypto-strength, but it should be good enough to get\n    rid of simple correlations.)\n\n    Args:\n        seed (Optional[int]): None seeds from an operating system specific randomness source.\n        max_bytes: Maximum number of bytes to use in the hashed seed.\n    \"\"\"\n    if seed is None:\n        seed = create_seed(max_bytes=max_bytes)\n    hash = hashlib.sha512(str(seed).encode('utf8')).digest()\n    return _bigint_from_bytes(hash[:max_bytes])\n\ndef create_seed(a=None, max_bytes=8):\n    \"\"\"Create a strong random seed. Otherwise, Python 2 would seed using\n    the system time, which might be non-robust especially in the\n    presence of concurrency.\n\n    Args:\n        a (Optional[int, str]): None seeds from an operating system specific randomness source.\n        max_bytes: Maximum number of bytes to use in the seed.\n    \"\"\"\n    # Adapted from https://svn.python.org/projects/python/tags/r32/Lib/random.py\n    if a is None:\n        a = _bigint_from_bytes(os.urandom(max_bytes))\n    elif isinstance(a, str):\n        a = a.encode('utf8')\n        a += hashlib.sha512(a).digest()\n        a = _bigint_from_bytes(a[:max_bytes])\n    elif isinstance(a, int):\n        a = a % 2**(8 * max_bytes)\n    else:\n        raise error.Error('Invalid type for seed: {} ({})'.format(type(a), a))\n\n    return a\n\n# TODO: don't hardcode sizeof_int here\ndef _bigint_from_bytes(bytes):\n    sizeof_int = 4\n    padding = sizeof_int - len(bytes) % sizeof_int\n    bytes += b'\\0' * padding\n    int_count = int(len(bytes) / sizeof_int)\n    unpacked = struct.unpack(\"{}I\".format(int_count), bytes)\n    accum = 0\n    for i, val in enumerate(unpacked):\n        accum += 2 ** (sizeof_int * 8 * i) * val\n    return accum\n\ndef _int_list_from_bigint(bigint):\n    # Special case 0\n    if bigint < 0:\n        raise error.Error('Seed must be non-negative, not {}'.format(bigint))\n    elif bigint == 0:\n        return [0]\n\n    ints = []\n    while bigint > 0:\n        bigint, mod = divmod(bigint, 2 ** 32)\n        ints.append(mod)\n    return ints",
        "\"\"\"A set of common utilities used within the environments. These are\nnot intended as API functions, and will not remain stable over time.\n\"\"\"\n\n# These submodules should not have any import-time dependencies.\n# We want this since we use `utils` during our import-time sanity checks\n# that verify that our dependencies are actually present.\nfrom .colorize import colorize\nfrom .ezpickle import EzPickle",
        "import gym\nimport pygame\nimport matplotlib\nimport argparse\nfrom gym import logger\ntry:\n    matplotlib.use('TkAgg')\n    import matplotlib.pyplot as plt\nexcept ImportError as e:\n    logger.warn('failed to set matplotlib backend, plotting will not work: %s' % str(e))\n    plt = None\n\nfrom collections import deque\nfrom pygame.locals import VIDEORESIZE\n\ndef display_arr(screen, arr, video_size, transpose):\n    arr_min, arr_max = arr.min(), arr.max()\n    arr = 255.0 * (arr - arr_min) / (arr_max - arr_min)\n    pyg_img = pygame.surfarray.make_surface(arr.swapaxes(0, 1) if transpose else arr)\n    pyg_img = pygame.transform.scale(pyg_img, video_size)\n    screen.blit(pyg_img, (0,0))\n\ndef play(env, transpose=True, fps=30, zoom=None, callback=None, keys_to_action=None):\n    \"\"\"Allows one to play the game using keyboard.\n\n    To simply play the game use:\n\n        play(gym.make(\"Pong-v4\"))\n\n    Above code works also if env is wrapped, so it's particularly useful in\n    verifying that the frame-level preprocessing does not render the game\n    unplayable.\n\n    If you wish to plot real time statistics as you play, you can use\n    gym.utils.play.PlayPlot. Here's a sample code for plotting the reward\n    for last 5 second of gameplay.\n\n        def callback(obs_t, obs_tp1, action, rew, done, info):\n            return [rew,]\n        plotter = PlayPlot(callback, 30 * 5, [\"reward\"])\n\n        env = gym.make(\"Pong-v4\")\n        play(env, callback=plotter.callback)\n\n\n    Arguments\n    ---------\n    env: gym.Env\n        Environment to use for playing.\n    transpose: bool\n        If True the output of observation is transposed.\n        Defaults to true.\n    fps: int\n        Maximum number of steps of the environment to execute every second.\n        Defaults to 30.\n    zoom: float\n        Make screen edge this many times bigger\n    callback: lambda or None\n        Callback if a callback is provided it will be executed after\n        every step. It takes the following input:\n            obs_t: observation before performing action\n            obs_tp1: observation after performing action\n            action: action that was executed\n            rew: reward that was received\n            done: whether the environment is done or not\n            info: debug info\n    keys_to_action: dict: tuple(int) -> int or None\n        Mapping from keys pressed to action performed.\n        For example if pressed 'w' and space at the same time is supposed\n        to trigger action number 2 then key_to_action dict would look like this:\n\n            {\n                # ...\n                sorted(ord('w'), ord(' ')) -> 2\n                # ...\n            }\n        If None, default key_to_action mapping for that env is used, if provided.\n    \"\"\"\n    env.reset()\n    rendered=env.render( mode='rgb_array')\n\n    if keys_to_action is None:\n        if hasattr(env, 'get_keys_to_action'):\n            keys_to_action = env.get_keys_to_action()\n        elif hasattr(env.unwrapped, 'get_keys_to_action'):\n            keys_to_action = env.unwrapped.get_keys_to_action()\n        else:\n            assert False, env.spec.id + \" does not have explicit key to action mapping, \" + \\\n                          \"please specify one manually\"\n    relevant_keys = set(sum(map(list, keys_to_action.keys()),[]))\n    \n    video_size=[rendered.shape[1],rendered.shape[0]]\n    if zoom is not None:\n        video_size = int(video_size[0] * zoom), int(video_size[1] * zoom)\n\n    pressed_keys = []\n    running = True\n    env_done = True\n\n    screen = pygame.display.set_mode(video_size)\n    clock = pygame.time.Clock()\n\n\n    while running:\n        if env_done:\n            env_done = False\n            obs = env.reset()\n        else:\n            action = keys_to_action.get(tuple(sorted(pressed_keys)), 0)\n            prev_obs = obs\n            obs, rew, env_done, info = env.step(action)\n            if callback is not None:\n                callback(prev_obs, obs, action, rew, env_done, info)\n        if obs is not None:\n            rendered=env.render( mode='rgb_array')\n            display_arr(screen, rendered, transpose=transpose, video_size=video_size)\n\n        # process pygame events\n        for event in pygame.event.get():\n            # test events, set key states\n            if event.type == pygame.KEYDOWN:\n                if event.key in relevant_keys:\n                    pressed_keys.append(event.key)\n                elif event.key == 27:\n                    running = False\n            elif event.type == pygame.KEYUP:\n                if event.key in relevant_keys:\n                    pressed_keys.remove(event.key)\n            elif event.type == pygame.QUIT:\n                running = False\n            elif event.type == VIDEORESIZE:\n                video_size = event.size\n                screen = pygame.display.set_mode(video_size)\n                print(video_size)\n\n        pygame.display.flip()\n        clock.tick(fps)\n    pygame.quit()\n\nclass PlayPlot(object):\n    def __init__(self, callback, horizon_timesteps, plot_names):\n        self.data_callback = callback\n        self.horizon_timesteps = horizon_timesteps\n        self.plot_names = plot_names\n\n        assert plt is not None, \"matplotlib backend failed, plotting will not work\"\n\n        num_plots = len(self.plot_names)\n        self.fig, self.ax = plt.subplots(num_plots)\n        if num_plots == 1:\n            self.ax = [self.ax]\n        for axis, name in zip(self.ax, plot_names):\n            axis.set_title(name)\n        self.t = 0\n        self.cur_plot = [None for _ in range(num_plots)]\n        self.data     = [deque(maxlen=horizon_timesteps) for _ in range(num_plots)]\n\n    def callback(self, obs_t, obs_tp1, action, rew, done, info):\n        points = self.data_callback(obs_t, obs_tp1, action, rew, done, info)\n        for point, data_series in zip(points, self.data):\n            data_series.append(point)\n        self.t += 1\n\n        xmin, xmax = max(0, self.t - self.horizon_timesteps), self.t\n\n        for i, plot in enumerate(self.cur_plot):\n            if plot is not None:\n                plot.remove()\n            self.cur_plot[i] = self.ax[i].scatter(range(xmin, xmax), list(self.data[i]), c='blue')\n            self.ax[i].set_xlim(xmin, xmax)\n        plt.pause(0.000001)\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--env', type=str, default='MontezumaRevengeNoFrameskip-v4', help='Define Environment')\n    args = parser.parse_args()\n    env = gym.make(args.env)\n    play(env, zoom=4, fps=60)\n\n\nif __name__ == '__main__':\n    main()",
        "class EzPickle(object):\n    \"\"\"Objects that are pickled and unpickled via their constructor\n    arguments.\n\n    Example usage:\n\n        class Dog(Animal, EzPickle):\n            def __init__(self, furcolor, tailkind=\"bushy\"):\n                Animal.__init__()\n                EzPickle.__init__(furcolor, tailkind)\n                ...\n\n    When this object is unpickled, a new Dog will be constructed by passing the provided\n    furcolor and tailkind into the constructor. However, philosophers are still not sure\n    whether it is still the same dog.\n\n    This is generally needed only for environments which wrap C/C++ code, such as MuJoCo\n    and Atari.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        self._ezpickle_args = args\n        self._ezpickle_kwargs = kwargs\n    def __getstate__(self):\n        return {\"_ezpickle_args\" : self._ezpickle_args, \"_ezpickle_kwargs\": self._ezpickle_kwargs}\n    def __setstate__(self, d):\n        out = type(self)(*d[\"_ezpickle_args\"], **d[\"_ezpickle_kwargs\"])\n        self.__dict__.update(out.__dict__)",
        "import atexit\nimport threading\nimport weakref\n\nclass Closer(object):\n    \"\"\"A registry that ensures your objects get closed, whether manually,\n    upon garbage collection, or upon exit. To work properly, your\n    objects need to cooperate and do something like the following:\n\n    ```\n    closer = Closer()\n    class Example(object):\n        def __init__(self):\n            self._id = closer.register(self)\n\n        def close(self):\n            # Probably worth making idempotent too!\n            ...\n            closer.unregister(self._id)\n\n        def __del__(self):\n            self.close()\n    ```\n\n    That is, your objects should:\n\n    - register() themselves and save the returned ID\n    - unregister() themselves upon close()\n    - include a __del__ method which close()'s the object\n    \"\"\"\n\n    def __init__(self, atexit_register=True):\n        self.lock = threading.Lock()\n        self.next_id = -1\n        self.closeables = weakref.WeakValueDictionary()\n\n        if atexit_register:\n            atexit.register(self.close)\n\n    def generate_next_id(self):\n        with self.lock:\n            self.next_id += 1\n            return self.next_id\n\n    def register(self, closeable):\n        \"\"\"Registers an object with a 'close' method.\n\n        Returns:\n            int: The registration ID of this object. It is the caller's responsibility to save this ID if early closing is desired.\n        \"\"\"\n        assert hasattr(closeable, 'close'), 'No close method for {}'.format(closeable)\n\n        next_id = self.generate_next_id()\n        self.closeables[next_id] = closeable\n        return next_id\n\n    def unregister(self, id):\n        assert id is not None\n        if id in self.closeables:\n            del self.closeables[id]\n\n    def close(self):\n        # Explicitly fetch all monitors first so that they can't disappear while\n        # we iterate. cf. http://stackoverflow.com/a/12429620\n        closeables = list(self.closeables.values())\n        for closeable in closeables:\n            closeable.close()",
        "\"\"\"A set of common utilities used within the environments. These are\nnot intended as API functions, and will not remain stable over time.\n\"\"\"\n\ncolor2num = dict(\n    gray=30,\n    red=31,\n    green=32,\n    yellow=33,\n    blue=34,\n    magenta=35,\n    cyan=36,\n    white=37,\n    crimson=38\n)\n\n\ndef colorize(string, color, bold=False, highlight = False):\n    \"\"\"Return string surrounded by appropriate terminal color codes to\n    print colorized text.  Valid colors: gray, red, green, yellow,\n    blue, magenta, cyan, white, crimson\n    \"\"\"\n\n    attr = []\n    num = color2num[color]\n    if highlight: num += 10\n    attr.append(str(num))\n    if bold: attr.append('1')\n    attrs = ';'.join(attr)\n    return '\\x1b[%sm%s\\x1b[0m' % (attrs, string)",
        "import numpy as np\n\ndef json_encode_np(obj):\n    \"\"\"\n    JSON can't serialize numpy types, so convert to pure python\n    \"\"\"\n    if isinstance(obj, np.ndarray):\n        return list(obj)\n    elif isinstance(obj, np.float32):\n        return float(obj)\n    elif isinstance(obj, np.float64):\n        return float(obj)\n    elif isinstance(obj, np.int8):\n        return int(obj)\n    elif isinstance(obj, np.int16):\n        return int(obj)\n    elif isinstance(obj, np.int32):\n        return int(obj)\n    elif isinstance(obj, np.int64):\n        return int(obj)\n    else:\n        return obj",
        "# Based on http://stackoverflow.com/questions/2333872/atomic-writing-to-file-with-python\n\nimport os\nfrom contextlib import contextmanager\n\n# We would ideally atomically replace any existing file with the new\n# version. However, on Windows there's no Python-only solution prior\n# to Python 3.3. (This library includes a C extension to do so:\n# https://pypi.python.org/pypi/pyosreplace/0.1.)\n#\n# Correspondingly, we make a best effort, but on Python < 3.3 use a\n# replace method which could result in the file temporarily\n# disappearing.\nimport sys\nif sys.version_info >= (3, 3):\n    # Python 3.3 and up have a native `replace` method\n    from os import replace\nelif sys.platform.startswith(\"win\"):\n    def replace(src, dst):\n        # TODO: on Windows, this will raise if the file is in use,\n        # which is possible. We'll need to make this more robust over\n        # time.\n        try:\n            os.remove(dst)\n        except OSError:\n            pass\n        os.rename(src, dst)\nelse:\n    # POSIX rename() is always atomic\n    from os import rename as replace\n\n@contextmanager\ndef atomic_write(filepath, binary=False, fsync=False):\n    \"\"\" Writeable file object that atomically updates a file (using a temporary file). In some cases (namely Python < 3.3 on Windows), this could result in an existing file being temporarily unlinked.\n\n    :param filepath: the file path to be opened\n    :param binary: whether to open the file in a binary mode instead of textual\n    :param fsync: whether to force write the file to disk\n    \"\"\"\n\n    tmppath = filepath + '~'\n    while os.path.isfile(tmppath):\n        tmppath += '~'\n    try:\n        with open(tmppath, 'wb' if binary else 'w') as file:\n            yield file\n            if fsync:\n                file.flush()\n                os.fsync(file.fileno())\n        replace(tmppath, filepath)\n    finally:\n        try:\n            os.remove(tmppath)\n        except (IOError, OSError):\n            pass",
        "import warnings\n\nfrom gym.utils import colorize\n\nDEBUG = 10\nINFO = 20\nWARN = 30\nERROR = 40\nDISABLED = 50\n\nMIN_LEVEL = 30\n\ndef set_level(level):\n    \"\"\"\n    Set logging threshold on current logger.\n    \"\"\"\n    global MIN_LEVEL\n    MIN_LEVEL = level\n\ndef debug(msg, *args):\n    if MIN_LEVEL <= DEBUG:\n        print('%s: %s'%('DEBUG', msg % args))\n\ndef info(msg, *args):\n    if MIN_LEVEL <= INFO:\n        print('%s: %s'%('INFO', msg % args))\n\ndef warn(msg, *args):\n    if MIN_LEVEL <= WARN:\n        warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n\ndef error(msg, *args):\n    if MIN_LEVEL <= ERROR:\n        print(colorize('%s: %s'%('ERROR', msg % args), 'red'))\n\n# DEPRECATED:\nsetLevel = set_level",
        "import re\nimport copy\nimport importlib\nimport warnings\n\nfrom gym import error, logger\n\n# This format is true today, but it's *not* an official spec.\n# [username/](env-name)-v(version)    env-name is group 1, version is group 2\n#\n# 2016-10-31: We're experimentally expanding the environment ID format\n# to include an optional username.\nenv_id_re = re.compile(r'^(?:[\\w:-]+\\/)?([\\w:.-]+)-v(\\d+)$')\n\n\ndef load(name):\n    mod_name, attr_name = name.split(\":\")\n    mod = importlib.import_module(mod_name)\n    fn = getattr(mod, attr_name)\n    return fn\n\n\nclass EnvSpec(object):\n    \"\"\"A specification for a particular instance of the environment. Used\n    to register the parameters for official evaluations.\n\n    Args:\n        id (str): The official environment ID\n        entry_point (Optional[str]): The Python entrypoint of the environment class (e.g. module.name:Class)\n        reward_threshold (Optional[int]): The reward threshold before the task is considered solved\n        nondeterministic (bool): Whether this environment is non-deterministic even after seeding\n        max_episode_steps (Optional[int]): The maximum number of steps that an episode can consist of\n        kwargs (dict): The kwargs to pass to the environment class\n\n    \"\"\"\n\n    def __init__(self, id, entry_point=None, reward_threshold=None, nondeterministic=False, max_episode_steps=None, kwargs=None):\n        self.id = id\n        self.entry_point = entry_point\n        self.reward_threshold = reward_threshold\n        self.nondeterministic = nondeterministic\n        self.max_episode_steps = max_episode_steps\n        self._kwargs = {} if kwargs is None else kwargs\n\n        match = env_id_re.search(id)\n        if not match:\n            raise error.Error('Attempted to register malformed environment ID: {}. (Currently all IDs must be of the form {}.)'.format(id, env_id_re.pattern))\n        self._env_name = match.group(1)            \n\n    def make(self, **kwargs):\n        \"\"\"Instantiates an instance of the environment with appropriate kwargs\"\"\"\n        if self.entry_point is None:\n            raise error.Error('Attempting to make deprecated env {}. (HINT: is there a newer registered version of this env?)'.format(self.id))\n        _kwargs = self._kwargs.copy()\n        _kwargs.update(kwargs)\n        if callable(self.entry_point):\n            env = self.entry_point(**_kwargs)\n        else:\n            cls = load(self.entry_point)\n            env = cls(**_kwargs)\n\n        # Make the environment aware of which spec it came from.\n        spec = copy.deepcopy(self)\n        spec._kwargs = _kwargs\n        env.unwrapped.spec = spec\n\n        return env\n\n    def __repr__(self):\n        return \"EnvSpec({})\".format(self.id)\n\n\nclass EnvRegistry(object):\n    \"\"\"Register an env by ID. IDs remain stable over time and are\n    guaranteed to resolve to the same environment dynamics (or be\n    desupported). The goal is that results on a particular environment\n    should always be comparable, and not depend on the version of the\n    code that was running.\n    \"\"\"\n\n    def __init__(self):\n        self.env_specs = {}\n\n    def make(self, path, **kwargs):\n        if len(kwargs) > 0:\n            logger.info('Making new env: %s (%s)', path, kwargs)\n        else:\n            logger.info('Making new env: %s', path)\n        spec = self.spec(path)\n        env = spec.make(**kwargs)\n        # We used to have people override _reset/_step rather than\n        # reset/step. Set _gym_disable_underscore_compat = True on\n        # your environment if you use these methods and don't want\n        # compatibility code to be invoked.\n        if hasattr(env, \"_reset\") and hasattr(env, \"_step\") and not getattr(env, \"_gym_disable_underscore_compat\", False):\n            patch_deprecated_methods(env)\n        if env.spec.max_episode_steps is not None:\n            from gym.wrappers.time_limit import TimeLimit\n            env = TimeLimit(env, max_episode_steps=env.spec.max_episode_steps)\n        return env\n\n    def all(self):\n        return self.env_specs.values()\n\n    def spec(self, path):\n        if ':' in path:\n            mod_name, _sep, id = path.partition(':')\n            try:\n                importlib.import_module(mod_name)\n            # catch ImportError for python2.7 compatibility\n            except ImportError:\n                raise error.Error('A module ({}) was specified for the environment but was not found, make sure the package is installed with `pip install` before calling `gym.make()`'.format(mod_name))\n        else:\n            id = path\n\n        match = env_id_re.search(id)\n        if not match:\n            raise error.Error('Attempted to look up malformed environment ID: {}. (Currently all IDs must be of the form {}.)'.format(id.encode('utf-8'), env_id_re.pattern))\n\n        try:\n            return self.env_specs[id]\n        except KeyError:\n            # Parse the env name and check to see if it matches the non-version\n            # part of a valid env (could also check the exact number here)\n            env_name = match.group(1)\n            matching_envs = [valid_env_name for valid_env_name, valid_env_spec in self.env_specs.items()\n                             if env_name == valid_env_spec._env_name]\n            if matching_envs:\n                raise error.DeprecatedEnv('Env {} not found (valid versions include {})'.format(id, matching_envs))\n            else:\n                raise error.UnregisteredEnv('No registered env with id: {}'.format(id))\n\n    def register(self, id, **kwargs):\n        if id in self.env_specs:\n            raise error.Error('Cannot re-register id: {}'.format(id))\n        self.env_specs[id] = EnvSpec(id, **kwargs)\n\n# Have a global registry\nregistry = EnvRegistry()\n\ndef register(id, **kwargs):\n    return registry.register(id, **kwargs)\n\ndef make(id, **kwargs):\n    return registry.make(id, **kwargs)\n\ndef spec(id):\n    return registry.spec(id)\n\nwarn_once = True\n\ndef patch_deprecated_methods(env):\n    \"\"\"\n    Methods renamed from '_method' to 'method', render() no longer has 'close' parameter, close is a separate method.\n    For backward compatibility, this makes it possible to work with unmodified environments.\n    \"\"\"\n    global warn_once\n    if warn_once:\n        logger.warn(\"Environment '%s' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\" % str(type(env)))\n        warn_once = False\n    env.reset = env._reset\n    env.step  = env._step\n    env.seed  = env._seed\n    def render(mode):\n        return env._render(mode, close=False)\n    def close():\n        env._render(\"human\", close=True)\n    env.render = render\n    env.close = close",
        "import numpy as np\nimport sys\nfrom gym.envs.toy_text import discrete\n\nUP = 0\nRIGHT = 1\nDOWN = 2\nLEFT = 3\n\n\nclass CliffWalkingEnv(discrete.DiscreteEnv):\n    \"\"\"\n    This is a simple implementation of the Gridworld Cliff\n    reinforcement learning task.\n\n    Adapted from Example 6.6 (page 106) from Reinforcement Learning: An Introduction\n    by Sutton and Barto:\n    http://incompleteideas.net/book/bookdraft2018jan1.pdf\n\n    With inspiration from:\n    https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py\n\n    The board is a 4x12 matrix, with (using Numpy matrix indexing):\n        [3, 0] as the start at bottom-left\n        [3, 11] as the goal at bottom-right\n        [3, 1..10] as the cliff at bottom-center\n\n    Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward\n    and a reset to the start. An episode terminates when the agent reaches the goal.\n    \"\"\"\n    metadata = {'render.modes': ['human', 'ansi']}\n\n    def __init__(self):\n        self.shape = (4, 12)\n        self.start_state_index = np.ravel_multi_index((3, 0), self.shape)\n\n        nS = np.prod(self.shape)\n        nA = 4\n\n        # Cliff Location\n        self._cliff = np.zeros(self.shape, dtype=np.bool)\n        self._cliff[3, 1:-1] = True\n\n        # Calculate transition probabilities and rewards\n        P = {}\n        for s in range(nS):\n            position = np.unravel_index(s, self.shape)\n            P[s] = {a: [] for a in range(nA)}\n            P[s][UP] = self._calculate_transition_prob(position, [-1, 0])\n            P[s][RIGHT] = self._calculate_transition_prob(position, [0, 1])\n            P[s][DOWN] = self._calculate_transition_prob(position, [1, 0])\n            P[s][LEFT] = self._calculate_transition_prob(position, [0, -1])\n\n        # Calculate initial state distribution\n        # We always start in state (3, 0)\n        isd = np.zeros(nS)\n        isd[self.start_state_index] = 1.0\n\n        super(CliffWalkingEnv, self).__init__(nS, nA, P, isd)\n\n    def _limit_coordinates(self, coord):\n        \"\"\"\n        Prevent the agent from falling out of the grid world\n        :param coord:\n        :return:\n        \"\"\"\n        coord[0] = min(coord[0], self.shape[0] - 1)\n        coord[0] = max(coord[0], 0)\n        coord[1] = min(coord[1], self.shape[1] - 1)\n        coord[1] = max(coord[1], 0)\n        return coord\n\n    def _calculate_transition_prob(self, current, delta):\n        \"\"\"\n        Determine the outcome for an action. Transition Prob is always 1.0.\n        :param current: Current position on the grid as (row, col)\n        :param delta: Change in position for transition\n        :return: (1.0, new_state, reward, done)\n        \"\"\"\n        new_position = np.array(current) + np.array(delta)\n        new_position = self._limit_coordinates(new_position).astype(int)\n        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n        if self._cliff[tuple(new_position)]:\n            return [(1.0, self.start_state_index, -100, False)]\n\n        terminal_state = (self.shape[0] - 1, self.shape[1] - 1)\n        is_done = tuple(new_position) == terminal_state\n        return [(1.0, new_state, -1, is_done)]\n\n    def render(self, mode='human'):\n        outfile = sys.stdout\n\n        for s in range(self.nS):\n            position = np.unravel_index(s, self.shape)\n            if self.s == s:\n                output = \" x \"\n            # Print terminal state\n            elif position == (3, 11):\n                output = \" T \"\n            elif self._cliff[position]:\n                output = \" C \"\n            else:\n                output = \" o \"\n\n            if position[1] == 0:\n                output = output.lstrip()\n            if position[1] == self.shape[1] - 1:\n                output = output.rstrip()\n                output += '\\n'\n\n            outfile.write(output)\n        outfile.write('\\n')\n",
        "import sys\nfrom contextlib import closing\nfrom io import StringIO\nfrom gym import utils\nfrom gym.envs.toy_text import discrete\nimport numpy as np\n\nMAP = [\n    \"+---------+\",\n    \"|R: | : :G|\",\n    \"| : | : : |\",\n    \"| : : : : |\",\n    \"| | : | : |\",\n    \"|Y| : |B: |\",\n    \"+---------+\",\n]\n\n\nclass TaxiEnv(discrete.DiscreteEnv):\n    \"\"\"\n    The Taxi Problem\n    from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"\n    by Tom Dietterich\n\n    Description:\n    There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n\n    Observations: \n    There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations. \n    \n    Passenger locations:\n    - 0: R(ed)\n    - 1: G(reen)\n    - 2: Y(ellow)\n    - 3: B(lue)\n    - 4: in taxi\n    \n    Destinations:\n    - 0: R(ed)\n    - 1: G(reen)\n    - 2: Y(ellow)\n    - 3: B(lue)\n        \n    Actions:\n    There are 6 discrete deterministic actions:\n    - 0: move south\n    - 1: move north\n    - 2: move east \n    - 3: move west \n    - 4: pickup passenger\n    - 5: dropoff passenger\n    \n    Rewards: \n    There is a reward of -1 for each action and an additional reward of +20 for delivering the passenger. There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally.\n    \n\n    Rendering:\n    - blue: passenger\n    - magenta: destination\n    - yellow: empty taxi\n    - green: full taxi\n    - other letters (R, G, Y and B): locations for passengers and destinations\n    \n\n    state space is represented by:\n        (taxi_row, taxi_col, passenger_location, destination)\n    \"\"\"\n    metadata = {'render.modes': ['human', 'ansi']}\n\n    def __init__(self):\n        self.desc = np.asarray(MAP, dtype='c')\n\n        self.locs = locs = [(0,0), (0,4), (4,0), (4,3)]\n\n        num_states = 500\n        num_rows = 5\n        num_columns = 5\n        max_row = num_rows - 1\n        max_col = num_columns - 1\n        initial_state_distrib = np.zeros(num_states)\n        num_actions = 6\n        P = {state: {action: []\n                     for action in range(num_actions)} for state in range(num_states)}\n        for row in range(num_rows):\n            for col in range(num_columns):\n                for pass_idx in range(len(locs) + 1):  # +1 for being inside taxi\n                    for dest_idx in range(len(locs)):\n                        state = self.encode(row, col, pass_idx, dest_idx)\n                        if pass_idx < 4 and pass_idx != dest_idx:\n                            initial_state_distrib[state] += 1\n                        for action in range(num_actions):\n                            # defaults\n                            new_row, new_col, new_pass_idx = row, col, pass_idx\n                            reward = -1 # default reward when there is no pickup/dropoff\n                            done = False\n                            taxi_loc = (row, col)\n\n                            if action == 0:\n                                new_row = min(row + 1, max_row)\n                            elif action == 1:\n                                new_row = max(row - 1, 0)\n                            if action == 2 and self.desc[1 + row, 2 * col + 2] == b\":\":\n                                new_col = min(col + 1, max_col)\n                            elif action == 3 and self.desc[1 + row, 2 * col] == b\":\":\n                                new_col = max(col - 1, 0)\n                            elif action == 4:  # pickup\n                                if (pass_idx < 4 and taxi_loc == locs[pass_idx]):\n                                    new_pass_idx = 4\n                                else: # passenger not at location\n                                    reward = -10\n                            elif action == 5:  # dropoff\n                                if (taxi_loc == locs[dest_idx]) and pass_idx == 4:\n                                    new_pass_idx = dest_idx\n                                    done = True\n                                    reward = 20\n                                elif (taxi_loc in locs) and pass_idx == 4:\n                                    new_pass_idx = locs.index(taxi_loc)\n                                else: # dropoff at wrong location\n                                    reward = -10\n                            new_state = self.encode(\n                                new_row, new_col, new_pass_idx, dest_idx)\n                            P[state][action].append(\n                                (1.0, new_state, reward, done))\n        initial_state_distrib /= initial_state_distrib.sum()\n        discrete.DiscreteEnv.__init__(\n            self, num_states, num_actions, P, initial_state_distrib)\n\n    def encode(self, taxi_row, taxi_col, pass_loc, dest_idx):\n        # (5) 5, 5, 4\n        i = taxi_row\n        i *= 5\n        i += taxi_col\n        i *= 5\n        i += pass_loc\n        i *= 4\n        i += dest_idx\n        return i\n\n    def decode(self, i):\n        out = []\n        out.append(i % 4)\n        i = i // 4\n        out.append(i % 5)\n        i = i // 5\n        out.append(i % 5)\n        i = i // 5\n        out.append(i)\n        assert 0 <= i < 5\n        return reversed(out)\n\n    def render(self, mode='human'):\n        outfile = StringIO() if mode == 'ansi' else sys.stdout\n\n        out = self.desc.copy().tolist()\n        out = [[c.decode('utf-8') for c in line] for line in out]\n        taxi_row, taxi_col, pass_idx, dest_idx = self.decode(self.s)\n\n        def ul(x): return \"_\" if x == \" \" else x\n        if pass_idx < 4:\n            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n                out[1 + taxi_row][2 * taxi_col + 1], 'yellow', highlight=True)\n            pi, pj = self.locs[pass_idx]\n            out[1 + pi][2 * pj + 1] = utils.colorize(out[1 + pi][2 * pj + 1], 'blue', bold=True)\n        else:  # passenger in taxi\n            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n                ul(out[1 + taxi_row][2 * taxi_col + 1]), 'green', highlight=True)\n\n        di, dj = self.locs[dest_idx]\n        out[1 + di][2 * dj + 1] = utils.colorize(out[1 + di][2 * dj + 1], 'magenta')\n        outfile.write(\"\\n\".join([\"\".join(row) for row in out]) + \"\\n\")\n        if self.lastaction is not None:\n            outfile.write(\"  ({})\\n\".format([\"South\", \"North\", \"East\", \"West\", \"Pickup\", \"Dropoff\"][self.lastaction]))\n        else: outfile.write(\"\\n\")\n\n        # No need to return anything for human\n        if mode != 'human':\n            with closing(outfile):\n                return outfile.getvalue()",
        "import gym\nfrom gym import spaces\nfrom gym.utils import seeding\n\nclass NChainEnv(gym.Env):\n    \"\"\"n-Chain environment\n\n    This game presents moves along a linear chain of states, with two actions:\n     0) forward, which moves along the chain but returns no reward\n     1) backward, which returns to the beginning and has a small reward\n\n    The end of the chain, however, presents a large reward, and by moving\n    'forward' at the end of the chain this large reward can be repeated.\n\n    At each action, there is a small probability that the agent 'slips' and the\n    opposite transition is instead taken.\n\n    The observed state is the current state in the chain (0 to n-1).\n\n    This environment is described in section 6.1 of:\n    A Bayesian Framework for Reinforcement Learning by Malcolm Strens (2000)\n    http://ceit.aut.ac.ir/~shiry/lecture/machine-learning/papers/BRL-2000.pdf\n    \"\"\"\n    def __init__(self, n=5, slip=0.2, small=2, large=10):\n        self.n = n\n        self.slip = slip  # probability of 'slipping' an action\n        self.small = small  # payout for 'backwards' action\n        self.large = large  # payout at end of chain for 'forwards' action\n        self.state = 0  # Start at beginning of the chain\n        self.action_space = spaces.Discrete(2)\n        self.observation_space = spaces.Discrete(self.n)\n        self.seed()\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def step(self, action):\n        assert self.action_space.contains(action)\n        if self.np_random.rand() < self.slip:\n            action = not action  # agent slipped, reverse action taken\n        if action:  # 'backwards': go back to the beginning, get small reward\n            reward = self.small\n            self.state = 0\n        elif self.state < self.n - 1:  # 'forwards': go up along the chain\n            reward = 0\n            self.state += 1\n        else:  # 'forwards': stay at the end of the chain, collect large reward\n            reward = self.large\n        done = False\n        return self.state, reward, done, {}\n\n    def reset(self):\n        self.state = 0\n        return self.state",
        "import sys\nfrom contextlib import closing\n\nimport numpy as np\nfrom io import StringIO\n\nfrom gym import utils\nfrom gym.envs.toy_text import discrete\n\nLEFT = 0\nDOWN = 1\nRIGHT = 2\nUP = 3\n\nMAPS = {\n    \"4x4\": [\n        \"SFFF\",\n        \"FHFH\",\n        \"FFFH\",\n        \"HFFG\"\n    ],\n    \"8x8\": [\n        \"SFFFFFFF\",\n        \"FFFFFFFF\",\n        \"FFFHFFFF\",\n        \"FFFFFHFF\",\n        \"FFFHFFFF\",\n        \"FHHFFFHF\",\n        \"FHFFHFHF\",\n        \"FFFHFFFG\"\n    ],\n}\n\n\ndef generate_random_map(size=8, p=0.8):\n    \"\"\"Generates a random valid map (one that has a path from start to goal)\n    :param size: size of each side of the grid\n    :param p: probability that a tile is frozen\n    \"\"\"\n    valid = False\n\n    # DFS to check that it's a valid path.\n    def is_valid(res):\n        frontier, discovered = [], set()\n        frontier.append((0,0))\n        while frontier:\n            r, c = frontier.pop()\n            if not (r,c) in discovered:\n                discovered.add((r,c))\n                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n                for x, y in directions:\n                    r_new = r + x\n                    c_new = c + y\n                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n                        continue\n                    if res[r_new][c_new] == 'G':\n                        return True\n                    if (res[r_new][c_new] not in '#H'):\n                        frontier.append((r_new, c_new))\n        return False\n\n    while not valid:\n        p = min(1, p)\n        res = np.random.choice(['F', 'H'], (size, size), p=[p, 1-p])\n        res[0][0] = 'S'\n        res[-1][-1] = 'G'\n        valid = is_valid(res)\n    return [\"\".join(x) for x in res]\n\n\nclass FrozenLakeEnv(discrete.DiscreteEnv):\n    \"\"\"\n    Winter is here. You and your friends were tossing around a frisbee at the park\n    when you made a wild throw that left the frisbee out in the middle of the lake.\n    The water is mostly frozen, but there are a few holes where the ice has melted.\n    If you step into one of those holes, you'll fall into the freezing water.\n    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n    you navigate across the lake and retrieve the disc.\n    However, the ice is slippery, so you won't always move in the direction you intend.\n    The surface is described using a grid like the following\n\n        SFFF\n        FHFH\n        FFFH\n        HFFG\n\n    S : starting point, safe\n    F : frozen surface, safe\n    H : hole, fall to your doom\n    G : goal, where the frisbee is located\n\n    The episode ends when you reach the goal or fall in a hole.\n    You receive a reward of 1 if you reach the goal, and zero otherwise.\n\n    \"\"\"\n\n    metadata = {'render.modes': ['human', 'ansi']}\n\n    def __init__(self, desc=None, map_name=\"4x4\",is_slippery=True):\n        if desc is None and map_name is None:\n            desc = generate_random_map()\n        elif desc is None:\n            desc = MAPS[map_name]\n        self.desc = desc = np.asarray(desc,dtype='c')\n        self.nrow, self.ncol = nrow, ncol = desc.shape\n        self.reward_range = (0, 1)\n\n        nA = 4\n        nS = nrow * ncol\n\n        isd = np.array(desc == b'S').astype('float64').ravel()\n        isd /= isd.sum()\n\n        P = {s : {a : [] for a in range(nA)} for s in range(nS)}\n\n        def to_s(row, col):\n            return row*ncol + col\n\n        def inc(row, col, a):\n            if a == LEFT:\n                col = max(col-1,0)\n            elif a == DOWN:\n                row = min(row+1,nrow-1)\n            elif a == RIGHT:\n                col = min(col+1,ncol-1)\n            elif a == UP:\n                row = max(row-1,0)\n            return (row, col)\n\n        for row in range(nrow):\n            for col in range(ncol):\n                s = to_s(row, col)\n                for a in range(4):\n                    li = P[s][a]\n                    letter = desc[row, col]\n                    if letter in b'GH':\n                        li.append((1.0, s, 0, True))\n                    else:\n                        if is_slippery:\n                            for b in [(a-1)%4, a, (a+1)%4]:\n                                newrow, newcol = inc(row, col, b)\n                                newstate = to_s(newrow, newcol)\n                                newletter = desc[newrow, newcol]\n                                done = bytes(newletter) in b'GH'\n                                rew = float(newletter == b'G')\n                                li.append((1.0/3.0, newstate, rew, done))\n                        else:\n                            newrow, newcol = inc(row, col, a)\n                            newstate = to_s(newrow, newcol)\n                            newletter = desc[newrow, newcol]\n                            done = bytes(newletter) in b'GH'\n                            rew = float(newletter == b'G')\n                            li.append((1.0, newstate, rew, done))\n\n        super(FrozenLakeEnv, self).__init__(nS, nA, P, isd)\n\n    def render(self, mode='human'):\n        outfile = StringIO() if mode == 'ansi' else sys.stdout\n\n        row, col = self.s // self.ncol, self.s % self.ncol\n        desc = self.desc.tolist()\n        desc = [[c.decode('utf-8') for c in line] for line in desc]\n        desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n        if self.lastaction is not None:\n            outfile.write(\"  ({})\\n\".format([\"Left\",\"Down\",\"Right\",\"Up\"][self.lastaction]))\n        else:\n            outfile.write(\"\\n\")\n        outfile.write(\"\\n\".join(''.join(line) for line in desc)+\"\\n\")\n\n        if mode != 'human':\n            with closing(outfile):\n                return outfile.getvalue()",
        "import numpy as np\n\nimport gym\nfrom gym import spaces\nfrom gym.utils import seeding\n\n\nclass GuessingGame(gym.Env):\n    \"\"\"Number guessing game\n\n    The object of the game is to guess within 1% of the randomly chosen number\n    within 200 time steps\n\n    After each step the agent is provided with one of four possible observations\n    which indicate where the guess is in relation to the randomly chosen number\n\n    0 - No guess yet submitted (only after reset)\n    1 - Guess is lower than the target\n    2 - Guess is equal to the target\n    3 - Guess is higher than the target\n\n    The rewards are:\n    0 if the agent's guess is outside of 1% of the target\n    1 if the agent's guess is inside 1% of the target\n\n    The episode terminates after the agent guesses within 1% of the target or\n    200 steps have been taken\n\n    The agent will need to use a memory of previously submitted actions and observations\n    in order to efficiently explore the available actions\n\n    The purpose is to have agents optimise their exploration parameters (e.g. how far to\n    explore from previous actions) based on previous experience. Because the goal changes\n    each episode a state-value or action-value function isn't able to provide any additional\n    benefit apart from being able to tell whether to increase or decrease the next guess.\n\n    The perfect agent would likely learn the bounds of the action space (without referring\n    to them explicitly) and then follow binary tree style exploration towards to goal number\n    \"\"\"\n    def __init__(self):\n        self.range = 1000  # Randomly selected number is within +/- this value\n        self.bounds = 10000\n\n        self.action_space = spaces.Box(low=np.array([-self.bounds]), high=np.array([self.bounds]),\n                                       dtype=np.float32)\n        self.observation_space = spaces.Discrete(4)\n\n        self.number = 0\n        self.guess_count = 0\n        self.guess_max = 200\n        self.observation = 0\n\n        self.seed()\n        self.reset()\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def step(self, action):\n        assert self.action_space.contains(action)\n\n        if action < self.number:\n            self.observation = 1\n\n        elif action == self.number:\n            self.observation = 2\n\n        elif action > self.number:\n            self.observation = 3\n\n        reward = 0\n        done = False\n\n        if (self.number - self.range * 0.01) < action < (self.number + self.range * 0.01):\n            reward = 1\n            done = True\n\n        self.guess_count += 1\n        if self.guess_count >= self.guess_max:\n            done = True\n\n        return self.observation, reward, done, {\"number\": self.number, \"guesses\": self.guess_count}\n\n    def reset(self):\n        self.number = self.np_random.uniform(-self.range, self.range)\n        self.guess_count = 0\n        self.observation = 0\n        return self.observation",
        "import numpy as np\n\nfrom gym import Env, spaces\nfrom gym.utils import seeding\n\ndef categorical_sample(prob_n, np_random):\n    \"\"\"\n    Sample from categorical distribution\n    Each row specifies class probabilities\n    \"\"\"\n    prob_n = np.asarray(prob_n)\n    csprob_n = np.cumsum(prob_n)\n    return (csprob_n > np_random.rand()).argmax()\n\n\nclass DiscreteEnv(Env):\n\n    \"\"\"\n    Has the following members\n    - nS: number of states\n    - nA: number of actions\n    - P: transitions (*)\n    - isd: initial state distribution (**)\n\n    (*) dictionary dict of dicts of lists, where\n      P[s][a] == [(probability, nextstate, reward, done), ...]\n    (**) list or array of length nS\n\n\n    \"\"\"\n    def __init__(self, nS, nA, P, isd):\n        self.P = P\n        self.isd = isd\n        self.lastaction = None # for rendering\n        self.nS = nS\n        self.nA = nA\n\n        self.action_space = spaces.Discrete(self.nA)\n        self.observation_space = spaces.Discrete(self.nS)\n\n        self.seed()\n        self.s = categorical_sample(self.isd, self.np_random)\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def reset(self):\n        self.s = categorical_sample(self.isd, self.np_random)\n        self.lastaction = None\n        return self.s\n\n    def step(self, a):\n        transitions = self.P[self.s][a]\n        i = categorical_sample([t[0] for t in transitions], self.np_random)\n        p, s, r, d= transitions[i]\n        self.s = s\n        self.lastaction = a\n        return (s, r, d, {\"prob\" : p})",
        "from gym.envs.toy_text.blackjack import BlackjackEnv\nfrom gym.envs.toy_text.roulette import RouletteEnv\nfrom gym.envs.toy_text.frozen_lake import FrozenLakeEnv\nfrom gym.envs.toy_text.nchain import NChainEnv\nfrom gym.envs.toy_text.hotter_colder import HotterColder\nfrom gym.envs.toy_text.guessing_game import GuessingGame\nfrom gym.envs.toy_text.kellycoinflip import KellyCoinflipEnv\nfrom gym.envs.toy_text.kellycoinflip import KellyCoinflipGeneralizedEnv\nfrom gym.envs.toy_text.cliffwalking import CliffWalkingEnv\nfrom gym.envs.toy_text.taxi import TaxiEnv\nfrom gym.envs.toy_text.guessing_game import GuessingGame\nfrom gym.envs.toy_text.hotter_colder import HotterColder",
        "import numpy as np\n\nimport gym\nfrom gym import spaces\nfrom gym.utils import seeding\n\n\nclass HotterColder(gym.Env):\n    \"\"\"Hotter Colder\n    The goal of hotter colder is to guess closer to a randomly selected number\n\n    After each step the agent receives an observation of:\n    0 - No guess yet submitted (only after reset)\n    1 - Guess is lower than the target\n    2 - Guess is equal to the target\n    3 - Guess is higher than the target\n\n    The rewards is calculated as:\n    (min(action, self.number) + self.range) / (max(action, self.number) + self.range)\n\n    Ideally an agent will be able to recognise the 'scent' of a higher reward and\n    increase the rate in which is guesses in that direction until the reward reaches\n    its maximum\n    \"\"\"\n    def __init__(self):\n        self.range = 1000  # +/- value the randomly select number can be between\n        self.bounds = 2000  # Action space bounds\n\n        self.action_space = spaces.Box(low=np.array([-self.bounds]), high=np.array([self.bounds]),\n                                       dtype=np.float32)\n        self.observation_space = spaces.Discrete(4)\n\n        self.number = 0\n        self.guess_count = 0\n        self.guess_max = 200\n        self.observation = 0\n\n        self.seed()\n        self.reset()\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def step(self, action):\n        assert self.action_space.contains(action)\n\n        if action < self.number:\n            self.observation = 1\n\n        elif action == self.number:\n            self.observation = 2\n\n        elif action > self.number:\n            self.observation = 3\n\n        reward = ((min(action, self.number) + self.bounds) / (max(action, self.number) + self.bounds)) ** 2\n\n        self.guess_count += 1\n        done = self.guess_count >= self.guess_max\n\n        return self.observation, reward[0], done, {\"number\": self.number, \"guesses\": self.guess_count}\n\n    def reset(self):\n        self.number = self.np_random.uniform(-self.range, self.range)\n        self.guess_count = 0\n        self.observation = 0\n        return self.observation",
        "import gym\nfrom gym import spaces\nfrom gym.utils import seeding\n\ndef cmp(a, b):\n    return float(a > b) - float(a < b)\n\n# 1 = Ace, 2-10 = Number cards, Jack/Queen/King = 10\ndeck = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10]\n\n\ndef draw_card(np_random):\n    return int(np_random.choice(deck))\n\n\ndef draw_hand(np_random):\n    return [draw_card(np_random), draw_card(np_random)]\n\n\ndef usable_ace(hand):  # Does this hand have a usable ace?\n    return 1 in hand and sum(hand) + 10 <= 21\n\n\ndef sum_hand(hand):  # Return current hand total\n    if usable_ace(hand):\n        return sum(hand) + 10\n    return sum(hand)\n\n\ndef is_bust(hand):  # Is this hand a bust?\n    return sum_hand(hand) > 21\n\n\ndef score(hand):  # What is the score of this hand (0 if bust)\n    return 0 if is_bust(hand) else sum_hand(hand)\n\n\ndef is_natural(hand):  # Is this hand a natural blackjack?\n    return sorted(hand) == [1, 10]\n\n\nclass BlackjackEnv(gym.Env):\n    \"\"\"Simple blackjack environment\n\n    Blackjack is a card game where the goal is to obtain cards that sum to as\n    near as possible to 21 without going over.  They're playing against a fixed\n    dealer.\n    Face cards (Jack, Queen, King) have point value 10.\n    Aces can either count as 11 or 1, and it's called 'usable' at 11.\n    This game is placed with an infinite deck (or with replacement).\n    The game starts with dealer having one face up and one face down card, while\n    player having two face up cards. (Virtually for all Blackjack games today).\n\n    The player can request additional cards (hit=1) until they decide to stop\n    (stick=0) or exceed 21 (bust).\n\n    After the player sticks, the dealer reveals their facedown card, and draws\n    until their sum is 17 or greater.  If the dealer goes bust the player wins.\n\n    If neither player nor dealer busts, the outcome (win, lose, draw) is\n    decided by whose sum is closer to 21.  The reward for winning is +1,\n    drawing is 0, and losing is -1.\n\n    The observation of a 3-tuple of: the players current sum,\n    the dealer's one showing card (1-10 where 1 is ace),\n    and whether or not the player holds a usable ace (0 or 1).\n\n    This environment corresponds to the version of the blackjack problem\n    described in Example 5.1 in Reinforcement Learning: An Introduction\n    by Sutton and Barto.\n    http://incompleteideas.net/book/the-book-2nd.html\n    \"\"\"\n    def __init__(self, natural=False):\n        self.action_space = spaces.Discrete(2)\n        self.observation_space = spaces.Tuple((\n            spaces.Discrete(32),\n            spaces.Discrete(11),\n            spaces.Discrete(2)))\n        self.seed()\n\n        # Flag to payout 1.5 on a \"natural\" blackjack win, like casino rules\n        # Ref: http://www.bicyclecards.com/how-to-play/blackjack/\n        self.natural = natural\n        # Start the first game\n        self.reset()\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def step(self, action):\n        assert self.action_space.contains(action)\n        if action:  # hit: add a card to players hand and return\n            self.player.append(draw_card(self.np_random))\n            if is_bust(self.player):\n                done = True\n                reward = -1.\n            else:\n                done = False\n                reward = 0.\n        else:  # stick: play out the dealers hand, and score\n            done = True\n            while sum_hand(self.dealer) < 17:\n                self.dealer.append(draw_card(self.np_random))\n            reward = cmp(score(self.player), score(self.dealer))\n            if self.natural and is_natural(self.player) and reward == 1.:\n                reward = 1.5\n        return self._get_obs(), reward, done, {}\n\n    def _get_obs(self):\n        return (sum_hand(self.player), self.dealer[0], usable_ace(self.player))\n\n    def reset(self):\n        self.dealer = draw_hand(self.np_random)\n        self.player = draw_hand(self.np_random)\n        return self._get_obs()",
        "import gym\nfrom gym import spaces\nfrom gym.utils import seeding\n\n\nclass RouletteEnv(gym.Env):\n    \"\"\"Simple roulette environment\n\n    The roulette wheel has 37 spots. If the bet is 0 and a 0 comes up,\n    you win a reward of 35. If the parity of your bet matches the parity\n    of the spin, you win 1. Otherwise you receive a reward of -1.\n\n    The long run reward for playing 0 should be -1/37 for any state\n\n    The last action (38) stops the rollout for a return of 0 (walking away)\n    \"\"\"\n    def __init__(self, spots=37):\n        self.n = spots + 1\n        self.action_space = spaces.Discrete(self.n)\n        self.observation_space = spaces.Discrete(1)\n        self.seed()\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def step(self, action):\n        assert self.action_space.contains(action)\n        if action == self.n - 1:\n            # observation, reward, done, info\n            return 0, 0, True, {}\n\n        # N.B. np.random.randint draws from [A, B) while random.randint draws from [A,B]\n        val = self.np_random.randint(0, self.n - 1)\n        if val == action == 0:\n            reward = self.n - 2.0\n        elif val != 0 and action != 0 and val % 2 == action % 2:\n            reward = 1.0\n        else:\n            reward = -1.0\n        return 0, reward, False, {}\n\n    def reset(self):\n        return 0",
        "from scipy.stats import genpareto\nimport numpy as np\n\nimport gym\nfrom gym import spaces\nfrom gym.utils import seeding\n\n\ndef flip(edge, np_random):\n    return 1 if np_random.uniform() < edge else -1\n\n\nclass KellyCoinflipEnv(gym.Env):\n    \"\"\"The Kelly coinflip game is a simple gambling introduced by Haghani & Dewey 2016's\n    'Rational Decision-Making Under Uncertainty: Observed Betting Patterns on a Biased\n    Coin' (https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2856963), to test human\n    decision-making in a setting like that of the stock market: positive expected value\n    but highly stochastic; they found many subjects performed badly, often going broke,\n    even though optimal play would reach the maximum with ~95% probability. In the\n    coinflip game, the player starts with $25.00 to gamble over 300 rounds; each round,\n    they can bet anywhere up to their net worth (in penny increments), and then a coin is\n    flipped; with P=0.6, the player wins twice what they bet, otherwise, they lose it.\n    $250 is the maximum players are allowed to have. At the end of the 300 rounds, they\n    keep whatever they have. The human subjects earned an average of $91; a simple use of\n    the Kelly criterion (https://en.wikipedia.org/wiki/Kelly_criterion), giving a\n    strategy of betting 20% until the cap is hit, would earn $240; a decision tree\n    analysis shows that optimal play earns $246 (https://www.gwern.net/Coin-flip).\n\n    The game short-circuits when either wealth = $0 (since one can never recover) or\n    wealth = cap (trivial optimal play: one simply bets nothing thereafter).\n\n    In this implementation, we default to the paper settings of $25, 60% odds, wealth cap\n    of $250, and 300 rounds. To specify the action space in advance, we multiply the\n    wealth cap (in dollars) by 100 (to allow for all penny bets); should one attempt to\n    bet more money than one has, it is rounded down to one's net worth. (Alternately, a\n    mistaken bet could end the episode immediately; it's not clear to me which version\n    would be better.) For a harder version which randomizes the 3 key parameters, see the\n    Generalized Kelly coinflip game.\"\"\"\n    metadata = {'render.modes': ['human']}\n\n    def __init__(self, initial_wealth=25.0, edge=0.6, max_wealth=250.0, max_rounds=300):\n\n        self.action_space = spaces.Discrete(int(max_wealth * 100))  # betting in penny\n        # increments\n        self.observation_space = spaces.Tuple((\n            spaces.Box(0, max_wealth, [1], dtype=np.float32),  # (w,b)\n            spaces.Discrete(max_rounds + 1)))\n        self.reward_range = (0, max_wealth)\n        self.edge = edge\n        self.wealth = initial_wealth\n        self.initial_wealth = initial_wealth\n        self.max_rounds = max_rounds\n        self.max_wealth = max_wealth\n        self.np_random = None\n        self.rounds = None\n        self.seed()\n        self.reset()\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def step(self, action):\n        bet_in_dollars = min(action/100.0, self.wealth)  # action = desired bet in pennies\n        self.rounds -= 1\n\n        coinflip = flip(self.edge, self.np_random)\n        self.wealth = min(self.max_wealth, self.wealth + coinflip * bet_in_dollars)\n\n        done = self.wealth < 0.01 or self.wealth == self.max_wealth or not self.rounds\n        reward = self.wealth if done else 0.0\n\n        return self._get_obs(), reward, done, {}\n\n    def _get_obs(self):\n        return np.array([self.wealth]), self.rounds\n\n    def reset(self):\n        self.rounds = self.max_rounds\n        self.wealth = self.initial_wealth\n        return self._get_obs()\n\n    def render(self, mode='human'):\n        print(\"Current wealth: \", self.wealth, \"; Rounds left: \", self.rounds)\n\n\nclass KellyCoinflipGeneralizedEnv(gym.Env):\n    \"\"\"The Generalized Kelly coinflip game is an extension by ArthurB & Gwern Branwen\n    which expands the Kelly coinflip game MDP into a POMDP, where the 3 key parameters\n    (edge, maximum wealth, and number of rounds) are unknown random variables drawn\n    from 3 distributions: a Beta(7,3) for the coinflip edge 0-1, a N(300,25) the total\n    number of rounds, and a Pareto(5,200) for the wealth cap. These distributions are\n    chosen to be conjugate & easily updatable, to allow for inference (other choices\n    like the geometric for number of rounds wouldn't make observations informative),\n    and to loosely reflect what a human might expect in the original Kelly coinflip\n    game given that the number of rounds wasn't strictly fixed and they weren't told\n    the wealth cap until they neared it. With these particular distributions, the\n    entire history of the game can be summarized into a few sufficient statistics of\n    rounds-elapsed/wins/losses/max-wealth-ever-reached, from which the Bayes-optimal\n    decision can (in theory) be made; to avoid all agents having to tediously track\n    those sufficient statistics manually in the same way, the observation space is\n    augmented from wealth/rounds-left (rounds-left is deleted because it is a hidden\n    variable) to current-wealth/rounds-elapsed/wins/losses/maximum-observed-wealth.\n    The simple Kelly coinflip game can easily be solved by calculating decision trees,\n    but the Generalized Kelly coinflip game may be intractable (although the analysis\n    for the edge case alone suggests that the Bayes-optimal value may be very close to\n    what one would calculate using a decision tree for any specific case), and\n    represents a good challenge for RL agents.\"\"\"\n    metadata = {'render.modes': ['human']}\n\n    def __init__(self, initial_wealth=25.0, edge_prior_alpha=7, edge_prior_beta=3,\n                 max_wealth_alpha=5.0, max_wealth_m=200.0, max_rounds_mean=300.0,\n                 max_rounds_sd=25.0, reseed=True):\n        # store the hyper-parameters for passing back into __init__() during resets so\n        # the same hyper-parameters govern the next game's parameters, as the user\n        # expects:\n        # TODO: this is boilerplate, is there any more elegant way to do this?\n        self.initial_wealth = float(initial_wealth)\n        self.edge_prior_alpha = edge_prior_alpha\n        self.edge_prior_beta = edge_prior_beta\n        self.max_wealth_alpha = max_wealth_alpha\n        self.max_wealth_m = max_wealth_m\n        self.max_rounds_mean = max_rounds_mean\n        self.max_rounds_sd = max_rounds_sd\n\n        if reseed or not hasattr(self, 'np_random'):\n            self.seed()\n\n        # draw this game's set of parameters:\n        edge = self.np_random.beta(edge_prior_alpha, edge_prior_beta)\n        max_wealth = round(genpareto.rvs(max_wealth_alpha, max_wealth_m,\n                                         random_state=self.np_random))\n        max_rounds = int(round(self.np_random.normal(max_rounds_mean, max_rounds_sd)))\n\n        # add an additional global variable which is the sufficient statistic for the\n        # Pareto distribution on wealth cap; alpha doesn't update, but x_m does, and\n        # simply is the highest wealth count we've seen to date:\n        self.max_ever_wealth = float(self.initial_wealth)\n        # for the coinflip edge, it is total wins/losses:\n        self.wins = 0\n        self.losses = 0\n        # for the number of rounds, we need to remember how many rounds we've played:\n        self.rounds_elapsed = 0\n\n        # the rest proceeds as before:\n        self.action_space = spaces.Discrete(int(max_wealth*100))\n        self.observation_space = spaces.Tuple((\n            spaces.Box(0, max_wealth, shape=[1], dtype=np.float32),  # current wealth\n            spaces.Discrete(max_rounds+1),  # rounds elapsed\n            spaces.Discrete(max_rounds+1),  # wins\n            spaces.Discrete(max_rounds+1),  # losses\n            spaces.Box(0, max_wealth, [1], dtype=np.float32)))  # maximum observed wealth\n        self.reward_range = (0, max_wealth)\n        self.edge = edge\n        self.wealth = self.initial_wealth\n        self.max_rounds = max_rounds\n        self.rounds = self.max_rounds\n        self.max_wealth = max_wealth\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def step(self, action):\n        bet_in_dollars = min(action/100.0, self.wealth)\n\n        self.rounds -= 1\n\n        coinflip = flip(self.edge, self.np_random)\n        self.wealth = min(self.max_wealth, self.wealth + coinflip * bet_in_dollars)\n        self.rounds_elapsed += 1\n\n        if coinflip:\n            self.max_ever_wealth = max(self.wealth, self.max_ever_wealth)\n            self.wins += 1\n        else:\n            self.losses += 1\n\n        done = self.wealth < 0.01 or self.wealth == self.max_wealth or not self.rounds\n        reward = self.wealth if done else 0.0\n\n        return self._get_obs(), reward, done, {}\n\n    def _get_obs(self):\n        return (np.array([float(self.wealth)]), self.rounds_elapsed, self.wins,\n                self.losses, np.array([float(self.max_ever_wealth)]))\n\n    def reset(self):\n        # re-init everything to draw new parameters etc, but preserve the RNG for\n        # reproducibility and pass in the same hyper-parameters as originally specified:\n        self.__init__(initial_wealth=self.initial_wealth,\n                      edge_prior_alpha=self.edge_prior_alpha,\n                      edge_prior_beta=self.edge_prior_beta,\n                      max_wealth_alpha=self.max_wealth_alpha,\n                      max_wealth_m=self.max_wealth_m,\n                      max_rounds_mean=self.max_rounds_mean,\n                      max_rounds_sd=self.max_rounds_sd,\n                      reseed=False)\n        return self._get_obs()\n\n    def render(self, mode='human'):\n        print(\"Current wealth: \", self.wealth, \"; Rounds left: \", self.rounds,\n              \"; True edge: \", self.edge, \"; True max wealth: \", self.max_wealth,\n              \"; True stopping time: \", self.max_rounds, \"; Rounds left: \",\n              self.max_rounds - self.rounds_elapsed)",
        "from gym.envs.unittest.cube_crash import CubeCrash\nfrom gym.envs.unittest.cube_crash import CubeCrashSparse\nfrom gym.envs.unittest.cube_crash import CubeCrashScreenBecomesBlack\nfrom gym.envs.unittest.memorize_digits import MemorizeDigits\n",
        "import numpy as np\nimport gym\nfrom gym import spaces\nfrom gym.utils import seeding\n\n# Unit test environment for CNNs.\n# Looks like this (RGB observations):\n#\n#  ---------------------------\n# |                           |\n# |         ******            |\n# |         ******            |\n# |       **      **          |\n# |       **      **          |\n# |               **          |\n# |               **          |\n# |           ****            |\n# |           ****            |\n# |       ****                |\n# |       ****                |\n# |       **********          |\n# |       **********          |\n# |                           |\n#  ---------------------------\n#\n# Agent should hit action 2 to gain reward. Catches off-by-one errors in your agent.\n#\n# To see how it works, run:\n#\n# python examples/agents/keyboard_agent.py MemorizeDigits-v0\n\nFIELD_W = 32\nFIELD_H = 24\n\nbogus_mnist = \\\n[[\n\" **** \",\n\"*    *\",\n\"*    *\",\n\"*    *\",\n\"*    *\",\n\" **** \"\n], [\n\"  **  \",\n\" * *  \",\n\"   *  \",\n\"   *  \",\n\"   *  \",\n\"  *** \"\n], [\n\" **** \",\n\"*    *\",\n\"     *\",\n\"  *** \",\n\"**    \",\n\"******\"\n], [\n\" **** \",\n\"*    *\",\n\"   ** \",\n\"     *\",\n\"*    *\",\n\" **** \"\n], [\n\" *  * \",\n\" *  * \",\n\" *  * \",\n\" **** \",\n\"    * \",\n\"    * \"\n], [\n\" **** \",\n\" *    \",\n\" **** \",\n\"    * \",\n\"    * \",\n\" **** \"\n], [\n\"  *** \",\n\" *    \",\n\" **** \",\n\" *  * \",\n\" *  * \",\n\" **** \"\n], [\n\" **** \",\n\"    * \",\n\"   *  \",\n\"   *  \",\n\"  *   \",\n\"  *   \"\n], [\n\" **** \",\n\"*    *\",\n\" **** \",\n\"*    *\",\n\"*    *\",\n\" **** \"\n], [\n\" **** \",\n\"*    *\",\n\"*    *\",\n\" *****\",\n\"     *\",\n\" **** \"\n]]\n\ncolor_black = np.array((0,0,0)).astype('float32')\ncolor_white = np.array((255,255,255)).astype('float32')\n\nclass MemorizeDigits(gym.Env):\n    metadata = {\n        'render.modes': ['human', 'rgb_array'],\n        'video.frames_per_second' : 60,\n        'video.res_w' : FIELD_W,\n        'video.res_h' : FIELD_H,\n    }\n\n    use_random_colors = False\n\n    def __init__(self):\n        self.seed()\n        self.viewer = None\n        self.observation_space = spaces.Box(0, 255, (FIELD_H,FIELD_W,3), dtype=np.uint8)\n        self.action_space = spaces.Discrete(10)\n        self.bogus_mnist = np.zeros( (10,6,6), dtype=np.uint8 )\n        for digit in range(10):\n            for y in range(6):\n                self.bogus_mnist[digit,y,:] = [ord(char) for char in bogus_mnist[digit][y]]\n        self.reset()\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def random_color(self):\n        return np.array([\n            self.np_random.randint(low=0, high=255),\n            self.np_random.randint(low=0, high=255),\n            self.np_random.randint(low=0, high=255),\n            ]).astype('uint8')\n\n    def reset(self):\n        self.digit_x = self.np_random.randint(low=FIELD_W//5, high=FIELD_W//5*4)\n        self.digit_y = self.np_random.randint(low=FIELD_H//5, high=FIELD_H//5*4)\n        self.color_bg = self.random_color() if self.use_random_colors else color_black\n        self.step_n = 0\n        while 1:\n            self.color_digit = self.random_color() if self.use_random_colors else color_white\n            if np.linalg.norm(self.color_digit - self.color_bg) < 50: continue\n            break\n        self.digit = -1\n        return self.step(0)[0]\n\n    def step(self, action):\n        reward = -1\n        done = False\n        self.step_n += 1\n        if self.digit==-1:\n            pass\n        else:\n            if self.digit==action:\n                reward = +1\n            done = self.step_n > 20 and 0==self.np_random.randint(low=0, high=5)\n        self.digit = self.np_random.randint(low=0, high=10)\n        obs = np.zeros( (FIELD_H,FIELD_W,3), dtype=np.uint8 )\n        obs[:,:,:] = self.color_bg\n        digit_img = np.zeros( (6,6,3), dtype=np.uint8 )\n        digit_img[:] = self.color_bg\n        xxx = self.bogus_mnist[self.digit]==42\n        digit_img[xxx] = self.color_digit\n        obs[self.digit_y-3:self.digit_y+3, self.digit_x-3:self.digit_x+3] = digit_img\n        self.last_obs = obs\n        return obs, reward, done, {}\n\n    def render(self, mode='human'):\n        if mode == 'rgb_array':\n            return self.last_obs\n\n        elif mode == 'human':\n            from gym.envs.classic_control import rendering\n            if self.viewer is None:\n                self.viewer = rendering.SimpleImageViewer()\n            self.viewer.imshow(self.last_obs)\n            return self.viewer.isopen\n\n        else:\n            assert 0, \"Render mode '%s' is not supported\" % mode\n\n    def close(self):\n        if self.viewer is not None:\n            self.viewer.close()\n            self.viewer = None\n",
        "import numpy as np\nimport gym\nfrom gym import spaces\nfrom gym.utils import seeding\n\n# Unit test environment for CNNs and CNN+RNN algorithms.\n# Looks like this (RGB observations):\n#\n#  ---------------------------\n# |                           |\n# |                           |\n# |                           |\n# |          **               |\n# |          **               |\n# |                           |\n# |                           |\n# |                           |\n# |                           |\n# |                           |\n#  ========     ==============\n#\n# Goal is to go through the hole at the bottom. Agent controls square using Left-Nop-Right actions.\n# It falls down automatically, episode length is a bit less than FIELD_H\n#\n# CubeCrash-v0                    # shaped reward\n# CubeCrashSparse-v0              # reward 0 or 1 at the end\n# CubeCrashScreenBecomesBlack-v0  # for RNNs\n#\n# To see how it works, run:\n#\n# python examples/agents/keyboard_agent.py CubeCrashScreen-v0\n\nFIELD_W = 32\nFIELD_H = 40\nHOLE_WIDTH = 8\n\ncolor_black = np.array((0,0,0)).astype('float32')\ncolor_white = np.array((255,255,255)).astype('float32')\ncolor_green = np.array((0,255,0)).astype('float32')\n\nclass CubeCrash(gym.Env):\n    metadata = {\n        'render.modes': ['human', 'rgb_array'],\n        'video.frames_per_second' : 60,\n        'video.res_w' : FIELD_W,\n        'video.res_h' : FIELD_H,\n    }\n\n    use_shaped_reward = True\n    use_black_screen  = False\n    use_random_colors = False   # Makes env too hard\n\n    def __init__(self):\n        self.seed()\n        self.viewer = None\n\n        self.observation_space = spaces.Box(0, 255, (FIELD_H,FIELD_W,3), dtype=np.uint8)\n        self.action_space = spaces.Discrete(3)\n\n        self.reset()\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def random_color(self):\n        return np.array([\n            self.np_random.randint(low=0, high=255),\n            self.np_random.randint(low=0, high=255),\n            self.np_random.randint(low=0, high=255),\n            ]).astype('uint8')\n\n    def reset(self):\n        self.cube_x = self.np_random.randint(low=3, high=FIELD_W-3)\n        self.cube_y = self.np_random.randint(low=3, high=FIELD_H//6)\n        self.hole_x = self.np_random.randint(low=HOLE_WIDTH, high=FIELD_W-HOLE_WIDTH)\n        self.bg_color = self.random_color() if self.use_random_colors else color_black\n        self.potential  = None\n        self.step_n = 0\n        while 1:\n            self.wall_color = self.random_color() if self.use_random_colors else color_white\n            self.cube_color = self.random_color() if self.use_random_colors else color_green\n            if np.linalg.norm(self.wall_color - self.bg_color) < 50 or np.linalg.norm(self.cube_color - self.bg_color) < 50: continue\n            break\n        return self.step(0)[0]\n\n    def step(self, action):\n        if action==0: pass\n        elif action==1: self.cube_x -= 1\n        elif action==2: self.cube_x += 1\n        else: assert 0, \"Action %i is out of range\" % action\n        self.cube_y += 1\n        self.step_n += 1\n\n        obs = np.zeros( (FIELD_H,FIELD_W,3), dtype=np.uint8 )\n        obs[:,:,:] = self.bg_color\n        obs[FIELD_H-5:FIELD_H,:,:] = self.wall_color\n        obs[FIELD_H-5:FIELD_H, self.hole_x-HOLE_WIDTH//2:self.hole_x+HOLE_WIDTH//2+1, :] = self.bg_color\n        obs[self.cube_y-1:self.cube_y+2, self.cube_x-1:self.cube_x+2, :] = self.cube_color\n        if self.use_black_screen and self.step_n > 4:\n            obs[:] = np.zeros((3,), dtype=np.uint8)\n\n        done = False\n        reward = 0\n        dist = np.abs(self.cube_x - self.hole_x)\n        if self.potential is not None and self.use_shaped_reward:\n            reward = (self.potential - dist) * 0.01\n        self.potential = dist\n\n        if self.cube_x-1 < 0 or self.cube_x+1 >= FIELD_W:\n            done = True\n            reward = -1\n        elif self.cube_y+1 >= FIELD_H-5:\n            if dist >= HOLE_WIDTH//2:\n                done = True\n                reward = -1\n            elif self.cube_y == FIELD_H:\n                done = True\n                reward = +1\n        self.last_obs = obs\n        return obs, reward, done, {}\n\n    def render(self, mode='human'):\n        if mode == 'rgb_array':\n            return self.last_obs\n\n        elif mode == 'human':\n            from gym.envs.classic_control import rendering\n            if self.viewer is None:\n                self.viewer = rendering.SimpleImageViewer()\n            self.viewer.imshow(self.last_obs)\n            return self.viewer.isopen\n\n        else:\n            assert 0, \"Render mode '%s' is not supported\" % mode\n\n    def close(self):\n        if self.viewer is not None:\n            self.viewer.close()\n            self.viewer = None\n\nclass CubeCrashSparse(CubeCrash):\n    use_shaped_reward = False\n\nclass CubeCrashScreenBecomesBlack(CubeCrash):\n    use_shaped_reward = False\n    use_black_screen = True\n",
        "import numpy as np\nimport os\nimport gym\nfrom gym import error, spaces\nfrom gym import utils\nfrom gym.utils import seeding\n\ntry:\n    import atari_py\nexcept ImportError as e:\n    raise error.DependencyNotInstalled(\n            \"{}. (HINT: you can install Atari dependencies by running \"\n            \"'pip install gym[atari]'.)\".format(e))\n\n\ndef to_ram(ale):\n    ram_size = ale.getRAMSize()\n    ram = np.zeros((ram_size), dtype=np.uint8)\n    ale.getRAM(ram)\n    return ram\n\n\nclass AtariEnv(gym.Env, utils.EzPickle):\n    metadata = {'render.modes': ['human', 'rgb_array']}\n\n    def __init__(\n            self,\n            game='pong',\n            mode=None,\n            difficulty=None,\n            obs_type='ram',\n            frameskip=(2, 5),\n            repeat_action_probability=0.,\n            full_action_space=False):\n        \"\"\"Frameskip should be either a tuple (indicating a random range to\n        choose from, with the top value exclude), or an int.\"\"\"\n\n        utils.EzPickle.__init__(\n                self,\n                game,\n                mode,\n                difficulty,\n                obs_type,\n                frameskip,\n                repeat_action_probability)\n        assert obs_type in ('ram', 'image')\n\n        self.game = game\n        self.game_path = atari_py.get_game_path(game)\n        self.game_mode = mode\n        self.game_difficulty = difficulty\n\n        if not os.path.exists(self.game_path):\n            msg = 'You asked for game %s but path %s does not exist'\n            raise IOError(msg % (game, self.game_path))\n        self._obs_type = obs_type\n        self.frameskip = frameskip\n        self.ale = atari_py.ALEInterface()\n        self.viewer = None\n\n        # Tune (or disable) ALE's action repeat:\n        # https://github.com/openai/gym/issues/349\n        assert isinstance(repeat_action_probability, (float, int)), \\\n                \"Invalid repeat_action_probability: {!r}\".format(repeat_action_probability)\n        self.ale.setFloat(\n                'repeat_action_probability'.encode('utf-8'),\n                repeat_action_probability)\n\n        self.seed()\n\n        self._action_set = (self.ale.getLegalActionSet() if full_action_space\n                            else self.ale.getMinimalActionSet())\n        self.action_space = spaces.Discrete(len(self._action_set))\n\n        (screen_width, screen_height) = self.ale.getScreenDims()\n        if self._obs_type == 'ram':\n            self.observation_space = spaces.Box(low=0, high=255, dtype=np.uint8, shape=(128,))\n        elif self._obs_type == 'image':\n            self.observation_space = spaces.Box(low=0, high=255, shape=(screen_height, screen_width, 3), dtype=np.uint8)\n        else:\n            raise error.Error('Unrecognized observation type: {}'.format(self._obs_type))\n\n    def seed(self, seed=None):\n        self.np_random, seed1 = seeding.np_random(seed)\n        # Derive a random seed. This gets passed as a uint, but gets\n        # checked as an int elsewhere, so we need to keep it below\n        # 2**31.\n        seed2 = seeding.hash_seed(seed1 + 1) % 2**31\n        # Empirically, we need to seed before loading the ROM.\n        self.ale.setInt(b'random_seed', seed2)\n        self.ale.loadROM(self.game_path)\n\n        if self.game_mode is not None:\n            modes = self.ale.getAvailableModes()\n\n            assert self.game_mode in modes, (\n                \"Invalid game mode \\\"{}\\\" for game {}.\\nAvailable modes are: {}\"\n            ).format(self.game_mode, self.game, modes)\n            self.ale.setMode(self.game_mode)\n\n        if self.game_difficulty is not None:\n            difficulties = self.ale.getAvailableDifficulties()\n\n            assert self.game_difficulty in difficulties, (\n                \"Invalid game difficulty \\\"{}\\\" for game {}.\\nAvailable difficulties are: {}\"\n            ).format(self.game_difficulty, self.game, difficulties)\n            self.ale.setDifficulty(self.game_difficulty)\n\n        return [seed1, seed2]\n\n    def step(self, a):\n        reward = 0.0\n        action = self._action_set[a]\n\n        if isinstance(self.frameskip, int):\n            num_steps = self.frameskip\n        else:\n            num_steps = self.np_random.randint(self.frameskip[0], self.frameskip[1])\n        for _ in range(num_steps):\n            reward += self.ale.act(action)\n        ob = self._get_obs()\n\n        return ob, reward, self.ale.game_over(), {\"ale.lives\": self.ale.lives()}\n\n    def _get_image(self):\n        return self.ale.getScreenRGB2()\n\n    def _get_ram(self):\n        return to_ram(self.ale)\n\n    @property\n    def _n_actions(self):\n        return len(self._action_set)\n\n    def _get_obs(self):\n        if self._obs_type == 'ram':\n            return self._get_ram()\n        elif self._obs_type == 'image':\n            img = self._get_image()\n        return img\n\n    # return: (states, observations)\n    def reset(self):\n        self.ale.reset_game()\n        return self._get_obs()\n\n    def render(self, mode='human'):\n        img = self._get_image()\n        if mode == 'rgb_array':\n            return img\n        elif mode == 'human':\n            from gym.envs.classic_control import rendering\n            if self.viewer is None:\n                self.viewer = rendering.SimpleImageViewer()\n            self.viewer.imshow(img)\n            return self.viewer.isopen\n\n    def close(self):\n        if self.viewer is not None:\n            self.viewer.close()\n            self.viewer = None\n\n    def get_action_meanings(self):\n        return [ACTION_MEANING[i] for i in self._action_set]\n\n    def get_keys_to_action(self):\n        KEYWORD_TO_KEY = {\n            'UP':      ord('w'),\n            'DOWN':    ord('s'),\n            'LEFT':    ord('a'),\n            'RIGHT':   ord('d'),\n            'FIRE':    ord(' '),\n        }\n\n        keys_to_action = {}\n\n        for action_id, action_meaning in enumerate(self.get_action_meanings()):\n            keys = []\n            for keyword, key in KEYWORD_TO_KEY.items():\n                if keyword in action_meaning:\n                    keys.append(key)\n            keys = tuple(sorted(keys))\n\n            assert keys not in keys_to_action\n            keys_to_action[keys] = action_id\n\n        return keys_to_action\n\n    def clone_state(self):\n        \"\"\"Clone emulator state w/o system state. Restoring this state will\n        *not* give an identical environment. For complete cloning and restoring\n        of the full state, see `{clone,restore}_full_state()`.\"\"\"\n        state_ref = self.ale.cloneState()\n        state = self.ale.encodeState(state_ref)\n        self.ale.deleteState(state_ref)\n        return state\n\n    def restore_state(self, state):\n        \"\"\"Restore emulator state w/o system state.\"\"\"\n        state_ref = self.ale.decodeState(state)\n        self.ale.restoreState(state_ref)\n        self.ale.deleteState(state_ref)\n\n    def clone_full_state(self):\n        \"\"\"Clone emulator state w/ system state including pseudorandomness.\n        Restoring this state will give an identical environment.\"\"\"\n        state_ref = self.ale.cloneSystemState()\n        state = self.ale.encodeState(state_ref)\n        self.ale.deleteState(state_ref)\n        return state\n\n    def restore_full_state(self, state):\n        \"\"\"Restore emulator state w/ system state including pseudorandomness.\"\"\"\n        state_ref = self.ale.decodeState(state)\n        self.ale.restoreSystemState(state_ref)\n        self.ale.deleteState(state_ref)\n\n\nACTION_MEANING = {\n    0: \"NOOP\",\n    1: \"FIRE\",\n    2: \"UP\",\n    3: \"RIGHT\",\n    4: \"LEFT\",\n    5: \"DOWN\",\n    6: \"UPRIGHT\",\n    7: \"UPLEFT\",\n    8: \"DOWNRIGHT\",\n    9: \"DOWNLEFT\",\n    10: \"UPFIRE\",\n    11: \"RIGHTFIRE\",\n    12: \"LEFTFIRE\",\n    13: \"DOWNFIRE\",\n    14: \"UPRIGHTFIRE\",\n    15: \"UPLEFTFIRE\",\n    16: \"DOWNRIGHTFIRE\",\n    17: \"DOWNLEFTFIRE\",\n}",
        "from gym.envs.atari.atari_env import AtariEnv",
        "import os\nimport numpy as np\n\nfrom gym import utils\nfrom gym.envs.robotics import hand_env\nfrom gym.envs.robotics.utils import robot_get_obs\n\n\nFINGERTIP_SITE_NAMES = [\n    'robot0:S_fftip',\n    'robot0:S_mftip',\n    'robot0:S_rftip',\n    'robot0:S_lftip',\n    'robot0:S_thtip',\n]\n\n\nDEFAULT_INITIAL_QPOS = {\n    'robot0:WRJ1': -0.16514339750464327,\n    'robot0:WRJ0': -0.31973286565062153,\n    'robot0:FFJ3': 0.14340512546557435,\n    'robot0:FFJ2': 0.32028208333591573,\n    'robot0:FFJ1': 0.7126053607727917,\n    'robot0:FFJ0': 0.6705281001412586,\n    'robot0:MFJ3': 0.000246444303701037,\n    'robot0:MFJ2': 0.3152655251085491,\n    'robot0:MFJ1': 0.7659800313729842,\n    'robot0:MFJ0': 0.7323156897425923,\n    'robot0:RFJ3': 0.00038520700007378114,\n    'robot0:RFJ2': 0.36743546201985233,\n    'robot0:RFJ1': 0.7119514095008576,\n    'robot0:RFJ0': 0.6699446327514138,\n    'robot0:LFJ4': 0.0525442258033891,\n    'robot0:LFJ3': -0.13615534724474673,\n    'robot0:LFJ2': 0.39872030433433003,\n    'robot0:LFJ1': 0.7415570009679252,\n    'robot0:LFJ0': 0.704096378652974,\n    'robot0:THJ4': 0.003673823825070126,\n    'robot0:THJ3': 0.5506291436028695,\n    'robot0:THJ2': -0.014515151997119306,\n    'robot0:THJ1': -0.0015229223564485414,\n    'robot0:THJ0': -0.7894883021600622,\n}\n\n\n# Ensure we get the path separator correct on windows\nMODEL_XML_PATH = os.path.join('hand', 'reach.xml')\n\n\ndef goal_distance(goal_a, goal_b):\n    assert goal_a.shape == goal_b.shape\n    return np.linalg.norm(goal_a - goal_b, axis=-1)\n\n\nclass HandReachEnv(hand_env.HandEnv, utils.EzPickle):\n    def __init__(\n        self, distance_threshold=0.01, n_substeps=20, relative_control=False,\n        initial_qpos=DEFAULT_INITIAL_QPOS, reward_type='sparse',\n    ):\n        utils.EzPickle.__init__(**locals())\n        self.distance_threshold = distance_threshold\n        self.reward_type = reward_type\n\n        hand_env.HandEnv.__init__(\n            self, MODEL_XML_PATH, n_substeps=n_substeps, initial_qpos=initial_qpos,\n            relative_control=relative_control)\n\n    def _get_achieved_goal(self):\n        goal = [self.sim.data.get_site_xpos(name) for name in FINGERTIP_SITE_NAMES]\n        return np.array(goal).flatten()\n\n    # GoalEnv methods\n    # ----------------------------\n\n    def compute_reward(self, achieved_goal, goal, info):\n        d = goal_distance(achieved_goal, goal)\n        if self.reward_type == 'sparse':\n            return -(d > self.distance_threshold).astype(np.float32)\n        else:\n            return -d\n\n    # RobotEnv methods\n    # ----------------------------\n\n    def _env_setup(self, initial_qpos):\n        for name, value in initial_qpos.items():\n            self.sim.data.set_joint_qpos(name, value)\n        self.sim.forward()\n\n        self.initial_goal = self._get_achieved_goal().copy()\n        self.palm_xpos = self.sim.data.body_xpos[self.sim.model.body_name2id('robot0:palm')].copy()\n\n    def _get_obs(self):\n        robot_qpos, robot_qvel = robot_get_obs(self.sim)\n        achieved_goal = self._get_achieved_goal().ravel()\n        observation = np.concatenate([robot_qpos, robot_qvel, achieved_goal])\n        return {\n            'observation': observation.copy(),\n            'achieved_goal': achieved_goal.copy(),\n            'desired_goal': self.goal.copy(),\n        }\n\n    def _sample_goal(self):\n        thumb_name = 'robot0:S_thtip'\n        finger_names = [name for name in FINGERTIP_SITE_NAMES if name != thumb_name]\n        finger_name = self.np_random.choice(finger_names)\n\n        thumb_idx = FINGERTIP_SITE_NAMES.index(thumb_name)\n        finger_idx = FINGERTIP_SITE_NAMES.index(finger_name)\n        assert thumb_idx != finger_idx\n\n        # Pick a meeting point above the hand.\n        meeting_pos = self.palm_xpos + np.array([0.0, -0.09, 0.05])\n        meeting_pos += self.np_random.normal(scale=0.005, size=meeting_pos.shape)\n\n        # Slightly move meeting goal towards the respective finger to avoid that they\n        # overlap.\n        goal = self.initial_goal.copy().reshape(-1, 3)\n        for idx in [thumb_idx, finger_idx]:\n            offset_direction = (meeting_pos - goal[idx])\n            offset_direction /= np.linalg.norm(offset_direction)\n            goal[idx] = meeting_pos - 0.005 * offset_direction\n\n        if self.np_random.uniform() < 0.1:\n            # With some probability, ask all fingers to move back to the origin.\n            # This avoids that the thumb constantly stays near the goal position already.\n            goal = self.initial_goal.copy()\n        return goal.flatten()\n\n    def _is_success(self, achieved_goal, desired_goal):\n        d = goal_distance(achieved_goal, desired_goal)\n        return (d < self.distance_threshold).astype(np.float32)\n\n    def _render_callback(self):\n        # Visualize targets.\n        sites_offset = (self.sim.data.site_xpos - self.sim.model.site_pos).copy()\n        goal = self.goal.reshape(5, 3)\n        for finger_idx in range(5):\n            site_name = 'target{}'.format(finger_idx)\n            site_id = self.sim.model.site_name2id(site_name)\n            self.sim.model.site_pos[site_id] = goal[finger_idx] - sites_offset[site_id]\n\n        # Visualize finger positions.\n        achieved_goal = self._get_achieved_goal().reshape(5, 3)\n        for finger_idx in range(5):\n            site_name = 'finger{}'.format(finger_idx)\n            site_id = self.sim.model.site_name2id(site_name)\n            self.sim.model.site_pos[site_id] = achieved_goal[finger_idx] - sites_offset[site_id]\n        self.sim.forward()",
        "",
        "import os\nimport numpy as np\n\nfrom gym import utils, error\nfrom gym.envs.robotics import rotations, hand_env\nfrom gym.envs.robotics.utils import robot_get_obs\n\ntry:\n    import mujoco_py\nexcept ImportError as e:\n    raise error.DependencyNotInstalled(\"{}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)\".format(e))\n\n\ndef quat_from_angle_and_axis(angle, axis):\n    assert axis.shape == (3,)\n    axis /= np.linalg.norm(axis)\n    quat = np.concatenate([[np.cos(angle / 2.)], np.sin(angle / 2.) * axis])\n    quat /= np.linalg.norm(quat)\n    return quat\n\n\n# Ensure we get the path separator correct on windows\nMANIPULATE_BLOCK_XML = os.path.join('hand', 'manipulate_block.xml')\nMANIPULATE_EGG_XML = os.path.join('hand', 'manipulate_egg.xml')\nMANIPULATE_PEN_XML = os.path.join('hand', 'manipulate_pen.xml')\n\n\nclass ManipulateEnv(hand_env.HandEnv):\n    def __init__(\n        self, model_path, target_position, target_rotation,\n        target_position_range, reward_type, initial_qpos=None,\n        randomize_initial_position=True, randomize_initial_rotation=True,\n        distance_threshold=0.01, rotation_threshold=0.1, n_substeps=20, relative_control=False,\n        ignore_z_target_rotation=False,\n    ):\n        \"\"\"Initializes a new Hand manipulation environment.\n\n        Args:\n            model_path (string): path to the environments XML file\n            target_position (string): the type of target position:\n                - ignore: target position is fully ignored, i.e. the object can be positioned arbitrarily\n                - fixed: target position is set to the initial position of the object\n                - random: target position is fully randomized according to target_position_range\n            target_rotation (string): the type of target rotation:\n                - ignore: target rotation is fully ignored, i.e. the object can be rotated arbitrarily\n                - fixed: target rotation is set to the initial rotation of the object\n                - xyz: fully randomized target rotation around the X, Y and Z axis\n                - z: fully randomized target rotation around the Z axis\n                - parallel: fully randomized target rotation around Z and axis-aligned rotation around X, Y\n            ignore_z_target_rotation (boolean): whether or not the Z axis of the target rotation is ignored\n            target_position_range (np.array of shape (3, 2)): range of the target_position randomization\n            reward_type ('sparse' or 'dense'): the reward type, i.e. sparse or dense\n            initial_qpos (dict): a dictionary of joint names and values that define the initial configuration\n            randomize_initial_position (boolean): whether or not to randomize the initial position of the object\n            randomize_initial_rotation (boolean): whether or not to randomize the initial rotation of the object\n            distance_threshold (float, in meters): the threshold after which the position of a goal is considered achieved\n            rotation_threshold (float, in radians): the threshold after which the rotation of a goal is considered achieved\n            n_substeps (int): number of substeps the simulation runs on every call to step\n            relative_control (boolean): whether or not the hand is actuated in absolute joint positions or relative to the current state\n        \"\"\"\n        self.target_position = target_position\n        self.target_rotation = target_rotation\n        self.target_position_range = target_position_range\n        self.parallel_quats = [rotations.euler2quat(r) for r in rotations.get_parallel_rotations()]\n        self.randomize_initial_rotation = randomize_initial_rotation\n        self.randomize_initial_position = randomize_initial_position\n        self.distance_threshold = distance_threshold\n        self.rotation_threshold = rotation_threshold\n        self.reward_type = reward_type\n        self.ignore_z_target_rotation = ignore_z_target_rotation\n\n        assert self.target_position in ['ignore', 'fixed', 'random']\n        assert self.target_rotation in ['ignore', 'fixed', 'xyz', 'z', 'parallel']\n        initial_qpos = initial_qpos or {}\n\n        hand_env.HandEnv.__init__(\n            self, model_path, n_substeps=n_substeps, initial_qpos=initial_qpos,\n            relative_control=relative_control)\n\n\n    def _get_achieved_goal(self):\n        # Object position and rotation.\n        object_qpos = self.sim.data.get_joint_qpos('object:joint')\n        assert object_qpos.shape == (7,)\n        return object_qpos\n\n    def _goal_distance(self, goal_a, goal_b):\n        assert goal_a.shape == goal_b.shape\n        assert goal_a.shape[-1] == 7\n\n        d_pos = np.zeros_like(goal_a[..., 0])\n        d_rot = np.zeros_like(goal_b[..., 0])\n        if self.target_position != 'ignore':\n            delta_pos = goal_a[..., :3] - goal_b[..., :3]\n            d_pos = np.linalg.norm(delta_pos, axis=-1)\n\n        if self.target_rotation != 'ignore':\n            quat_a, quat_b = goal_a[..., 3:], goal_b[..., 3:]\n\n            if self.ignore_z_target_rotation:\n                # Special case: We want to ignore the Z component of the rotation.\n                # This code here assumes Euler angles with xyz convention. We first transform\n                # to euler, then set the Z component to be equal between the two, and finally\n                # transform back into quaternions.\n                euler_a = rotations.quat2euler(quat_a)\n                euler_b = rotations.quat2euler(quat_b)\n                euler_a[2] = euler_b[2]\n                quat_a = rotations.euler2quat(euler_a)\n\n            # Subtract quaternions and extract angle between them.\n            quat_diff = rotations.quat_mul(quat_a, rotations.quat_conjugate(quat_b))\n            angle_diff = 2 * np.arccos(np.clip(quat_diff[..., 0], -1., 1.))\n            d_rot = angle_diff\n        assert d_pos.shape == d_rot.shape\n        return d_pos, d_rot\n\n    # GoalEnv methods\n    # ----------------------------\n\n    def compute_reward(self, achieved_goal, goal, info):\n        if self.reward_type == 'sparse':\n            success = self._is_success(achieved_goal, goal).astype(np.float32)\n            return (success - 1.)\n        else:\n            d_pos, d_rot = self._goal_distance(achieved_goal, goal)\n            # We weigh the difference in position to avoid that `d_pos` (in meters) is completely\n            # dominated by `d_rot` (in radians).\n            return -(10. * d_pos + d_rot)\n\n    # RobotEnv methods\n    # ----------------------------\n\n    def _is_success(self, achieved_goal, desired_goal):\n        d_pos, d_rot = self._goal_distance(achieved_goal, desired_goal)\n        achieved_pos = (d_pos < self.distance_threshold).astype(np.float32)\n        achieved_rot = (d_rot < self.rotation_threshold).astype(np.float32)\n        achieved_both = achieved_pos * achieved_rot\n        return achieved_both\n\n    def _env_setup(self, initial_qpos):\n        for name, value in initial_qpos.items():\n            self.sim.data.set_joint_qpos(name, value)\n        self.sim.forward()\n\n    def _reset_sim(self):\n        self.sim.set_state(self.initial_state)\n        self.sim.forward()\n\n        initial_qpos = self.sim.data.get_joint_qpos('object:joint').copy()\n        initial_pos, initial_quat = initial_qpos[:3], initial_qpos[3:]\n        assert initial_qpos.shape == (7,)\n        assert initial_pos.shape == (3,)\n        assert initial_quat.shape == (4,)\n        initial_qpos = None\n\n        # Randomization initial rotation.\n        if self.randomize_initial_rotation:\n            if self.target_rotation == 'z':\n                angle = self.np_random.uniform(-np.pi, np.pi)\n                axis = np.array([0., 0., 1.])\n                offset_quat = quat_from_angle_and_axis(angle, axis)\n                initial_quat = rotations.quat_mul(initial_quat, offset_quat)\n            elif self.target_rotation == 'parallel':\n                angle = self.np_random.uniform(-np.pi, np.pi)\n                axis = np.array([0., 0., 1.])\n                z_quat = quat_from_angle_and_axis(angle, axis)\n                parallel_quat = self.parallel_quats[self.np_random.randint(len(self.parallel_quats))]\n                offset_quat = rotations.quat_mul(z_quat, parallel_quat)\n                initial_quat = rotations.quat_mul(initial_quat, offset_quat)\n            elif self.target_rotation in ['xyz', 'ignore']:\n                angle = self.np_random.uniform(-np.pi, np.pi)\n                axis = self.np_random.uniform(-1., 1., size=3)\n                offset_quat = quat_from_angle_and_axis(angle, axis)\n                initial_quat = rotations.quat_mul(initial_quat, offset_quat)\n            elif self.target_rotation == 'fixed':\n                pass\n            else:\n                raise error.Error('Unknown target_rotation option \"{}\".'.format(self.target_rotation))\n\n        # Randomize initial position.\n        if self.randomize_initial_position:\n            if self.target_position != 'fixed':\n                initial_pos += self.np_random.normal(size=3, scale=0.005)\n\n        initial_quat /= np.linalg.norm(initial_quat)\n        initial_qpos = np.concatenate([initial_pos, initial_quat])\n        self.sim.data.set_joint_qpos('object:joint', initial_qpos)\n\n        def is_on_palm():\n            self.sim.forward()\n            cube_middle_idx = self.sim.model.site_name2id('object:center')\n            cube_middle_pos = self.sim.data.site_xpos[cube_middle_idx]\n            is_on_palm = (cube_middle_pos[2] > 0.04)\n            return is_on_palm\n\n        # Run the simulation for a bunch of timesteps to let everything settle in.\n        for _ in range(10):\n            self._set_action(np.zeros(20))\n            try:\n                self.sim.step()\n            except mujoco_py.MujocoException:\n                return False\n        return is_on_palm()\n\n    def _sample_goal(self):\n        # Select a goal for the object position.\n        target_pos = None\n        if self.target_position == 'random':\n            assert self.target_position_range.shape == (3, 2)\n            offset = self.np_random.uniform(self.target_position_range[:, 0], self.target_position_range[:, 1])\n            assert offset.shape == (3,)\n            target_pos = self.sim.data.get_joint_qpos('object:joint')[:3] + offset\n        elif self.target_position in ['ignore', 'fixed']:\n            target_pos = self.sim.data.get_joint_qpos('object:joint')[:3]\n        else:\n            raise error.Error('Unknown target_position option \"{}\".'.format(self.target_position))\n        assert target_pos is not None\n        assert target_pos.shape == (3,)\n\n        # Select a goal for the object rotation.\n        target_quat = None\n        if self.target_rotation == 'z':\n            angle = self.np_random.uniform(-np.pi, np.pi)\n            axis = np.array([0., 0., 1.])\n            target_quat = quat_from_angle_and_axis(angle, axis)\n        elif self.target_rotation == 'parallel':\n            angle = self.np_random.uniform(-np.pi, np.pi)\n            axis = np.array([0., 0., 1.])\n            target_quat = quat_from_angle_and_axis(angle, axis)\n            parallel_quat = self.parallel_quats[self.np_random.randint(len(self.parallel_quats))]\n            target_quat = rotations.quat_mul(target_quat, parallel_quat)\n        elif self.target_rotation == 'xyz':\n            angle = self.np_random.uniform(-np.pi, np.pi)\n            axis = self.np_random.uniform(-1., 1., size=3)\n            target_quat = quat_from_angle_and_axis(angle, axis)\n        elif self.target_rotation in ['ignore', 'fixed']:\n            target_quat = self.sim.data.get_joint_qpos('object:joint')\n        else:\n            raise error.Error('Unknown target_rotation option \"{}\".'.format(self.target_rotation))\n        assert target_quat is not None\n        assert target_quat.shape == (4,)\n\n        target_quat /= np.linalg.norm(target_quat)  # normalized quaternion\n        goal = np.concatenate([target_pos, target_quat])\n        return goal\n\n    def _render_callback(self):\n        # Assign current state to target object but offset a bit so that the actual object\n        # is not obscured.\n        goal = self.goal.copy()\n        assert goal.shape == (7,)\n        if self.target_position == 'ignore':\n            # Move the object to the side since we do not care about it's position.\n            goal[0] += 0.15\n        self.sim.data.set_joint_qpos('target:joint', goal)\n        self.sim.data.set_joint_qvel('target:joint', np.zeros(6))\n\n        if 'object_hidden' in self.sim.model.geom_names:\n            hidden_id = self.sim.model.geom_name2id('object_hidden')\n            self.sim.model.geom_rgba[hidden_id, 3] = 1.\n        self.sim.forward()\n\n    def _get_obs(self):\n        robot_qpos, robot_qvel = robot_get_obs(self.sim)\n        object_qvel = self.sim.data.get_joint_qvel('object:joint')\n        achieved_goal = self._get_achieved_goal().ravel()  # this contains the object position + rotation\n        observation = np.concatenate([robot_qpos, robot_qvel, object_qvel, achieved_goal])\n        return {\n            'observation': observation.copy(),\n            'achieved_goal': achieved_goal.copy(),\n            'desired_goal': self.goal.ravel().copy(),\n        }\n\n\nclass HandBlockEnv(ManipulateEnv, utils.EzPickle):\n    def __init__(self, target_position='random', target_rotation='xyz', reward_type='sparse'):\n        utils.EzPickle.__init__(self, target_position, target_rotation, reward_type)\n        ManipulateEnv.__init__(self,\n            model_path=MANIPULATE_BLOCK_XML, target_position=target_position,\n            target_rotation=target_rotation,\n            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),\n            reward_type=reward_type)\n\n\nclass HandEggEnv(ManipulateEnv, utils.EzPickle):\n    def __init__(self, target_position='random', target_rotation='xyz', reward_type='sparse'):\n        utils.EzPickle.__init__(self, target_position, target_rotation, reward_type)\n        ManipulateEnv.__init__(self,\n            model_path=MANIPULATE_EGG_XML, target_position=target_position,\n            target_rotation=target_rotation,\n            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),\n            reward_type=reward_type)\n\n\nclass HandPenEnv(ManipulateEnv, utils.EzPickle):\n    def __init__(self, target_position='random', target_rotation='xyz', reward_type='sparse'):\n        utils.EzPickle.__init__(self, target_position, target_rotation, reward_type)\n        ManipulateEnv.__init__(self,\n            model_path=MANIPULATE_PEN_XML, target_position=target_position,\n            target_rotation=target_rotation,\n            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),\n            randomize_initial_rotation=False, reward_type=reward_type,\n            ignore_z_target_rotation=True, distance_threshold=0.05)",
        "import os\nimport numpy as np\n\nfrom gym import utils, error, spaces\nfrom gym.envs.robotics.hand import manipulate\n\n# Ensure we get the path separator correct on windows\nMANIPULATE_BLOCK_XML = os.path.join('hand', 'manipulate_block_touch_sensors.xml')\nMANIPULATE_EGG_XML = os.path.join('hand', 'manipulate_egg_touch_sensors.xml')\nMANIPULATE_PEN_XML = os.path.join('hand', 'manipulate_pen_touch_sensors.xml')\n\n\nclass ManipulateTouchSensorsEnv(manipulate.ManipulateEnv):\n    def __init__(\n        self, model_path, target_position, target_rotation,\n        target_position_range, reward_type, initial_qpos={},\n        randomize_initial_position=True, randomize_initial_rotation=True,\n        distance_threshold=0.01, rotation_threshold=0.1, n_substeps=20, relative_control=False,\n        ignore_z_target_rotation=False, touch_visualisation=\"on_touch\", touch_get_obs=\"sensordata\",\n    ):\n        \"\"\"Initializes a new Hand manipulation environment with touch sensors.\n\n        Args:\n            touch_visualisation (string): how touch sensor sites are visualised\n                - \"on_touch\": shows touch sensor sites only when touch values > 0\n                - \"always\": always shows touch sensor sites\n                - \"off\" or else: does not show touch sensor sites\n            touch_get_obs (string): touch sensor readings\n                - \"boolean\": returns 1 if touch sensor reading != 0.0 else 0\n                - \"sensordata\": returns original touch sensor readings from self.sim.data.sensordata[id]\n                - \"log\": returns log(x+1) touch sensor readings from self.sim.data.sensordata[id]\n                - \"off\" or else: does not add touch sensor readings to the observation\n\n        \"\"\"\n        self.touch_visualisation = touch_visualisation\n        self.touch_get_obs = touch_get_obs\n        self._touch_sensor_id_site_id = []\n        self._touch_sensor_id = []\n        self.touch_color = [1, 0, 0, 0.5]\n        self.notouch_color = [0, 0.5, 0, 0.2]\n\n        manipulate.ManipulateEnv.__init__(\n            self, model_path, target_position, target_rotation,\n            target_position_range, reward_type, initial_qpos=initial_qpos,\n            randomize_initial_position=randomize_initial_position, randomize_initial_rotation=randomize_initial_rotation,\n            distance_threshold=distance_threshold, rotation_threshold=rotation_threshold, n_substeps=n_substeps, relative_control=relative_control,\n            ignore_z_target_rotation=ignore_z_target_rotation,\n        )\n\n        for k, v in self.sim.model._sensor_name2id.items():  # get touch sensor site names and their ids\n            if 'robot0:TS_' in k:\n                self._touch_sensor_id_site_id.append((v, self.sim.model._site_name2id[k.replace('robot0:TS_', 'robot0:T_')]))\n                self._touch_sensor_id.append(v)\n\n        if self.touch_visualisation == 'off':  # set touch sensors rgba values\n            for _, site_id in self._touch_sensor_id_site_id:\n                self.sim.model.site_rgba[site_id][3] = 0.0\n        elif self.touch_visualisation == 'always':\n            pass\n\n        obs = self._get_obs()\n        self.observation_space = spaces.Dict(dict(\n            desired_goal=spaces.Box(-np.inf, np.inf, shape=obs['achieved_goal'].shape, dtype='float32'),\n            achieved_goal=spaces.Box(-np.inf, np.inf, shape=obs['achieved_goal'].shape, dtype='float32'),\n            observation=spaces.Box(-np.inf, np.inf, shape=obs['observation'].shape, dtype='float32'),\n        ))\n\n    def _render_callback(self):\n        super(ManipulateTouchSensorsEnv, self)._render_callback()\n        if self.touch_visualisation == 'on_touch':\n            for touch_sensor_id, site_id in self._touch_sensor_id_site_id:\n                if self.sim.data.sensordata[touch_sensor_id] != 0.0:\n                    self.sim.model.site_rgba[site_id] = self.touch_color\n                else:\n                    self.sim.model.site_rgba[site_id] = self.notouch_color\n\n    def _get_obs(self):\n        robot_qpos, robot_qvel = manipulate.robot_get_obs(self.sim)\n        object_qvel = self.sim.data.get_joint_qvel('object:joint')\n        achieved_goal = self._get_achieved_goal().ravel()  # this contains the object position + rotation\n        touch_values = []  # get touch sensor readings. if there is one, set value to 1\n        if self.touch_get_obs == 'sensordata':\n            touch_values = self.sim.data.sensordata[self._touch_sensor_id]\n        elif self.touch_get_obs == 'boolean':\n            touch_values = self.sim.data.sensordata[self._touch_sensor_id] > 0.0\n        elif self.touch_get_obs == 'log':\n            touch_values = np.log(self.sim.data.sensordata[self._touch_sensor_id] + 1.0)\n        observation = np.concatenate([robot_qpos, robot_qvel, object_qvel, touch_values, achieved_goal])\n\n        return {\n            'observation': observation.copy(),\n            'achieved_goal': achieved_goal.copy(),\n            'desired_goal': self.goal.ravel().copy(),\n        }\n\n\nclass HandBlockTouchSensorsEnv(ManipulateTouchSensorsEnv, utils.EzPickle):\n    def __init__(self, target_position='random', target_rotation='xyz', touch_get_obs='sensordata', reward_type='sparse'):\n        utils.EzPickle.__init__(self, target_position, target_rotation, touch_get_obs, reward_type)\n        ManipulateTouchSensorsEnv.__init__(self,\n            model_path=MANIPULATE_BLOCK_XML,\n            touch_get_obs=touch_get_obs,\n            target_rotation=target_rotation,\n            target_position=target_position,\n            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),\n            reward_type=reward_type)\n\n\nclass HandEggTouchSensorsEnv(ManipulateTouchSensorsEnv, utils.EzPickle):\n    def __init__(self, target_position='random', target_rotation='xyz', touch_get_obs='sensordata', reward_type='sparse'):\n        utils.EzPickle.__init__(self, target_position, target_rotation, touch_get_obs, reward_type)\n        ManipulateTouchSensorsEnv.__init__(self,\n            model_path=MANIPULATE_EGG_XML,\n            touch_get_obs=touch_get_obs,\n            target_rotation=target_rotation,\n            target_position=target_position,\n            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),\n            reward_type=reward_type)\n\n\nclass HandPenTouchSensorsEnv(ManipulateTouchSensorsEnv, utils.EzPickle):\n    def __init__(self, target_position='random', target_rotation='xyz', touch_get_obs='sensordata', reward_type='sparse'):\n        utils.EzPickle.__init__(self, target_position, target_rotation, touch_get_obs, reward_type)\n        ManipulateTouchSensorsEnv.__init__(self,\n            model_path=MANIPULATE_PEN_XML,\n            touch_get_obs=touch_get_obs,\n            target_rotation=target_rotation,\n            target_position=target_position,\n            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),\n            randomize_initial_rotation=False, reward_type=reward_type,\n            ignore_z_target_rotation=True, distance_threshold=0.05)",
        "import os\nimport copy\nimport numpy as np\n\nimport gym\nfrom gym import error, spaces\nfrom gym.utils import seeding\nfrom gym.envs.robotics import robot_env\n\n\nclass HandEnv(robot_env.RobotEnv):\n    def __init__(self, model_path, n_substeps, initial_qpos, relative_control):\n        self.relative_control = relative_control\n\n        super(HandEnv, self).__init__(\n            model_path=model_path, n_substeps=n_substeps, n_actions=20,\n            initial_qpos=initial_qpos)\n\n    # RobotEnv methods\n    # ----------------------------\n\n    def _set_action(self, action):\n        assert action.shape == (20,)\n\n        ctrlrange = self.sim.model.actuator_ctrlrange\n        actuation_range = (ctrlrange[:, 1] - ctrlrange[:, 0]) / 2.\n        if self.relative_control:\n            actuation_center = np.zeros_like(action)\n            for i in range(self.sim.data.ctrl.shape[0]):\n                actuation_center[i] = self.sim.data.get_joint_qpos(\n                    self.sim.model.actuator_names[i].replace(':A_', ':'))\n            for joint_name in ['FF', 'MF', 'RF', 'LF']:\n                act_idx = self.sim.model.actuator_name2id(\n                    'robot0:A_{}J1'.format(joint_name))\n                actuation_center[act_idx] += self.sim.data.get_joint_qpos(\n                    'robot0:{}J0'.format(joint_name))\n        else:\n            actuation_center = (ctrlrange[:, 1] + ctrlrange[:, 0]) / 2.\n        self.sim.data.ctrl[:] = actuation_center + action * actuation_range\n        self.sim.data.ctrl[:] = np.clip(self.sim.data.ctrl, ctrlrange[:, 0], ctrlrange[:, 1])\n\n    def _viewer_setup(self):\n        body_id = self.sim.model.body_name2id('robot0:palm')\n        lookat = self.sim.data.body_xpos[body_id]\n        for idx, value in enumerate(lookat):\n            self.viewer.cam.lookat[idx] = value\n        self.viewer.cam.distance = 0.5\n        self.viewer.cam.azimuth = 55.\n        self.viewer.cam.elevation = -25.\n\n    def render(self, mode='human', width=500, height=500):\n        return super(HandEnv, self).render(mode, width, height)",
        "from gym.envs.robotics.fetch_env import FetchEnv\nfrom gym.envs.robotics.fetch.slide import FetchSlideEnv\nfrom gym.envs.robotics.fetch.pick_and_place import FetchPickAndPlaceEnv\nfrom gym.envs.robotics.fetch.push import FetchPushEnv\nfrom gym.envs.robotics.fetch.reach import FetchReachEnv\n\nfrom gym.envs.robotics.hand.reach import HandReachEnv\nfrom gym.envs.robotics.hand.manipulate import HandBlockEnv\nfrom gym.envs.robotics.hand.manipulate import HandEggEnv\nfrom gym.envs.robotics.hand.manipulate import HandPenEnv\n\nfrom gym.envs.robotics.hand.manipulate_touch_sensors import HandBlockTouchSensorsEnv\nfrom gym.envs.robotics.hand.manipulate_touch_sensors import HandEggTouchSensorsEnv\nfrom gym.envs.robotics.hand.manipulate_touch_sensors import HandPenTouchSensorsEnv",
        "# Copyright (c) 2009-2017, Matthew Brett and Christoph Gohlke\n#    All rights reserved.\n#\n#    Redistribution and use in source and binary forms, with or without\n#    modification, are permitted provided that the following conditions are\n#    met:\n#\n#    1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n#    2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n#    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS\n#    IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n#    THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n#    PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n#    CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n#    EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n#    PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n#    PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n#    LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n#    NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n#    SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n# Many methods borrow heavily or entirely from transforms3d:\n# https://github.com/matthew-brett/transforms3d\n# They have mostly been modified to support batched operations.\n\nimport numpy as np\nimport itertools\n\n'''\nRotations\n=========\n\nNote: these have caused many subtle bugs in the past.\nBe careful while updating these methods and while using them in clever ways.\n\nSee MuJoCo documentation here: http://mujoco.org/book/modeling.html#COrientation\n\nConventions\n-----------\n    - All functions accept batches as well as individual rotations\n    - All rotation conventions match respective MuJoCo defaults\n    - All angles are in radians\n    - Matricies follow LR convention\n    - Euler Angles are all relative with 'xyz' axes ordering\n    - See specific representation for more information\n\nRepresentations\n---------------\n\nEuler\n    There are many euler angle frames -- here we will strive to use the default\n        in MuJoCo, which is eulerseq='xyz'.\n    This frame is a relative rotating frame, about x, y, and z axes in order.\n        Relative rotating means that after we rotate about x, then we use the\n        new (rotated) y, and the same for z.\n\nQuaternions\n    These are defined in terms of rotation (angle) about a unit vector (x, y, z)\n    We use the following <q0, q1, q2, q3> convention:\n            q0 = cos(angle / 2)\n            q1 = sin(angle / 2) * x\n            q2 = sin(angle / 2) * y\n            q3 = sin(angle / 2) * z\n        This is also sometimes called qw, qx, qy, qz.\n    Note that quaternions are ambiguous, because we can represent a rotation by\n        angle about vector <x, y, z> and -angle about vector <-x, -y, -z>.\n        To choose between these, we pick \"first nonzero positive\", where we\n        make the first nonzero element of the quaternion positive.\n    This can result in mismatches if you're converting an quaternion that is not\n        \"first nonzero positive\" to a different representation and back.\n\nAxis Angle\n    (Not currently implemented)\n    These are very straightforward.  Rotation is angle about a unit vector.\n\nXY Axes\n    (Not currently implemented)\n    We are given x axis and y axis, and z axis is cross product of x and y.\n\nZ Axis\n    This is NOT RECOMMENDED.  Defines a unit vector for the Z axis,\n        but rotation about this axis is not well defined.\n    Instead pick a fixed reference direction for another axis (e.g. X)\n        and calculate the other (e.g. Y = Z cross-product X),\n        then use XY Axes rotation instead.\n\nSO3\n    (Not currently implemented)\n    While not supported by MuJoCo, this representation has a lot of nice features.\n    We expect to add support for these in the future.\n\nTODO / Missing\n--------------\n    - Rotation integration or derivatives (e.g. velocity conversions)\n    - More representations (SO3, etc)\n    - Random sampling (e.g. sample uniform random rotation)\n    - Performance benchmarks/measurements\n    - (Maybe) define everything as to/from matricies, for simplicity\n'''\n\n# For testing whether a number is close to zero\n_FLOAT_EPS = np.finfo(np.float64).eps\n_EPS4 = _FLOAT_EPS * 4.0\n\n\ndef euler2mat(euler):\n    \"\"\" Convert Euler Angles to Rotation Matrix.  See rotation.py for notes \"\"\"\n    euler = np.asarray(euler, dtype=np.float64)\n    assert euler.shape[-1] == 3, \"Invalid shaped euler {}\".format(euler)\n\n    ai, aj, ak = -euler[..., 2], -euler[..., 1], -euler[..., 0]\n    si, sj, sk = np.sin(ai), np.sin(aj), np.sin(ak)\n    ci, cj, ck = np.cos(ai), np.cos(aj), np.cos(ak)\n    cc, cs = ci * ck, ci * sk\n    sc, ss = si * ck, si * sk\n\n    mat = np.empty(euler.shape[:-1] + (3, 3), dtype=np.float64)\n    mat[..., 2, 2] = cj * ck\n    mat[..., 2, 1] = sj * sc - cs\n    mat[..., 2, 0] = sj * cc + ss\n    mat[..., 1, 2] = cj * sk\n    mat[..., 1, 1] = sj * ss + cc\n    mat[..., 1, 0] = sj * cs - sc\n    mat[..., 0, 2] = -sj\n    mat[..., 0, 1] = cj * si\n    mat[..., 0, 0] = cj * ci\n    return mat\n\n\ndef euler2quat(euler):\n    \"\"\" Convert Euler Angles to Quaternions.  See rotation.py for notes \"\"\"\n    euler = np.asarray(euler, dtype=np.float64)\n    assert euler.shape[-1] == 3, \"Invalid shape euler {}\".format(euler)\n\n    ai, aj, ak = euler[..., 2] / 2, -euler[..., 1] / 2, euler[..., 0] / 2\n    si, sj, sk = np.sin(ai), np.sin(aj), np.sin(ak)\n    ci, cj, ck = np.cos(ai), np.cos(aj), np.cos(ak)\n    cc, cs = ci * ck, ci * sk\n    sc, ss = si * ck, si * sk\n\n    quat = np.empty(euler.shape[:-1] + (4,), dtype=np.float64)\n    quat[..., 0] = cj * cc + sj * ss\n    quat[..., 3] = cj * sc - sj * cs\n    quat[..., 2] = -(cj * ss + sj * cc)\n    quat[..., 1] = cj * cs - sj * sc\n    return quat\n\n\ndef mat2euler(mat):\n    \"\"\" Convert Rotation Matrix to Euler Angles.  See rotation.py for notes \"\"\"\n    mat = np.asarray(mat, dtype=np.float64)\n    assert mat.shape[-2:] == (3, 3), \"Invalid shape matrix {}\".format(mat)\n\n    cy = np.sqrt(mat[..., 2, 2] * mat[..., 2, 2] + mat[..., 1, 2] * mat[..., 1, 2])\n    condition = cy > _EPS4\n    euler = np.empty(mat.shape[:-1], dtype=np.float64)\n    euler[..., 2] = np.where(condition,\n                             -np.arctan2(mat[..., 0, 1], mat[..., 0, 0]),\n                             -np.arctan2(-mat[..., 1, 0], mat[..., 1, 1]))\n    euler[..., 1] = np.where(condition,\n                             -np.arctan2(-mat[..., 0, 2], cy),\n                             -np.arctan2(-mat[..., 0, 2], cy))\n    euler[..., 0] = np.where(condition,\n                             -np.arctan2(mat[..., 1, 2], mat[..., 2, 2]),\n                             0.0)\n    return euler\n\n\ndef mat2quat(mat):\n    \"\"\" Convert Rotation Matrix to Quaternion.  See rotation.py for notes \"\"\"\n    mat = np.asarray(mat, dtype=np.float64)\n    assert mat.shape[-2:] == (3, 3), \"Invalid shape matrix {}\".format(mat)\n\n    Qxx, Qyx, Qzx = mat[..., 0, 0], mat[..., 0, 1], mat[..., 0, 2]\n    Qxy, Qyy, Qzy = mat[..., 1, 0], mat[..., 1, 1], mat[..., 1, 2]\n    Qxz, Qyz, Qzz = mat[..., 2, 0], mat[..., 2, 1], mat[..., 2, 2]\n    # Fill only lower half of symmetric matrix\n    K = np.zeros(mat.shape[:-2] + (4, 4), dtype=np.float64)\n    K[..., 0, 0] = Qxx - Qyy - Qzz\n    K[..., 1, 0] = Qyx + Qxy\n    K[..., 1, 1] = Qyy - Qxx - Qzz\n    K[..., 2, 0] = Qzx + Qxz\n    K[..., 2, 1] = Qzy + Qyz\n    K[..., 2, 2] = Qzz - Qxx - Qyy\n    K[..., 3, 0] = Qyz - Qzy\n    K[..., 3, 1] = Qzx - Qxz\n    K[..., 3, 2] = Qxy - Qyx\n    K[..., 3, 3] = Qxx + Qyy + Qzz\n    K /= 3.0\n    # TODO: vectorize this -- probably could be made faster\n    q = np.empty(K.shape[:-2] + (4,))\n    it = np.nditer(q[..., 0], flags=['multi_index'])\n    while not it.finished:\n        # Use Hermitian eigenvectors, values for speed\n        vals, vecs = np.linalg.eigh(K[it.multi_index])\n        # Select largest eigenvector, reorder to w,x,y,z quaternion\n        q[it.multi_index] = vecs[[3, 0, 1, 2], np.argmax(vals)]\n        # Prefer quaternion with positive w\n        # (q * -1 corresponds to same rotation as q)\n        if q[it.multi_index][0] < 0:\n            q[it.multi_index] *= -1\n        it.iternext()\n    return q\n\n\ndef quat2euler(quat):\n    \"\"\" Convert Quaternion to Euler Angles.  See rotation.py for notes \"\"\"\n    return mat2euler(quat2mat(quat))\n\n\ndef subtract_euler(e1, e2):\n    assert e1.shape == e2.shape\n    assert e1.shape[-1] == 3\n    q1 = euler2quat(e1)\n    q2 = euler2quat(e2)\n    q_diff = quat_mul(q1, quat_conjugate(q2))\n    return quat2euler(q_diff)\n\n\ndef quat2mat(quat):\n    \"\"\" Convert Quaternion to Euler Angles.  See rotation.py for notes \"\"\"\n    quat = np.asarray(quat, dtype=np.float64)\n    assert quat.shape[-1] == 4, \"Invalid shape quat {}\".format(quat)\n\n    w, x, y, z = quat[..., 0], quat[..., 1], quat[..., 2], quat[..., 3]\n    Nq = np.sum(quat * quat, axis=-1)\n    s = 2.0 / Nq\n    X, Y, Z = x * s, y * s, z * s\n    wX, wY, wZ = w * X, w * Y, w * Z\n    xX, xY, xZ = x * X, x * Y, x * Z\n    yY, yZ, zZ = y * Y, y * Z, z * Z\n\n    mat = np.empty(quat.shape[:-1] + (3, 3), dtype=np.float64)\n    mat[..., 0, 0] = 1.0 - (yY + zZ)\n    mat[..., 0, 1] = xY - wZ\n    mat[..., 0, 2] = xZ + wY\n    mat[..., 1, 0] = xY + wZ\n    mat[..., 1, 1] = 1.0 - (xX + zZ)\n    mat[..., 1, 2] = yZ - wX\n    mat[..., 2, 0] = xZ - wY\n    mat[..., 2, 1] = yZ + wX\n    mat[..., 2, 2] = 1.0 - (xX + yY)\n    return np.where((Nq > _FLOAT_EPS)[..., np.newaxis, np.newaxis], mat, np.eye(3))\n\ndef quat_conjugate(q):\n    inv_q = -q\n    inv_q[..., 0] *= -1\n    return inv_q\n\ndef quat_mul(q0, q1):\n    assert q0.shape == q1.shape\n    assert q0.shape[-1] == 4\n    assert q1.shape[-1] == 4\n\n    w0 = q0[..., 0]\n    x0 = q0[..., 1]\n    y0 = q0[..., 2]\n    z0 = q0[..., 3]\n\n    w1 = q1[..., 0]\n    x1 = q1[..., 1]\n    y1 = q1[..., 2]\n    z1 = q1[..., 3]\n\n    w = w0 * w1 - x0 * x1 - y0 * y1 - z0 * z1\n    x = w0 * x1 + x0 * w1 + y0 * z1 - z0 * y1\n    y = w0 * y1 + y0 * w1 + z0 * x1 - x0 * z1\n    z = w0 * z1 + z0 * w1 + x0 * y1 - y0 * x1\n    q = np.array([w, x, y, z])\n    if q.ndim == 2:\n        q = q.swapaxes(0, 1)\n    assert q.shape == q0.shape\n    return q\n\ndef quat_rot_vec(q, v0):\n    q_v0 = np.array([0, v0[0], v0[1], v0[2]])\n    q_v = quat_mul(q, quat_mul(q_v0, quat_conjugate(q)))\n    v = q_v[1:]\n    return v\n\ndef quat_identity():\n    return np.array([1, 0, 0, 0])\n\ndef quat2axisangle(quat):\n    theta = 0;\n    axis = np.array([0, 0, 1]);\n    sin_theta = np.linalg.norm(quat[1:])\n\n    if (sin_theta > 0.0001):\n        theta = 2 * np.arcsin(sin_theta)\n        theta *= 1 if quat[0] >= 0 else -1\n        axis = quat[1:] / sin_theta\n\n    return axis, theta\n\ndef euler2point_euler(euler):\n    _euler = euler.copy()\n    if len(_euler.shape) < 2:\n        _euler = np.expand_dims(_euler,0)\n    assert(_euler.shape[1] == 3)\n    _euler_sin = np.sin(_euler)\n    _euler_cos = np.cos(_euler)\n    return np.concatenate([_euler_sin, _euler_cos], axis=-1)\n\ndef point_euler2euler(euler):\n    _euler = euler.copy()\n    if len(_euler.shape) < 2:\n        _euler = np.expand_dims(_euler,0)\n    assert(_euler.shape[1] == 6)\n    angle = np.arctan(_euler[..., :3] / _euler[..., 3:])\n    angle[_euler[..., 3:] < 0] += np.pi\n    return angle\n\ndef quat2point_quat(quat):\n    # Should be in qw, qx, qy, qz\n    _quat = quat.copy()\n    if len(_quat.shape) < 2:\n        _quat = np.expand_dims(_quat, 0)\n    assert(_quat.shape[1] == 4)\n    angle = np.arccos(_quat[:,[0]]) * 2\n    xyz = _quat[:, 1:]\n    xyz[np.squeeze(np.abs(np.sin(angle/2))) >= 1e-5] = (xyz / np.sin(angle / 2))[np.squeeze(np.abs(np.sin(angle/2))) >= 1e-5]\n    return np.concatenate([np.sin(angle),np.cos(angle), xyz], axis=-1)\n\ndef point_quat2quat(quat):\n    _quat = quat.copy()\n    if len(_quat.shape) < 2:\n        _quat = np.expand_dims(_quat, 0)\n    assert(_quat.shape[1] == 5)\n    angle = np.arctan(_quat[:,[0]] / _quat[:,[1]])\n    qw = np.cos(angle / 2)\n\n    qxyz = _quat[:, 2:]\n    qxyz[np.squeeze(np.abs(np.sin(angle/2))) >= 1e-5] = (qxyz * np.sin(angle/2))[np.squeeze(np.abs(np.sin(angle/2))) >= 1e-5]\n    return np.concatenate([qw, qxyz], axis=-1)\n\ndef normalize_angles(angles):\n    '''Puts angles in [-pi, pi] range.'''\n    angles = angles.copy()\n    if angles.size > 0:\n        angles = (angles + np.pi) % (2 * np.pi) - np.pi\n        assert -np.pi-1e-6 <= angles.min() and angles.max() <= np.pi+1e-6\n    return angles\n\ndef round_to_straight_angles(angles):\n    '''Returns closest angle modulo 90 degrees '''\n    angles = np.round(angles / (np.pi / 2)) * (np.pi / 2)\n    return normalize_angles(angles)\n\ndef get_parallel_rotations():\n    mult90 = [0, np.pi/2, -np.pi/2, np.pi]\n    parallel_rotations = []\n    for euler in itertools.product(mult90, repeat=3):\n        canonical = mat2euler(euler2mat(euler))\n        canonical = np.round(canonical / (np.pi / 2))\n        if canonical[0] == -2:\n            canonical[0] = 2\n        if canonical[2] == -2:\n            canonical[2] = 2\n        canonical *= np.pi / 2\n        if all([(canonical != rot).any() for rot in parallel_rotations]):\n            parallel_rotations += [canonical]\n    assert len(parallel_rotations) == 24\n    return parallel_rotations",
        "import os\nfrom gym import utils\nfrom gym.envs.robotics import fetch_env\n\n\n# Ensure we get the path separator correct on windows\nMODEL_XML_PATH = os.path.join('fetch', 'reach.xml')\n\n\nclass FetchReachEnv(fetch_env.FetchEnv, utils.EzPickle):\n    def __init__(self, reward_type='sparse'):\n        initial_qpos = {\n            'robot0:slide0': 0.4049,\n            'robot0:slide1': 0.48,\n            'robot0:slide2': 0.0,\n        }\n        fetch_env.FetchEnv.__init__(\n            self, MODEL_XML_PATH, has_object=False, block_gripper=True, n_substeps=20,\n            gripper_extra_height=0.2, target_in_the_air=True, target_offset=0.0,\n            obj_range=0.15, target_range=0.15, distance_threshold=0.05,\n            initial_qpos=initial_qpos, reward_type=reward_type)\n        utils.EzPickle.__init__(self)",
        "import os\nimport numpy as np\n\nfrom gym import utils\nfrom gym.envs.robotics import fetch_env\n\n\n# Ensure we get the path separator correct on windows\nMODEL_XML_PATH = os.path.join('fetch', 'slide.xml')\n\n\nclass FetchSlideEnv(fetch_env.FetchEnv, utils.EzPickle):\n    def __init__(self, reward_type='sparse'):\n        initial_qpos = {\n            'robot0:slide0': 0.05,\n            'robot0:slide1': 0.48,\n            'robot0:slide2': 0.0,\n            'object0:joint': [1.7, 1.1, 0.41, 1., 0., 0., 0.],\n        }\n        fetch_env.FetchEnv.__init__(\n            self, MODEL_XML_PATH, has_object=True, block_gripper=True, n_substeps=20,\n            gripper_extra_height=-0.02, target_in_the_air=False, target_offset=np.array([0.4, 0.0, 0.0]),\n            obj_range=0.1, target_range=0.3, distance_threshold=0.05,\n            initial_qpos=initial_qpos, reward_type=reward_type)\n        utils.EzPickle.__init__(self)",
        "",
        "import os\nfrom gym import utils\nfrom gym.envs.robotics import fetch_env\n\n\n# Ensure we get the path separator correct on windows\nMODEL_XML_PATH = os.path.join('fetch', 'pick_and_place.xml')\n\n\nclass FetchPickAndPlaceEnv(fetch_env.FetchEnv, utils.EzPickle):\n    def __init__(self, reward_type='sparse'):\n        initial_qpos = {\n            'robot0:slide0': 0.405,\n            'robot0:slide1': 0.48,\n            'robot0:slide2': 0.0,\n            'object0:joint': [1.25, 0.53, 0.4, 1., 0., 0., 0.],\n        }\n        fetch_env.FetchEnv.__init__(\n            self, MODEL_XML_PATH, has_object=True, block_gripper=False, n_substeps=20,\n            gripper_extra_height=0.2, target_in_the_air=True, target_offset=0.0,\n            obj_range=0.15, target_range=0.15, distance_threshold=0.05,\n            initial_qpos=initial_qpos, reward_type=reward_type)\n        utils.EzPickle.__init__(self)",
        "import os\nfrom gym import utils\nfrom gym.envs.robotics import fetch_env\n\n\n# Ensure we get the path separator correct on windows\nMODEL_XML_PATH = os.path.join('fetch', 'push.xml')\n\n\nclass FetchPushEnv(fetch_env.FetchEnv, utils.EzPickle):\n    def __init__(self, reward_type='sparse'):\n        initial_qpos = {\n            'robot0:slide0': 0.405,\n            'robot0:slide1': 0.48,\n            'robot0:slide2': 0.0,\n            'object0:joint': [1.25, 0.53, 0.4, 1., 0., 0., 0.],\n        }\n        fetch_env.FetchEnv.__init__(\n            self, MODEL_XML_PATH, has_object=True, block_gripper=True, n_substeps=20,\n            gripper_extra_height=0.0, target_in_the_air=False, target_offset=0.0,\n            obj_range=0.15, target_range=0.15, distance_threshold=0.05,\n            initial_qpos=initial_qpos, reward_type=reward_type)\n        utils.EzPickle.__init__(self)",
        "import numpy as np\n\nfrom gym import error\ntry:\n    import mujoco_py\nexcept ImportError as e:\n    raise error.DependencyNotInstalled(\"{}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)\".format(e))\n\n\ndef robot_get_obs(sim):\n    \"\"\"Returns all joint positions and velocities associated with\n    a robot.\n    \"\"\"\n    if sim.data.qpos is not None and sim.model.joint_names:\n        names = [n for n in sim.model.joint_names if n.startswith('robot')]\n        return (\n            np.array([sim.data.get_joint_qpos(name) for name in names]),\n            np.array([sim.data.get_joint_qvel(name) for name in names]),\n        )\n    return np.zeros(0), np.zeros(0)\n\n\ndef ctrl_set_action(sim, action):\n    \"\"\"For torque actuators it copies the action into mujoco ctrl field.\n    For position actuators it sets the target relative to the current qpos.\n    \"\"\"\n    if sim.model.nmocap > 0:\n        _, action = np.split(action, (sim.model.nmocap * 7, ))\n    if sim.data.ctrl is not None:\n        for i in range(action.shape[0]):\n            if sim.model.actuator_biastype[i] == 0:\n                sim.data.ctrl[i] = action[i]\n            else:\n                idx = sim.model.jnt_qposadr[sim.model.actuator_trnid[i, 0]]\n                sim.data.ctrl[i] = sim.data.qpos[idx] + action[i]\n\n\ndef mocap_set_action(sim, action):\n    \"\"\"The action controls the robot using mocaps. Specifically, bodies\n    on the robot (for example the gripper wrist) is controlled with\n    mocap bodies. In this case the action is the desired difference\n    in position and orientation (quaternion), in world coordinates,\n    of the of the target body. The mocap is positioned relative to\n    the target body according to the delta, and the MuJoCo equality\n    constraint optimizer tries to center the welded body on the mocap.\n    \"\"\"\n    if sim.model.nmocap > 0:\n        action, _ = np.split(action, (sim.model.nmocap * 7, ))\n        action = action.reshape(sim.model.nmocap, 7)\n\n        pos_delta = action[:, :3]\n        quat_delta = action[:, 3:]\n\n        reset_mocap2body_xpos(sim)\n        sim.data.mocap_pos[:] = sim.data.mocap_pos + pos_delta\n        sim.data.mocap_quat[:] = sim.data.mocap_quat + quat_delta\n\n\ndef reset_mocap_welds(sim):\n    \"\"\"Resets the mocap welds that we use for actuation.\n    \"\"\"\n    if sim.model.nmocap > 0 and sim.model.eq_data is not None:\n        for i in range(sim.model.eq_data.shape[0]):\n            if sim.model.eq_type[i] == mujoco_py.const.EQ_WELD:\n                sim.model.eq_data[i, :] = np.array(\n                    [0., 0., 0., 1., 0., 0., 0.])\n    sim.forward()\n\n\ndef reset_mocap2body_xpos(sim):\n    \"\"\"Resets the position and orientation of the mocap bodies to the same\n    values as the bodies they're welded to.\n    \"\"\"\n\n    if (sim.model.eq_type is None or\n        sim.model.eq_obj1id is None or\n        sim.model.eq_obj2id is None):\n        return\n    for eq_type, obj1_id, obj2_id in zip(sim.model.eq_type,\n                                         sim.model.eq_obj1id,\n                                         sim.model.eq_obj2id):\n        if eq_type != mujoco_py.const.EQ_WELD:\n            continue\n\n        mocap_id = sim.model.body_mocapid[obj1_id]\n        if mocap_id != -1:\n            # obj1 is the mocap, obj2 is the welded body\n            body_idx = obj2_id\n        else:\n            # obj2 is the mocap, obj1 is the welded body\n            mocap_id = sim.model.body_mocapid[obj2_id]\n            body_idx = obj1_id\n\n        assert (mocap_id != -1)\n        sim.data.mocap_pos[mocap_id][:] = sim.data.body_xpos[body_idx]\n        sim.data.mocap_quat[mocap_id][:] = sim.data.body_xquat[body_idx]",
        "import numpy as np\n\nfrom gym.envs.robotics import rotations, robot_env, utils\n\n\ndef goal_distance(goal_a, goal_b):\n    assert goal_a.shape == goal_b.shape\n    return np.linalg.norm(goal_a - goal_b, axis=-1)\n\n\nclass FetchEnv(robot_env.RobotEnv):\n    \"\"\"Superclass for all Fetch environments.\n    \"\"\"\n\n    def __init__(\n        self, model_path, n_substeps, gripper_extra_height, block_gripper,\n        has_object, target_in_the_air, target_offset, obj_range, target_range,\n        distance_threshold, initial_qpos, reward_type,\n    ):\n        \"\"\"Initializes a new Fetch environment.\n\n        Args:\n            model_path (string): path to the environments XML file\n            n_substeps (int): number of substeps the simulation runs on every call to step\n            gripper_extra_height (float): additional height above the table when positioning the gripper\n            block_gripper (boolean): whether or not the gripper is blocked (i.e. not movable) or not\n            has_object (boolean): whether or not the environment has an object\n            target_in_the_air (boolean): whether or not the target should be in the air above the table or on the table surface\n            target_offset (float or array with 3 elements): offset of the target\n            obj_range (float): range of a uniform distribution for sampling initial object positions\n            target_range (float): range of a uniform distribution for sampling a target\n            distance_threshold (float): the threshold after which a goal is considered achieved\n            initial_qpos (dict): a dictionary of joint names and values that define the initial configuration\n            reward_type ('sparse' or 'dense'): the reward type, i.e. sparse or dense\n        \"\"\"\n        self.gripper_extra_height = gripper_extra_height\n        self.block_gripper = block_gripper\n        self.has_object = has_object\n        self.target_in_the_air = target_in_the_air\n        self.target_offset = target_offset\n        self.obj_range = obj_range\n        self.target_range = target_range\n        self.distance_threshold = distance_threshold\n        self.reward_type = reward_type\n\n        super(FetchEnv, self).__init__(\n            model_path=model_path, n_substeps=n_substeps, n_actions=4,\n            initial_qpos=initial_qpos)\n\n    # GoalEnv methods\n    # ----------------------------\n\n    def compute_reward(self, achieved_goal, goal, info):\n        # Compute distance between goal and the achieved goal.\n        d = goal_distance(achieved_goal, goal)\n        if self.reward_type == 'sparse':\n            return -(d > self.distance_threshold).astype(np.float32)\n        else:\n            return -d\n\n    # RobotEnv methods\n    # ----------------------------\n\n    def _step_callback(self):\n        if self.block_gripper:\n            self.sim.data.set_joint_qpos('robot0:l_gripper_finger_joint', 0.)\n            self.sim.data.set_joint_qpos('robot0:r_gripper_finger_joint', 0.)\n            self.sim.forward()\n\n    def _set_action(self, action):\n        assert action.shape == (4,)\n        action = action.copy()  # ensure that we don't change the action outside of this scope\n        pos_ctrl, gripper_ctrl = action[:3], action[3]\n\n        pos_ctrl *= 0.05  # limit maximum change in position\n        rot_ctrl = [1., 0., 1., 0.]  # fixed rotation of the end effector, expressed as a quaternion\n        gripper_ctrl = np.array([gripper_ctrl, gripper_ctrl])\n        assert gripper_ctrl.shape == (2,)\n        if self.block_gripper:\n            gripper_ctrl = np.zeros_like(gripper_ctrl)\n        action = np.concatenate([pos_ctrl, rot_ctrl, gripper_ctrl])\n\n        # Apply action to simulation.\n        utils.ctrl_set_action(self.sim, action)\n        utils.mocap_set_action(self.sim, action)\n\n    def _get_obs(self):\n        # positions\n        grip_pos = self.sim.data.get_site_xpos('robot0:grip')\n        dt = self.sim.nsubsteps * self.sim.model.opt.timestep\n        grip_velp = self.sim.data.get_site_xvelp('robot0:grip') * dt\n        robot_qpos, robot_qvel = utils.robot_get_obs(self.sim)\n        if self.has_object:\n            object_pos = self.sim.data.get_site_xpos('object0')\n            # rotations\n            object_rot = rotations.mat2euler(self.sim.data.get_site_xmat('object0'))\n            # velocities\n            object_velp = self.sim.data.get_site_xvelp('object0') * dt\n            object_velr = self.sim.data.get_site_xvelr('object0') * dt\n            # gripper state\n            object_rel_pos = object_pos - grip_pos\n            object_velp -= grip_velp\n        else:\n            object_pos = object_rot = object_velp = object_velr = object_rel_pos = np.zeros(0)\n        gripper_state = robot_qpos[-2:]\n        gripper_vel = robot_qvel[-2:] * dt  # change to a scalar if the gripper is made symmetric\n\n        if not self.has_object:\n            achieved_goal = grip_pos.copy()\n        else:\n            achieved_goal = np.squeeze(object_pos.copy())\n        obs = np.concatenate([\n            grip_pos, object_pos.ravel(), object_rel_pos.ravel(), gripper_state, object_rot.ravel(),\n            object_velp.ravel(), object_velr.ravel(), grip_velp, gripper_vel,\n        ])\n\n        return {\n            'observation': obs.copy(),\n            'achieved_goal': achieved_goal.copy(),\n            'desired_goal': self.goal.copy(),\n        }\n\n    def _viewer_setup(self):\n        body_id = self.sim.model.body_name2id('robot0:gripper_link')\n        lookat = self.sim.data.body_xpos[body_id]\n        for idx, value in enumerate(lookat):\n            self.viewer.cam.lookat[idx] = value\n        self.viewer.cam.distance = 2.5\n        self.viewer.cam.azimuth = 132.\n        self.viewer.cam.elevation = -14.\n\n    def _render_callback(self):\n        # Visualize target.\n        sites_offset = (self.sim.data.site_xpos - self.sim.model.site_pos).copy()\n        site_id = self.sim.model.site_name2id('target0')\n        self.sim.model.site_pos[site_id] = self.goal - sites_offset[0]\n        self.sim.forward()\n\n    def _reset_sim(self):\n        self.sim.set_state(self.initial_state)\n\n        # Randomize start position of object.\n        if self.has_object:\n            object_xpos = self.initial_gripper_xpos[:2]\n            while np.linalg.norm(object_xpos - self.initial_gripper_xpos[:2]) < 0.1:\n                object_xpos = self.initial_gripper_xpos[:2] + self.np_random.uniform(-self.obj_range, self.obj_range, size=2)\n            object_qpos = self.sim.data.get_joint_qpos('object0:joint')\n            assert object_qpos.shape == (7,)\n            object_qpos[:2] = object_xpos\n            self.sim.data.set_joint_qpos('object0:joint', object_qpos)\n\n        self.sim.forward()\n        return True\n\n    def _sample_goal(self):\n        if self.has_object:\n            goal = self.initial_gripper_xpos[:3] + self.np_random.uniform(-self.target_range, self.target_range, size=3)\n            goal += self.target_offset\n            goal[2] = self.height_offset\n            if self.target_in_the_air and self.np_random.uniform() < 0.5:\n                goal[2] += self.np_random.uniform(0, 0.45)\n        else:\n            goal = self.initial_gripper_xpos[:3] + self.np_random.uniform(-self.target_range, self.target_range, size=3)\n        return goal.copy()\n\n    def _is_success(self, achieved_goal, desired_goal):\n        d = goal_distance(achieved_goal, desired_goal)\n        return (d < self.distance_threshold).astype(np.float32)\n\n    def _env_setup(self, initial_qpos):\n        for name, value in initial_qpos.items():\n            self.sim.data.set_joint_qpos(name, value)\n        utils.reset_mocap_welds(self.sim)\n        self.sim.forward()\n\n        # Move end effector into position.\n        gripper_target = np.array([-0.498, 0.005, -0.431 + self.gripper_extra_height]) + self.sim.data.get_site_xpos('robot0:grip')\n        gripper_rotation = np.array([1., 0., 1., 0.])\n        self.sim.data.set_mocap_pos('robot0:mocap', gripper_target)\n        self.sim.data.set_mocap_quat('robot0:mocap', gripper_rotation)\n        for _ in range(10):\n            self.sim.step()\n\n        # Extract information for sampling goals.\n        self.initial_gripper_xpos = self.sim.data.get_site_xpos('robot0:grip').copy()\n        if self.has_object:\n            self.height_offset = self.sim.data.get_site_xpos('object0')[2]\n\n    def render(self, mode='human', width=500, height=500):\n        return super(FetchEnv, self).render(mode, width, height)",
        "import os\nimport copy\nimport numpy as np\n\nimport gym\nfrom gym import error, spaces\nfrom gym.utils import seeding\n\ntry:\n    import mujoco_py\nexcept ImportError as e:\n    raise error.DependencyNotInstalled(\"{}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)\".format(e))\n\nDEFAULT_SIZE = 500\n\nclass RobotEnv(gym.GoalEnv):\n    def __init__(self, model_path, initial_qpos, n_actions, n_substeps):\n        if model_path.startswith('/'):\n            fullpath = model_path\n        else:\n            fullpath = os.path.join(os.path.dirname(__file__), 'assets', model_path)\n        if not os.path.exists(fullpath):\n            raise IOError('File {} does not exist'.format(fullpath))\n\n        model = mujoco_py.load_model_from_path(fullpath)\n        self.sim = mujoco_py.MjSim(model, nsubsteps=n_substeps)\n        self.viewer = None\n        self._viewers = {}\n\n        self.metadata = {\n            'render.modes': ['human', 'rgb_array'],\n            'video.frames_per_second': int(np.round(1.0 / self.dt))\n        }\n\n        self.seed()\n        self._env_setup(initial_qpos=initial_qpos)\n        self.initial_state = copy.deepcopy(self.sim.get_state())\n\n        self.goal = self._sample_goal()\n        obs = self._get_obs()\n        self.action_space = spaces.Box(-1., 1., shape=(n_actions,), dtype='float32')\n        self.observation_space = spaces.Dict(dict(\n            desired_goal=spaces.Box(-np.inf, np.inf, shape=obs['achieved_goal'].shape, dtype='float32'),\n            achieved_goal=spaces.Box(-np.inf, np.inf, shape=obs['achieved_goal'].shape, dtype='float32'),\n            observation=spaces.Box(-np.inf, np.inf, shape=obs['observation'].shape, dtype='float32'),\n        ))\n\n    @property\n    def dt(self):\n        return self.sim.model.opt.timestep * self.sim.nsubsteps\n\n    # Env methods\n    # ----------------------------\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def step(self, action):\n        action = np.clip(action, self.action_space.low, self.action_space.high)\n        self._set_action(action)\n        self.sim.step()\n        self._step_callback()\n        obs = self._get_obs()\n\n        done = False\n        info = {\n            'is_success': self._is_success(obs['achieved_goal'], self.goal),\n        }\n        reward = self.compute_reward(obs['achieved_goal'], self.goal, info)\n        return obs, reward, done, info\n\n    def reset(self):\n        # Attempt to reset the simulator. Since we randomize initial conditions, it\n        # is possible to get into a state with numerical issues (e.g. due to penetration or\n        # Gimbel lock) or we may not achieve an initial condition (e.g. an object is within the hand).\n        # In this case, we just keep randomizing until we eventually achieve a valid initial\n        # configuration.\n        super(RobotEnv, self).reset()\n        did_reset_sim = False\n        while not did_reset_sim:\n            did_reset_sim = self._reset_sim()\n        self.goal = self._sample_goal().copy()\n        obs = self._get_obs()\n        return obs\n\n    def close(self):\n        if self.viewer is not None:\n            # self.viewer.finish()\n            self.viewer = None\n            self._viewers = {}\n\n    def render(self, mode='human', width=DEFAULT_SIZE, height=DEFAULT_SIZE):\n        self._render_callback()\n        if mode == 'rgb_array':\n            self._get_viewer(mode).render(width, height)\n            # window size used for old mujoco-py:\n            data = self._get_viewer(mode).read_pixels(width, height, depth=False)\n            # original image is upside-down, so flip it\n            return data[::-1, :, :]\n        elif mode == 'human':\n            self._get_viewer(mode).render()\n\n    def _get_viewer(self, mode):\n        self.viewer = self._viewers.get(mode)\n        if self.viewer is None:\n            if mode == 'human':\n                self.viewer = mujoco_py.MjViewer(self.sim)\n            elif mode == 'rgb_array':\n                self.viewer = mujoco_py.MjRenderContextOffscreen(self.sim, device_id=-1)\n            self._viewer_setup()\n            self._viewers[mode] = self.viewer\n        return self.viewer\n\n    # Extension methods\n    # ----------------------------\n\n    def _reset_sim(self):\n        \"\"\"Resets a simulation and indicates whether or not it was successful.\n        If a reset was unsuccessful (e.g. if a randomized state caused an error in the\n        simulation), this method should indicate such a failure by returning False.\n        In such a case, this method will be called again to attempt a the reset again.\n        \"\"\"\n        self.sim.set_state(self.initial_state)\n        self.sim.forward()\n        return True\n\n    def _get_obs(self):\n        \"\"\"Returns the observation.\n        \"\"\"\n        raise NotImplementedError()\n\n    def _set_action(self, action):\n        \"\"\"Applies the given action to the simulation.\n        \"\"\"\n        raise NotImplementedError()\n\n    def _is_success(self, achieved_goal, desired_goal):\n        \"\"\"Indicates whether or not the achieved goal successfully achieved the desired goal.\n        \"\"\"\n        raise NotImplementedError()\n\n    def _sample_goal(self):\n        \"\"\"Samples a new goal and returns it.\n        \"\"\"\n        raise NotImplementedError()\n\n    def _env_setup(self, initial_qpos):\n        \"\"\"Initial configuration of the environment. Can be used to configure initial state\n        and extract information from the simulation.\n        \"\"\"\n        pass\n\n    def _viewer_setup(self):\n        \"\"\"Initial configuration of the viewer. Can be used to set the camera position,\n        for example.\n        \"\"\"\n        pass\n\n    def _render_callback(self):\n        \"\"\"A custom callback that is called before rendering. Can be used\n        to implement custom visualizations.\n        \"\"\"\n        pass\n\n    def _step_callback(self):\n        \"\"\"A custom callback that is called after stepping the simulation. Can be used\n        to enforce additional constraints on the simulation state.\n        \"\"\"\n        pass",
        "import numpy as np\nimport pytest\n\nfrom gym.envs.tests.spec_list import spec_list\n\n@pytest.mark.parametrize(\"spec\", spec_list)\ndef test_env(spec):\n    # Note that this precludes running this test in multiple\n    # threads. However, we probably already can't do multithreading\n    # due to some environments.\n    env1 = spec.make()\n    env1.seed(0)\n    initial_observation1 = env1.reset()\n    env1.action_space.seed(0)\n    action_samples1 = [env1.action_space.sample() for i in range(4)]\n    step_responses1 = [env1.step(action) for action in action_samples1]\n    env1.close()\n\n    env2 = spec.make()\n    env2.seed(0)\n    initial_observation2 = env2.reset()\n    env2.action_space.seed(0)\n    action_samples2 = [env2.action_space.sample() for i in range(4)]\n    step_responses2 = [env2.step(action) for action in action_samples2]\n    env2.close()\n\n    for i, (action_sample1, action_sample2) in enumerate(zip(action_samples1, action_samples2)):\n        try:\n            assert_equals(action_sample1, action_sample2)\n        except AssertionError:\n            print('env1.action_space=', env1.action_space)\n            print('env2.action_space=', env2.action_space)\n            print('action_samples1=', action_samples1)\n            print('action_samples2=', action_samples2)\n            print('[{}] action_sample1: {}, action_sample2: {}'.format(i, action_sample1, action_sample2))\n            raise\n\n    # Don't check rollout equality if it's a a nondeterministic\n    # environment.\n    if spec.nondeterministic:\n        return\n\n    assert_equals(initial_observation1, initial_observation2)\n\n    for i, ((o1, r1, d1, i1), (o2, r2, d2, i2)) in enumerate(zip(step_responses1, step_responses2)):\n        assert_equals(o1, o2, '[{}] '.format(i))\n        assert r1 == r2, '[{}] r1: {}, r2: {}'.format(i, r1, r2)\n        assert d1 == d2, '[{}] d1: {}, d2: {}'.format(i, d1, d2)\n\n        # Go returns a Pachi game board in info, which doesn't\n        # properly check equality. For now, we hack around this by\n        # just skipping Go.\n        if spec.id not in ['Go9x9-v0', 'Go19x19-v0']:\n            assert_equals(i1, i2, '[{}] '.format(i))\n\ndef assert_equals(a, b, prefix=None):\n    assert type(a) == type(b), \"{}Differing types: {} and {}\".format(prefix, a, b)\n    if isinstance(a, dict):\n        assert list(a.keys()) == list(b.keys()), \"{}Key sets differ: {} and {}\".format(prefix, a, b)\n\n        for k in a.keys():\n            v_a = a[k]\n            v_b = b[k]\n            assert_equals(v_a, v_b)\n    elif isinstance(a, np.ndarray):\n        np.testing.assert_array_equal(a, b)\n    elif isinstance(a, tuple):\n        for elem_from_a, elem_from_b in zip(a, b):\n            assert_equals(elem_from_a, elem_from_b)\n    else:\n        assert a == b",
        "\"\"\"\nCurrently disabled since this was done in a very poor way\nHashed str representation of objects\n\"\"\"\n\n\nimport json\nimport hashlib\nimport os\n\nimport pytest\nfrom gym import spaces, logger\nfrom gym.envs.tests.spec_list import spec_list\n\nDATA_DIR = os.path.dirname(__file__)\nROLLOUT_STEPS = 100\nepisodes = ROLLOUT_STEPS\nsteps = ROLLOUT_STEPS\n\nROLLOUT_FILE = os.path.join(DATA_DIR, 'rollout.json')\n\nif not os.path.isfile(ROLLOUT_FILE):\n    with open(ROLLOUT_FILE, \"w\") as outfile:\n        json.dump({}, outfile, indent=2)\n\ndef hash_object(unhashed):\n    return hashlib.sha256(str(unhashed).encode('utf-16')).hexdigest() # This is really bad, str could be same while values change\n\ndef generate_rollout_hash(spec):\n    spaces.seed(0)\n    env = spec.make()\n    env.seed(0)\n\n    observation_list = []\n    action_list = []\n    reward_list = []\n    done_list = []\n\n    total_steps = 0\n    for episode in range(episodes):\n        if total_steps >= ROLLOUT_STEPS: break\n        observation = env.reset()\n\n        for step in range(steps):\n            action = env.action_space.sample()\n            observation, reward, done, _ = env.step(action)\n\n            action_list.append(action)\n            observation_list.append(observation)\n            reward_list.append(reward)\n            done_list.append(done)\n\n            total_steps += 1\n            if total_steps >= ROLLOUT_STEPS: break\n\n            if done: break\n\n    observations_hash = hash_object(observation_list)\n    actions_hash = hash_object(action_list)\n    rewards_hash = hash_object(reward_list)\n    dones_hash = hash_object(done_list)\n\n    env.close()\n    return observations_hash, actions_hash, rewards_hash, dones_hash\n\n@pytest.mark.parametrize(\"spec\", spec_list)\ndef test_env_semantics(spec):\n    logger.warn(\"Skipping this test. Existing hashes were generated in a bad way\")\n    return\n    with open(ROLLOUT_FILE) as data_file:\n        rollout_dict = json.load(data_file)\n\n    if spec.id not in rollout_dict:\n        if not spec.nondeterministic:\n            logger.warn(\"Rollout does not exist for {}, run generate_json.py to generate rollouts for new envs\".format(spec.id))\n        return\n\n    logger.info(\"Testing rollout for {} environment...\".format(spec.id))\n\n    observations_now, actions_now, rewards_now, dones_now = generate_rollout_hash(spec)\n\n    errors = []\n    if rollout_dict[spec.id]['observations'] != observations_now:\n        errors.append('Observations not equal for {} -- expected {} but got {}'.format(spec.id, rollout_dict[spec.id]['observations'], observations_now))\n    if rollout_dict[spec.id]['actions'] != actions_now:\n        errors.append('Actions not equal for {} -- expected {} but got {}'.format(spec.id, rollout_dict[spec.id]['actions'], actions_now))\n    if rollout_dict[spec.id]['rewards'] != rewards_now:\n        errors.append('Rewards not equal for {} -- expected {} but got {}'.format(spec.id, rollout_dict[spec.id]['rewards'], rewards_now))\n    if rollout_dict[spec.id]['dones'] != dones_now:\n        errors.append('Dones not equal for {} -- expected {} but got {}'.format(spec.id, rollout_dict[spec.id]['dones'], dones_now))\n    if len(errors):\n        for error in errors:\n            logger.warn(error)\n        raise ValueError(errors)",
        "import pytest\nimport numpy as np\n\nfrom gym import envs\nfrom gym.envs.tests.spec_list import spec_list\n\n# This runs a smoketest on each official registered env. We may want\n# to try also running environments which are not officially registered\n# envs.\n@pytest.mark.parametrize(\"spec\", spec_list)\ndef test_env(spec):\n    # Capture warnings\n    with pytest.warns(None) as warnings:\n        env = spec.make()\n\n    # Check that dtype is explicitly declared for gym.Box spaces\n    for warning_msg in warnings:\n        assert not 'autodetected dtype' in str(warning_msg.message)\n\n    ob_space = env.observation_space\n    act_space = env.action_space\n    ob = env.reset()\n    assert ob_space.contains(ob), 'Reset observation: {!r} not in space'.format(ob)\n    a = act_space.sample()\n    observation, reward, done, _info = env.step(a)\n    assert ob_space.contains(observation), 'Step observation: {!r} not in space'.format(observation)\n    assert np.isscalar(reward), \"{} is not a scalar for {}\".format(reward, env)\n    assert isinstance(done, bool), \"Expected {} to be a boolean\".format(done)\n\n    for mode in env.metadata.get('render.modes', []):\n        env.render(mode=mode)\n\n    # Make sure we can render the environment after close.\n    for mode in env.metadata.get('render.modes', []):\n        env.render(mode=mode)\n\n    env.close()\n\n# Run a longer rollout on some environments\ndef test_random_rollout():\n    for env in [envs.make('CartPole-v0'), envs.make('FrozenLake-v0')]:\n        agent = lambda ob: env.action_space.sample()\n        ob = env.reset()\n        for _ in range(10):\n            assert env.observation_space.contains(ob)\n            a = agent(ob)\n            assert env.action_space.contains(a)\n            (ob, _reward, done, _info) = env.step(a)\n            if done: break\n        env.close()\n\n\ndef test_env_render_result_is_immutable():\n    environs = [\n        envs.make('Taxi-v3'),\n        envs.make('FrozenLake-v0'),\n        envs.make('Reverse-v0'),\n    ]\n\n    for env in environs:\n        env.reset()\n        output = env.render(mode='ansi')\n        assert isinstance(output, str)\n        env.close()",
        "import unittest\nimport numpy as np\nfrom gym import envs\nfrom gym.envs.tests.spec_list import skip_mujoco, SKIP_MUJOCO_WARNING_MESSAGE\n\n\ndef verify_environments_match(old_environment_id,\n                              new_environment_id,\n                              seed=1,\n                              num_actions=1000):\n    old_environment = envs.make(old_environment_id)\n    new_environment = envs.make(new_environment_id)\n\n    old_environment.seed(seed)\n    new_environment.seed(seed)\n\n    old_reset_observation = old_environment.reset()\n    new_reset_observation = new_environment.reset()\n\n    np.testing.assert_allclose(old_reset_observation, new_reset_observation)\n\n    for i in range(num_actions):\n        action = old_environment.action_space.sample()\n        old_observation, old_reward, old_done, old_info = old_environment.step(\n            action)\n        new_observation, new_reward, new_done, new_info = new_environment.step(\n            action)\n\n        eps = 1e-6\n        np.testing.assert_allclose(old_observation, new_observation, atol=eps)\n        np.testing.assert_allclose(old_reward, new_reward, atol=eps)\n        np.testing.assert_allclose(old_done, new_done, atol=eps)\n\n        for key in old_info:\n            np.testing.assert_allclose(old_info[key], new_info[key], atol=eps)\n\n\n@unittest.skipIf(skip_mujoco, SKIP_MUJOCO_WARNING_MESSAGE)\nclass Mujocov2Tov3ConversionTest(unittest.TestCase):\n    def test_environments_match(self):\n        test_cases = (\n            {\n                'old_id': 'Swimmer-v2',\n                'new_id': 'Swimmer-v3'\n             },\n            {\n                'old_id': 'Hopper-v2',\n                'new_id': 'Hopper-v3'\n             },\n            {\n                'old_id': 'Walker2d-v2',\n                'new_id': 'Walker2d-v3'\n             },\n            {\n                'old_id': 'HalfCheetah-v2',\n                'new_id': 'HalfCheetah-v3'\n             },\n            {\n                'old_id': 'Ant-v2',\n                'new_id': 'Ant-v3'\n             },\n            {\n                'old_id': 'Humanoid-v2',\n                'new_id': 'Humanoid-v3'\n             },\n        )\n\n        for test_case in test_cases:\n            verify_environments_match(test_case['old_id'], test_case['new_id'])\n\n        # Raises KeyError because the new envs have extra info\n        with self.assertRaises(KeyError):\n            verify_environments_match('Swimmer-v3', 'Swimmer-v2')\n\n        # Raises KeyError because the new envs have extra info\n        with self.assertRaises(KeyError):\n            verify_environments_match('Humanoid-v3', 'Humanoid-v2')\n\n        # Raises KeyError because the new envs have extra info\n        with self.assertRaises(KeyError):\n            verify_environments_match('Swimmer-v3', 'Swimmer-v2')\n\n\nif __name__ == '__main__':\n    unittest.main()",
        "",
        "from gym.envs.toy_text.kellycoinflip import KellyCoinflipEnv\n\n\nclass TestKellyCoinflipEnv:\n    @staticmethod\n    def test_done_when_reaches_max_wealth():\n        # https://github.com/openai/gym/issues/1266\n        env = KellyCoinflipEnv()\n        env.seed(1)\n        env.reset()\n        done = False\n\n        while not done:\n            action = int(env.wealth * 20)  # bet 20% of the wealth\n            observation, reward, done, info = env.step(action)\n\n        assert env.wealth == env.max_wealth",
        "from gym import envs, logger\nimport os\n\n\nSKIP_MUJOCO_WARNING_MESSAGE = (\n    \"Cannot run mujoco test (either license key not found or mujoco not\"\n    \"installed properly).\")\n\n\nskip_mujoco = not (os.environ.get('MUJOCO_KEY'))\nif not skip_mujoco:\n    try:\n        import mujoco_py\n    except ImportError:\n        skip_mujoco = True\n\ndef should_skip_env_spec_for_tests(spec):\n    # We skip tests for envs that require dependencies or are otherwise\n    # troublesome to run frequently\n    ep = spec.entry_point\n    # Skip mujoco tests for pull request CI\n    if skip_mujoco and (ep.startswith('gym.envs.mujoco') or ep.startswith('gym.envs.robotics:')):\n        return True\n    try:\n        import atari_py\n    except ImportError:\n        if ep.startswith('gym.envs.atari'):\n            return True\n    try:\n        import Box2D\n    except ImportError:\n        if ep.startswith('gym.envs.box2d'):\n            return True\n\n    if (    'GoEnv' in ep or\n            'HexEnv' in ep or\n            (ep.startswith(\"gym.envs.atari\") and not spec.id.startswith(\"Pong\") and not spec.id.startswith(\"Seaquest\"))\n    ):\n        logger.warn(\"Skipping tests for env {}\".format(ep))\n        return True\n    return False\n\nspec_list = [spec for spec in sorted(envs.registry.all(), key=lambda x: x.id) if spec.entry_point is not None and not should_skip_env_spec_for_tests(spec)]",
        "# -*- coding: utf-8 -*-\nimport gym\nfrom gym import error, envs\nfrom gym.envs import registration\nfrom gym.envs.classic_control import cartpole\n\nclass ArgumentEnv(gym.Env):\n    def __init__(self, arg1, arg2, arg3):\n        self.arg1 = arg1\n        self.arg2 = arg2\n        self.arg3 = arg3\n\ngym.register(\n    id='test.ArgumentEnv-v0',\n    entry_point='gym.envs.tests.test_registration:ArgumentEnv',\n    kwargs={\n        'arg1': 'arg1',\n        'arg2': 'arg2',\n    }\n)\n\ndef test_make():\n    env = envs.make('CartPole-v0')\n    assert env.spec.id == 'CartPole-v0'\n    assert isinstance(env.unwrapped, cartpole.CartPoleEnv)\n\ndef test_make_with_kwargs():\n    env = envs.make('test.ArgumentEnv-v0', arg2='override_arg2', arg3='override_arg3')\n    assert env.spec.id == 'test.ArgumentEnv-v0'\n    assert isinstance(env.unwrapped, ArgumentEnv)\n    assert env.arg1 == 'arg1'\n    assert env.arg2 == 'override_arg2'\n    assert env.arg3 == 'override_arg3'\n\ndef test_make_deprecated():\n    try:\n        envs.make('Humanoid-v0')\n    except error.Error:\n        pass\n    else:\n        assert False\n\ndef test_spec():\n    spec = envs.spec('CartPole-v0')\n    assert spec.id == 'CartPole-v0'\n\ndef test_spec_with_kwargs():\n    map_name_value = '8x8'\n    env = gym.make('FrozenLake-v0', map_name=map_name_value)\n    assert env.spec._kwargs['map_name'] == map_name_value\n\ndef test_missing_lookup():\n    registry = registration.EnvRegistry()\n    registry.register(id='Test-v0', entry_point=None)\n    registry.register(id='Test-v15', entry_point=None)\n    registry.register(id='Test-v9', entry_point=None)\n    registry.register(id='Other-v100', entry_point=None)\n    try:\n        registry.spec('Test-v1')  # must match an env name but not the version above\n    except error.DeprecatedEnv:\n        pass\n    else:\n        assert False\n\n    try:\n        registry.spec('Unknown-v1')\n    except error.UnregisteredEnv:\n        pass\n    else:\n        assert False\n\ndef test_malformed_lookup():\n    registry = registration.EnvRegistry()\n    try:\n        registry.spec(u'âBreakout-v0â')\n    except error.Error as e:\n        assert 'malformed environment ID' in '{}'.format(e), 'Unexpected message: {}'.format(e)\n    else:\n        assert False",
        "import pytest\nimport numpy as np\n\nfrom gym.envs.toy_text.frozen_lake import generate_random_map\n\n# Test that FrozenLake map generation creates valid maps of various sizes.\ndef test_frozenlake_dfs_map_generation():\n\n    def frozenlake_dfs_path_exists(res):\n        frontier, discovered = [], set()\n        frontier.append((0,0))\n        while frontier:\n            r, c = frontier.pop()\n            if not (r,c) in discovered:\n                discovered.add((r,c))\n                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n                for x, y in directions:\n                    r_new = r + x\n                    c_new = c + y\n                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n                        continue\n                    if res[r_new][c_new] == 'G':\n                        return True\n                    if (res[r_new][c_new] not in '#H'):\n                        frontier.append((r_new, c_new))\n        return False\n\n    map_sizes = [5, 10, 200]\n    for size in map_sizes:\n        new_frozenlake = generate_random_map(size)\n        assert len(new_frozenlake) == size\n        assert len(new_frozenlake[0]) == size\n        assert frozenlake_dfs_path_exists(new_frozenlake)",
        "import numpy as np\nfrom gym import utils\nfrom gym.envs.mujoco import mujoco_env\n\nimport mujoco_py\n\nclass PusherEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    def __init__(self):\n        utils.EzPickle.__init__(self)\n        mujoco_env.MujocoEnv.__init__(self, 'pusher.xml', 5)\n\n    def step(self, a):\n        vec_1 = self.get_body_com(\"object\") - self.get_body_com(\"tips_arm\")\n        vec_2 = self.get_body_com(\"object\") - self.get_body_com(\"goal\")\n\n        reward_near = - np.linalg.norm(vec_1)\n        reward_dist = - np.linalg.norm(vec_2)\n        reward_ctrl = - np.square(a).sum()\n        reward = reward_dist + 0.1 * reward_ctrl + 0.5 * reward_near\n\n        self.do_simulation(a, self.frame_skip)\n        ob = self._get_obs()\n        done = False\n        return ob, reward, done, dict(reward_dist=reward_dist,\n                reward_ctrl=reward_ctrl)\n\n    def viewer_setup(self):\n        self.viewer.cam.trackbodyid = -1\n        self.viewer.cam.distance = 4.0\n\n    def reset_model(self):\n        qpos = self.init_qpos\n\n        self.goal_pos = np.asarray([0, 0])\n        while True:\n            self.cylinder_pos = np.concatenate([\n                    self.np_random.uniform(low=-0.3, high=0, size=1),\n                    self.np_random.uniform(low=-0.2, high=0.2, size=1)])\n            if np.linalg.norm(self.cylinder_pos - self.goal_pos) > 0.17:\n                break\n\n        qpos[-4:-2] = self.cylinder_pos\n        qpos[-2:] = self.goal_pos\n        qvel = self.init_qvel + self.np_random.uniform(low=-0.005,\n                high=0.005, size=self.model.nv)\n        qvel[-4:] = 0\n        self.set_state(qpos, qvel)\n        return self._get_obs()\n\n    def _get_obs(self):\n        return np.concatenate([\n            self.sim.data.qpos.flat[:7],\n            self.sim.data.qvel.flat[:7],\n            self.get_body_com(\"tips_arm\"),\n            self.get_body_com(\"object\"),\n            self.get_body_com(\"goal\"),\n        ])",
        "import numpy as np\nfrom gym.envs.mujoco import mujoco_env\nfrom gym import utils\n\n\nDEFAULT_CAMERA_CONFIG = {\n    'trackbodyid': 1,\n    'distance': 4.0,\n    'lookat': np.array((0.0, 0.0, 2.0)),\n    'elevation': -20.0,\n}\n\n\ndef mass_center(model, sim):\n    mass = np.expand_dims(model.body_mass, axis=1)\n    xpos = sim.data.xipos\n    return (np.sum(mass * xpos, axis=0) / np.sum(mass))[0:2].copy()\n\n\nclass HumanoidEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    def __init__(self,\n                 xml_file='humanoid.xml',\n                 forward_reward_weight=1.25,\n                 ctrl_cost_weight=0.1,\n                 contact_cost_weight=5e-7,\n                 contact_cost_range=(-np.inf, 10.0),\n                 healthy_reward=5.0,\n                 terminate_when_unhealthy=True,\n                 healthy_z_range=(1.0, 2.0),\n                 reset_noise_scale=1e-2,\n                 exclude_current_positions_from_observation=True):\n        utils.EzPickle.__init__(**locals())\n\n        self._forward_reward_weight = forward_reward_weight\n        self._ctrl_cost_weight = ctrl_cost_weight\n        self._contact_cost_weight = contact_cost_weight\n        self._contact_cost_range = contact_cost_range\n        self._healthy_reward = healthy_reward\n        self._terminate_when_unhealthy = terminate_when_unhealthy\n        self._healthy_z_range = healthy_z_range\n\n        self._reset_noise_scale = reset_noise_scale\n\n        self._exclude_current_positions_from_observation = (\n            exclude_current_positions_from_observation)\n\n        mujoco_env.MujocoEnv.__init__(self, xml_file, 5)\n\n    @property\n    def healthy_reward(self):\n        return float(\n            self.is_healthy\n            or self._terminate_when_unhealthy\n        ) * self._healthy_reward\n\n    def control_cost(self, action):\n        control_cost = self._ctrl_cost_weight * np.sum(\n            np.square(self.sim.data.ctrl))\n        return control_cost\n\n    @property\n    def contact_cost(self):\n        contact_forces = self.sim.data.cfrc_ext\n        contact_cost = self._contact_cost_weight * np.sum(\n            np.square(contact_forces))\n        min_cost, max_cost = self._contact_cost_range\n        contact_cost = np.clip(contact_cost, min_cost, max_cost)\n        return contact_cost\n\n    @property\n    def is_healthy(self):\n        min_z, max_z = self._healthy_z_range\n        is_healthy = min_z < self.sim.data.qpos[2] < max_z\n\n        return is_healthy\n\n    @property\n    def done(self):\n        done = ((not self.is_healthy)\n                if self._terminate_when_unhealthy\n                else False)\n        return done\n\n    def _get_obs(self):\n        position = self.sim.data.qpos.flat.copy()\n        velocity = self.sim.data.qvel.flat.copy()\n\n        com_inertia = self.sim.data.cinert.flat.copy()\n        com_velocity = self.sim.data.cvel.flat.copy()\n\n        actuator_forces = self.sim.data.qfrc_actuator.flat.copy()\n        external_contact_forces = self.sim.data.cfrc_ext.flat.copy()\n\n        if self._exclude_current_positions_from_observation:\n            position = position[2:]\n\n        return np.concatenate((\n            position,\n            velocity,\n            com_inertia,\n            com_velocity,\n            actuator_forces,\n            external_contact_forces,\n        ))\n\n    def step(self, action):\n        xy_position_before = mass_center(self.model, self.sim)\n        self.do_simulation(action, self.frame_skip)\n        xy_position_after = mass_center(self.model, self.sim)\n\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\n        x_velocity, y_velocity = xy_velocity\n\n        ctrl_cost = self.control_cost(action)\n        contact_cost = self.contact_cost\n\n        forward_reward = self._forward_reward_weight * x_velocity\n        healthy_reward = self.healthy_reward\n\n        rewards = forward_reward + healthy_reward\n        costs = ctrl_cost + contact_cost\n\n        observation = self._get_obs()\n        reward = rewards - costs\n        done = self.done\n        info = {\n            'reward_linvel': forward_reward,\n            'reward_quadctrl': -ctrl_cost,\n            'reward_alive': healthy_reward,\n            'reward_impact': -contact_cost,\n\n            'x_position': xy_position_after[0],\n            'y_position': xy_position_after[1],\n            'distance_from_origin': np.linalg.norm(xy_position_after, ord=2),\n\n            'x_velocity': x_velocity,\n            'y_velocity': y_velocity,\n            'forward_reward': forward_reward,\n        }\n\n        return observation, reward, done, info\n\n    def reset_model(self):\n        noise_low = -self._reset_noise_scale\n        noise_high = self._reset_noise_scale\n\n        qpos = self.init_qpos + self.np_random.uniform(\n            low=noise_low, high=noise_high, size=self.model.nq)\n        qvel = self.init_qvel + self.np_random.uniform(\n            low=noise_low, high=noise_high, size=self.model.nv)\n        self.set_state(qpos, qvel)\n\n        observation = self._get_obs()\n        return observation\n\n    def viewer_setup(self):\n        for key, value in DEFAULT_CAMERA_CONFIG.items():\n            if isinstance(value, np.ndarray):\n                getattr(self.viewer.cam, key)[:] = value\n            else:\n                setattr(self.viewer.cam, key, value)",
        "import numpy as np\nfrom gym.envs.mujoco import mujoco_env\nfrom gym import utils\n\ndef mass_center(model, sim):\n    mass = np.expand_dims(model.body_mass, 1)\n    xpos = sim.data.xipos\n    return (np.sum(mass * xpos, 0) / np.sum(mass))[0]\n\nclass HumanoidEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    def __init__(self):\n        mujoco_env.MujocoEnv.__init__(self, 'humanoid.xml', 5)\n        utils.EzPickle.__init__(self)\n\n    def _get_obs(self):\n        data = self.sim.data\n        return np.concatenate([data.qpos.flat[2:],\n                               data.qvel.flat,\n                               data.cinert.flat,\n                               data.cvel.flat,\n                               data.qfrc_actuator.flat,\n                               data.cfrc_ext.flat])\n\n    def step(self, a):\n        pos_before = mass_center(self.model, self.sim)\n        self.do_simulation(a, self.frame_skip)\n        pos_after = mass_center(self.model, self.sim)\n        alive_bonus = 5.0\n        data = self.sim.data\n        lin_vel_cost = 1.25 * (pos_after - pos_before) / self.dt\n        quad_ctrl_cost = 0.1 * np.square(data.ctrl).sum()\n        quad_impact_cost = .5e-6 * np.square(data.cfrc_ext).sum()\n        quad_impact_cost = min(quad_impact_cost, 10)\n        reward = lin_vel_cost - quad_ctrl_cost - quad_impact_cost + alive_bonus\n        qpos = self.sim.data.qpos\n        done = bool((qpos[2] < 1.0) or (qpos[2] > 2.0))\n        return self._get_obs(), reward, done, dict(reward_linvel=lin_vel_cost, reward_quadctrl=-quad_ctrl_cost, reward_alive=alive_bonus, reward_impact=-quad_impact_cost)\n\n    def reset_model(self):\n        c = 0.01\n        self.set_state(\n            self.init_qpos + self.np_random.uniform(low=-c, high=c, size=self.model.nq),\n            self.init_qvel + self.np_random.uniform(low=-c, high=c, size=self.model.nv,)\n        )\n        return self._get_obs()\n\n    def viewer_setup(self):\n        self.viewer.cam.trackbodyid = 1\n        self.viewer.cam.distance = self.model.stat.extent * 1.0\n        self.viewer.cam.lookat[2] = 2.0\n        self.viewer.cam.elevation = -20",
        "import numpy as np\nfrom gym.envs.mujoco import mujoco_env\nfrom gym import utils\n\n\nDEFAULT_CAMERA_CONFIG = {}\n\n\nclass SwimmerEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    def __init__(self,\n                 xml_file='swimmer.xml',\n                 forward_reward_weight=1.0,\n                 ctrl_cost_weight=1e-4,\n                 reset_noise_scale=0.1,\n                 exclude_current_positions_from_observation=True):\n        utils.EzPickle.__init__(**locals())\n\n        self._forward_reward_weight = forward_reward_weight\n        self._ctrl_cost_weight = ctrl_cost_weight\n\n        self._reset_noise_scale = reset_noise_scale\n\n        self._exclude_current_positions_from_observation = (\n            exclude_current_positions_from_observation)\n\n        mujoco_env.MujocoEnv.__init__(self, xml_file, 4)\n\n    def control_cost(self, action):\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\n        return control_cost\n\n    def step(self, action):\n        xy_position_before = self.sim.data.qpos[0:2].copy()\n        self.do_simulation(action, self.frame_skip)\n        xy_position_after = self.sim.data.qpos[0:2].copy()\n\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\n        x_velocity, y_velocity = xy_velocity\n\n        forward_reward = self._forward_reward_weight * x_velocity\n\n        ctrl_cost = self.control_cost(action)\n\n        observation = self._get_obs()\n        reward = forward_reward - ctrl_cost\n        done = False\n        info = {\n            'reward_fwd': forward_reward,\n            'reward_ctrl': -ctrl_cost,\n\n            'x_position': xy_position_after[0],\n            'y_position': xy_position_after[1],\n            'distance_from_origin': np.linalg.norm(xy_position_after, ord=2),\n\n            'x_velocity': x_velocity,\n            'y_velocity': y_velocity,\n            'forward_reward': forward_reward,\n        }\n\n        return observation, reward, done, info\n\n    def _get_obs(self):\n        position = self.sim.data.qpos.flat.copy()\n        velocity = self.sim.data.qvel.flat.copy()\n\n        if self._exclude_current_positions_from_observation:\n            position = position[2:]\n\n        observation = np.concatenate([position, velocity]).ravel()\n        return observation\n\n    def reset_model(self):\n        noise_low = -self._reset_noise_scale\n        noise_high = self._reset_noise_scale\n\n        qpos = self.init_qpos + self.np_random.uniform(\n            low=noise_low, high=noise_high, size=self.model.nq)\n        qvel = self.init_qvel + self.np_random.uniform(\n            low=noise_low, high=noise_high, size=self.model.nv)\n\n        self.set_state(qpos, qvel)\n\n        observation = self._get_obs()\n        return observation\n\n    def viewer_setup(self):\n        for key, value in DEFAULT_CAMERA_CONFIG.items():\n            if isinstance(value, np.ndarray):\n                getattr(self.viewer.cam, key)[:] = value\n            else:\n                setattr(self.viewer.cam, key, value)",
        "from gym.envs.mujoco import mujoco_env\nfrom gym import utils\nimport numpy as np\n\nclass HumanoidStandupEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    def __init__(self):\n        mujoco_env.MujocoEnv.__init__(self, 'humanoidstandup.xml', 5)\n        utils.EzPickle.__init__(self)\n\n    def _get_obs(self):\n        data = self.sim.data\n        return np.concatenate([data.qpos.flat[2:],\n                               data.qvel.flat,\n                               data.cinert.flat,\n                               data.cvel.flat,\n                               data.qfrc_actuator.flat,\n                               data.cfrc_ext.flat])\n\n    def step(self, a):\n        self.do_simulation(a, self.frame_skip)\n        pos_after = self.sim.data.qpos[2]\n        data = self.sim.data\n        uph_cost = (pos_after - 0) / self.model.opt.timestep\n\n        quad_ctrl_cost = 0.1 * np.square(data.ctrl).sum()\n        quad_impact_cost = .5e-6 * np.square(data.cfrc_ext).sum()\n        quad_impact_cost = min(quad_impact_cost, 10)\n        reward = uph_cost - quad_ctrl_cost - quad_impact_cost + 1\n\n        done = bool(False)\n        return self._get_obs(), reward, done, dict(reward_linup=uph_cost, reward_quadctrl=-quad_ctrl_cost, reward_impact=-quad_impact_cost)\n\n    def reset_model(self):\n        c = 0.01\n        self.set_state(\n            self.init_qpos + self.np_random.uniform(low=-c, high=c, size=self.model.nq),\n            self.init_qvel + self.np_random.uniform(low=-c, high=c, size=self.model.nv,)\n        )\n        return self._get_obs()\n\n    def viewer_setup(self):\n        self.viewer.cam.trackbodyid = 1\n        self.viewer.cam.distance = self.model.stat.extent * 1.0\n        self.viewer.cam.lookat[2] = 0.8925\n        self.viewer.cam.elevation = -20",
        "import numpy as np\nfrom gym import utils\nfrom gym.envs.mujoco import mujoco_env\n\nclass ReacherEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    def __init__(self):\n        utils.EzPickle.__init__(self)\n        mujoco_env.MujocoEnv.__init__(self, 'reacher.xml', 2)\n\n    def step(self, a):\n        vec = self.get_body_com(\"fingertip\")-self.get_body_com(\"target\")\n        reward_dist = - np.linalg.norm(vec)\n        reward_ctrl = - np.square(a).sum()\n        reward = reward_dist + reward_ctrl\n        self.do_simulation(a, self.frame_skip)\n        ob = self._get_obs()\n        done = False\n        return ob, reward, done, dict(reward_dist=reward_dist, reward_ctrl=reward_ctrl)\n\n    def viewer_setup(self):\n        self.viewer.cam.trackbodyid = 0\n\n    def reset_model(self):\n        qpos = self.np_random.uniform(low=-0.1, high=0.1, size=self.model.nq) + self.init_qpos\n        while True:\n            self.goal = self.np_random.uniform(low=-.2, high=.2, size=2)\n            if np.linalg.norm(self.goal) < 0.2:\n                break\n        qpos[-2:] = self.goal\n        qvel = self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)\n        qvel[-2:] = 0\n        self.set_state(qpos, qvel)\n        return self._get_obs()\n\n    def _get_obs(self):\n        theta = self.sim.data.qpos.flat[:2]\n        return np.concatenate([\n            np.cos(theta),\n            np.sin(theta),\n            self.sim.data.qpos.flat[2:],\n            self.sim.data.qvel.flat[:2],\n            self.get_body_com(\"fingertip\") - self.get_body_com(\"target\")\n        ])",
        "import numpy as np\nfrom gym import utils\nfrom gym.envs.mujoco import mujoco_env\n\n\nDEFAULT_CAMERA_CONFIG = {\n    'distance': 4.0,\n}\n\n\nclass AntEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    def __init__(self,\n                 xml_file='ant.xml',\n                 ctrl_cost_weight=0.5,\n                 contact_cost_weight=5e-4,\n                 healthy_reward=1.0,\n                 terminate_when_unhealthy=True,\n                 healthy_z_range=(0.2, 1.0),\n                 contact_force_range=(-1.0, 1.0),\n                 reset_noise_scale=0.1,\n                 exclude_current_positions_from_observation=True):\n        utils.EzPickle.__init__(**locals())\n\n        self._ctrl_cost_weight = ctrl_cost_weight\n        self._contact_cost_weight = contact_cost_weight\n\n        self._healthy_reward = healthy_reward\n        self._terminate_when_unhealthy = terminate_when_unhealthy\n        self._healthy_z_range = healthy_z_range\n\n        self._contact_force_range = contact_force_range\n\n        self._reset_noise_scale = reset_noise_scale\n\n        self._exclude_current_positions_from_observation = (\n            exclude_current_positions_from_observation)\n\n        mujoco_env.MujocoEnv.__init__(self, xml_file, 5)\n\n    @property\n    def healthy_reward(self):\n        return float(\n            self.is_healthy\n            or self._terminate_when_unhealthy\n        ) * self._healthy_reward\n\n    def control_cost(self, action):\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\n        return control_cost\n\n    @property\n    def contact_forces(self):\n        raw_contact_forces = self.sim.data.cfrc_ext\n        min_value, max_value = self._contact_force_range\n        contact_forces = np.clip(raw_contact_forces, min_value, max_value)\n        return contact_forces\n\n    @property\n    def contact_cost(self):\n        contact_cost = self._contact_cost_weight * np.sum(\n            np.square(self.contact_forces))\n        return contact_cost\n\n    @property\n    def is_healthy(self):\n        state = self.state_vector()\n        min_z, max_z = self._healthy_z_range\n        is_healthy = (np.isfinite(state).all() and min_z <= state[2] <= max_z)\n        return is_healthy\n\n    @property\n    def done(self):\n        done = (not self.is_healthy\n                if self._terminate_when_unhealthy\n                else False)\n        return done\n\n    def step(self, action):\n        xy_position_before = self.get_body_com(\"torso\")[:2].copy()\n        self.do_simulation(action, self.frame_skip)\n        xy_position_after = self.get_body_com(\"torso\")[:2].copy()\n\n        xy_velocity = (xy_position_after - xy_position_before) / self.dt\n        x_velocity, y_velocity = xy_velocity\n\n        ctrl_cost = self.control_cost(action)\n        contact_cost = self.contact_cost\n\n        forward_reward = x_velocity\n        healthy_reward = self.healthy_reward\n\n        rewards = forward_reward + healthy_reward\n        costs = ctrl_cost + contact_cost\n\n        reward = rewards - costs\n        done = self.done\n        observation = self._get_obs()\n        info = {\n            'reward_forward': forward_reward,\n            'reward_ctrl': -ctrl_cost,\n            'reward_contact': -contact_cost,\n            'reward_survive': healthy_reward,\n\n            'x_position': xy_position_after[0],\n            'y_position': xy_position_after[1],\n            'distance_from_origin': np.linalg.norm(xy_position_after, ord=2),\n\n            'x_velocity': x_velocity,\n            'y_velocity': y_velocity,\n            'forward_reward': forward_reward,\n        }\n\n        return observation, reward, done, info\n\n    def _get_obs(self):\n        position = self.sim.data.qpos.flat.copy()\n        velocity = self.sim.data.qvel.flat.copy()\n        contact_force = self.contact_forces.flat.copy()\n\n        if self._exclude_current_positions_from_observation:\n            position = position[2:]\n\n        observations = np.concatenate((position, velocity, contact_force))\n\n        return observations\n\n    def reset_model(self):\n        noise_low = -self._reset_noise_scale\n        noise_high = self._reset_noise_scale\n\n        qpos = self.init_qpos + self.np_random.uniform(\n            low=noise_low, high=noise_high, size=self.model.nq)\n        qvel = self.init_qvel + self._reset_noise_scale * self.np_random.randn(\n            self.model.nv)\n        self.set_state(qpos, qvel)\n\n        observation = self._get_obs()\n\n        return observation\n\n    def viewer_setup(self):\n        for key, value in DEFAULT_CAMERA_CONFIG.items():\n            if isinstance(value, np.ndarray):\n                getattr(self.viewer.cam, key)[:] = value\n            else:\n                setattr(self.viewer.cam, key, value)",
        "import numpy as np\nfrom gym import utils\nfrom gym.envs.mujoco import mujoco_env\n\nclass StrikerEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    def __init__(self):\n        utils.EzPickle.__init__(self)\n        self._striked = False\n        self._min_strike_dist = np.inf\n        self.strike_threshold = 0.1\n        mujoco_env.MujocoEnv.__init__(self, 'striker.xml', 5)\n\n    def step(self, a):\n        vec_1 = self.get_body_com(\"object\") - self.get_body_com(\"tips_arm\")\n        vec_2 = self.get_body_com(\"object\") - self.get_body_com(\"goal\")\n        self._min_strike_dist = min(self._min_strike_dist, np.linalg.norm(vec_2))\n\n        if np.linalg.norm(vec_1) < self.strike_threshold:\n            self._striked = True\n            self._strike_pos = self.get_body_com(\"tips_arm\")\n\n        if self._striked:\n            vec_3 = self.get_body_com(\"object\") - self._strike_pos\n            reward_near = - np.linalg.norm(vec_3)\n        else:\n            reward_near = - np.linalg.norm(vec_1)\n\n        reward_dist = - np.linalg.norm(self._min_strike_dist)\n        reward_ctrl = - np.square(a).sum()\n        reward = 3 * reward_dist + 0.1 * reward_ctrl + 0.5 * reward_near\n\n        self.do_simulation(a, self.frame_skip)\n        ob = self._get_obs()\n        done = False\n        return ob, reward, done, dict(reward_dist=reward_dist,\n                reward_ctrl=reward_ctrl)\n\n    def viewer_setup(self):\n        self.viewer.cam.trackbodyid = 0\n        self.viewer.cam.distance = 4.0\n\n    def reset_model(self):\n        self._min_strike_dist = np.inf\n        self._striked = False\n        self._strike_pos = None\n\n        qpos = self.init_qpos\n\n        self.ball = np.array([0.5, -0.175])\n        while True:\n            self.goal = np.concatenate([\n                    self.np_random.uniform(low=0.15, high=0.7, size=1),\n                    self.np_random.uniform(low=0.1, high=1.0, size=1)])\n            if np.linalg.norm(self.ball - self.goal) > 0.17:\n                break\n\n        qpos[-9:-7] = [self.ball[1], self.ball[0]]\n        qpos[-7:-5] = self.goal\n        diff = self.ball - self.goal\n        angle = -np.arctan(diff[0] / (diff[1] + 1e-8))\n        qpos[-1] = angle / 3.14\n        qvel = self.init_qvel + self.np_random.uniform(low=-.1, high=.1,\n                size=self.model.nv)\n        qvel[7:] = 0\n        self.set_state(qpos, qvel)\n        return self._get_obs()\n\n    def _get_obs(self):\n        return np.concatenate([\n            self.sim.data.qpos.flat[:7],\n            self.sim.data.qvel.flat[:7],\n            self.get_body_com(\"tips_arm\"),\n            self.get_body_com(\"object\"),\n            self.get_body_com(\"goal\"),\n        ])",
        "import numpy as np\nfrom gym import utils\nfrom gym.envs.mujoco import mujoco_env\n\nclass AntEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    def __init__(self):\n        mujoco_env.MujocoEnv.__init__(self, 'ant.xml', 5)\n        utils.EzPickle.__init__(self)\n\n    def step(self, a):\n        xposbefore = self.get_body_com(\"torso\")[0]\n        self.do_simulation(a, self.frame_skip)\n        xposafter = self.get_body_com(\"torso\")[0]\n        forward_reward = (xposafter - xposbefore)/self.dt\n        ctrl_cost = .5 * np.square(a).sum()\n        contact_cost = 0.5 * 1e-3 * np.sum(\n            np.square(np.clip(self.sim.data.cfrc_ext, -1, 1)))\n        survive_reward = 1.0\n        reward = forward_reward - ctrl_cost - contact_cost + survive_reward\n        state = self.state_vector()\n        notdone = np.isfinite(state).all() \\\n            and state[2] >= 0.2 and state[2] <= 1.0\n        done = not notdone\n        ob = self._get_obs()\n        return ob, reward, done, dict(\n            reward_forward=forward_reward,\n            reward_ctrl=-ctrl_cost,\n            reward_contact=-contact_cost,\n            reward_survive=survive_reward)\n\n    def _get_obs(self):\n        return np.concatenate([\n            self.sim.data.qpos.flat[2:],\n            self.sim.data.qvel.flat,\n            np.clip(self.sim.data.cfrc_ext, -1, 1).flat,\n        ])\n\n    def reset_model(self):\n        qpos = self.init_qpos + self.np_random.uniform(size=self.model.nq, low=-.1, high=.1)\n        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1\n        self.set_state(qpos, qvel)\n        return self._get_obs()\n\n    def viewer_setup(self):\n        self.viewer.cam.distance = self.model.stat.extent * 0.5",
        "from gym.envs.mujoco.mujoco_env import MujocoEnv\n# ^^^^^ so that user gets the correct error\n# message if mujoco is not installed correctly\nfrom gym.envs.mujoco.ant import AntEnv\nfrom gym.envs.mujoco.half_cheetah import HalfCheetahEnv\nfrom gym.envs.mujoco.hopper import HopperEnv\nfrom gym.envs.mujoco.walker2d import Walker2dEnv\nfrom gym.envs.mujoco.humanoid import HumanoidEnv\nfrom gym.envs.mujoco.inverted_pendulum import InvertedPendulumEnv\nfrom gym.envs.mujoco.inverted_double_pendulum import InvertedDoublePendulumEnv\nfrom gym.envs.mujoco.reacher import ReacherEnv\nfrom gym.envs.mujoco.swimmer import SwimmerEnv\nfrom gym.envs.mujoco.humanoidstandup import HumanoidStandupEnv\nfrom gym.envs.mujoco.pusher import PusherEnv\nfrom gym.envs.mujoco.thrower import ThrowerEnv\nfrom gym.envs.mujoco.striker import StrikerEnv",
        "import numpy as np\nfrom gym import utils\nfrom gym.envs.mujoco import mujoco_env\n\nclass HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    def __init__(self):\n        mujoco_env.MujocoEnv.__init__(self, 'half_cheetah.xml', 5)\n        utils.EzPickle.__init__(self)\n\n    def step(self, action):\n        xposbefore = self.sim.data.qpos[0]\n        self.do_simulation(action, self.frame_skip)\n        xposafter = self.sim.data.qpos[0]\n        ob = self._get_obs()\n        reward_ctrl = - 0.1 * np.square(action).sum()\n        reward_run = (xposafter - xposbefore)/self.dt\n        reward = reward_ctrl + reward_run\n        done = False\n        return ob, reward, done, dict(reward_run=reward_run, reward_ctrl=reward_ctrl)\n\n    def _get_obs(self):\n        return np.concatenate([\n            self.sim.data.qpos.flat[1:],\n            self.sim.data.qvel.flat,\n        ])\n\n    def reset_model(self):\n        qpos = self.init_qpos + self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)\n        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1\n        self.set_state(qpos, qvel)\n        return self._get_obs()\n\n    def viewer_setup(self):\n        self.viewer.cam.distance = self.model.stat.extent * 0.5",
        "from collections import OrderedDict\nimport os\n\n\nfrom gym import error, spaces\nfrom gym.utils import seeding\nimport numpy as np\nfrom os import path\nimport gym\n\ntry:\n    import mujoco_py\nexcept ImportError as e:\n    raise error.DependencyNotInstalled(\"{}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)\".format(e))\n\nDEFAULT_SIZE = 500\n\n\ndef convert_observation_to_space(observation):\n    if isinstance(observation, dict):\n        space = spaces.Dict(OrderedDict([\n            (key, convert_observation_to_space(value))\n            for key, value in observation.items()\n        ]))\n    elif isinstance(observation, np.ndarray):\n        low = np.full(observation.shape, -float('inf'), dtype=np.float32)\n        high = np.full(observation.shape, float('inf'), dtype=np.float32)\n        space = spaces.Box(low, high, dtype=observation.dtype)\n    else:\n        raise NotImplementedError(type(observation), observation)\n\n    return space\n\n\nclass MujocoEnv(gym.Env):\n    \"\"\"Superclass for all MuJoCo environments.\n    \"\"\"\n\n    def __init__(self, model_path, frame_skip):\n        if model_path.startswith(\"/\"):\n            fullpath = model_path\n        else:\n            fullpath = os.path.join(os.path.dirname(__file__), \"assets\", model_path)\n        if not path.exists(fullpath):\n            raise IOError(\"File %s does not exist\" % fullpath)\n        self.frame_skip = frame_skip\n        self.model = mujoco_py.load_model_from_path(fullpath)\n        self.sim = mujoco_py.MjSim(self.model)\n        self.data = self.sim.data\n        self.viewer = None\n        self._viewers = {}\n\n        self.metadata = {\n            'render.modes': ['human', 'rgb_array', 'depth_array'],\n            'video.frames_per_second': int(np.round(1.0 / self.dt))\n        }\n\n        self.init_qpos = self.sim.data.qpos.ravel().copy()\n        self.init_qvel = self.sim.data.qvel.ravel().copy()\n\n        self._set_action_space()\n\n        action = self.action_space.sample()\n        observation, _reward, done, _info = self.step(action)\n        assert not done\n\n        self._set_observation_space(observation)\n\n        self.seed()\n\n    def _set_action_space(self):\n        bounds = self.model.actuator_ctrlrange.copy().astype(np.float32)\n        low, high = bounds.T\n        self.action_space = spaces.Box(low=low, high=high, dtype=np.float32)\n        return self.action_space\n\n    def _set_observation_space(self, observation):\n        self.observation_space = convert_observation_to_space(observation)\n        return self.observation_space\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    # methods to override:\n    # ----------------------------\n\n    def reset_model(self):\n        \"\"\"\n        Reset the robot degrees of freedom (qpos and qvel).\n        Implement this in each subclass.\n        \"\"\"\n        raise NotImplementedError\n\n    def viewer_setup(self):\n        \"\"\"\n        This method is called when the viewer is initialized.\n        Optionally implement this method, if you need to tinker with camera position\n        and so forth.\n        \"\"\"\n        pass\n\n    # -----------------------------\n\n    def reset(self):\n        self.sim.reset()\n        ob = self.reset_model()\n        return ob\n\n    def set_state(self, qpos, qvel):\n        assert qpos.shape == (self.model.nq,) and qvel.shape == (self.model.nv,)\n        old_state = self.sim.get_state()\n        new_state = mujoco_py.MjSimState(old_state.time, qpos, qvel,\n                                         old_state.act, old_state.udd_state)\n        self.sim.set_state(new_state)\n        self.sim.forward()\n\n    @property\n    def dt(self):\n        return self.model.opt.timestep * self.frame_skip\n\n    def do_simulation(self, ctrl, n_frames):\n        self.sim.data.ctrl[:] = ctrl\n        for _ in range(n_frames):\n            self.sim.step()\n\n    def render(self,\n               mode='human',\n               width=DEFAULT_SIZE,\n               height=DEFAULT_SIZE,\n               camera_id=None,\n               camera_name=None):\n        if mode == 'rgb_array':\n            if camera_id is not None and camera_name is not None:\n                raise ValueError(\"Both `camera_id` and `camera_name` cannot be\"\n                                 \" specified at the same time.\")\n\n            no_camera_specified = camera_name is None and camera_id is None\n            if no_camera_specified:\n                camera_name = 'track'\n\n            if camera_id is None and camera_name in self.model._camera_name2id:\n                camera_id = self.model.camera_name2id(camera_name)\n\n            self._get_viewer(mode).render(width, height, camera_id=camera_id)\n            # window size used for old mujoco-py:\n            data = self._get_viewer(mode).read_pixels(width, height, depth=False)\n            # original image is upside-down, so flip it\n            return data[::-1, :, :]\n        elif mode == 'depth_array':\n            self._get_viewer(mode).render(width, height)\n            # window size used for old mujoco-py:\n            # Extract depth part of the read_pixels() tuple\n            data = self._get_viewer(mode).read_pixels(width, height, depth=True)[1]\n            # original image is upside-down, so flip it\n            return data[::-1, :]\n        elif mode == 'human':\n            self._get_viewer(mode).render()\n\n    def close(self):\n        if self.viewer is not None:\n            # self.viewer.finish()\n            self.viewer = None\n            self._viewers = {}\n\n    def _get_viewer(self, mode):\n        self.viewer = self._viewers.get(mode)\n        if self.viewer is None:\n            if mode == 'human':\n                self.viewer = mujoco_py.MjViewer(self.sim)\n            elif mode == 'rgb_array' or mode == 'depth_array':\n                self.viewer = mujoco_py.MjRenderContextOffscreen(self.sim, -1)\n\n            self.viewer_setup()\n            self._viewers[mode] = self.viewer\n        return self.viewer\n\n    def get_body_com(self, body_name):\n        return self.data.get_body_xpos(body_name)\n\n    def state_vector(self):\n        return np.concatenate([\n            self.sim.data.qpos.flat,\n            self.sim.data.qvel.flat\n        ])",
        "import numpy as np\nfrom gym import utils\nfrom gym.envs.mujoco import mujoco_env\n\nclass HopperEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    def __init__(self):\n        mujoco_env.MujocoEnv.__init__(self, 'hopper.xml', 4)\n        utils.EzPickle.__init__(self)\n\n    def step(self, a):\n        posbefore = self.sim.data.qpos[0]\n        self.do_simulation(a, self.frame_skip)\n        posafter, height, ang = self.sim.data.qpos[0:3]\n        alive_bonus = 1.0\n        reward = (posafter - posbefore) / self.dt\n        reward += alive_bonus\n        reward -= 1e-3 * np.square(a).sum()\n        s = self.state_vector()\n        done = not (np.isfinite(s).all() and (np.abs(s[2:]) < 100).all() and\n                    (height > .7) and (abs(ang) < .2))\n        ob = self._get_obs()\n        return ob, reward, done, {}\n\n    def _get_obs(self):\n        return np.concatenate([\n            self.sim.data.qpos.flat[1:],\n            np.clip(self.sim.data.qvel.flat, -10, 10)\n        ])\n\n    def reset_model(self):\n        qpos = self.init_qpos + self.np_random.uniform(low=-.005, high=.005, size=self.model.nq)\n        qvel = self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)\n        self.set_state(qpos, qvel)\n        return self._get_obs()\n\n    def viewer_setup(self):\n        self.viewer.cam.trackbodyid = 2\n        self.viewer.cam.distance = self.model.stat.extent * 0.75\n        self.viewer.cam.lookat[2] = 1.15\n        self.viewer.cam.elevation = -20",
        "import numpy as np\nfrom gym import utils\nfrom gym.envs.mujoco import mujoco_env\n\nclass ThrowerEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    def __init__(self):\n        utils.EzPickle.__init__(self)\n        self._ball_hit_ground = False\n        self._ball_hit_location = None\n        mujoco_env.MujocoEnv.__init__(self, 'thrower.xml', 5)\n\n    def step(self, a):\n        ball_xy = self.get_body_com(\"ball\")[:2]\n        goal_xy = self.get_body_com(\"goal\")[:2]\n\n        if not self._ball_hit_ground and self.get_body_com(\"ball\")[2] < -0.25:\n            self._ball_hit_ground = True\n            self._ball_hit_location = self.get_body_com(\"ball\")\n\n        if self._ball_hit_ground:\n            ball_hit_xy = self._ball_hit_location[:2]\n            reward_dist = -np.linalg.norm(ball_hit_xy - goal_xy)\n        else:\n            reward_dist = -np.linalg.norm(ball_xy - goal_xy)\n        reward_ctrl = - np.square(a).sum()\n\n        reward = reward_dist + 0.002 * reward_ctrl\n        self.do_simulation(a, self.frame_skip)\n        ob = self._get_obs()\n        done = False\n        return ob, reward, done, dict(reward_dist=reward_dist,\n                reward_ctrl=reward_ctrl)\n\n    def viewer_setup(self):\n        self.viewer.cam.trackbodyid = 0\n        self.viewer.cam.distance = 4.0\n\n    def reset_model(self):\n        self._ball_hit_ground = False\n        self._ball_hit_location = None\n\n        qpos = self.init_qpos\n        self.goal = np.array([self.np_random.uniform(low=-0.3, high=0.3),\n                              self.np_random.uniform(low=-0.3, high=0.3)])\n\n        qpos[-9:-7] = self.goal\n        qvel = self.init_qvel + self.np_random.uniform(low=-0.005,\n                high=0.005, size=self.model.nv)\n        qvel[7:] = 0\n        self.set_state(qpos, qvel)\n        return self._get_obs()\n\n    def _get_obs(self):\n        return np.concatenate([\n            self.sim.data.qpos.flat[:7],\n            self.sim.data.qvel.flat[:7],\n            self.get_body_com(\"r_wrist_roll_link\"),\n            self.get_body_com(\"ball\"),\n            self.get_body_com(\"goal\"),\n        ])",
        "import numpy as np\nfrom gym.envs.mujoco import mujoco_env\nfrom gym import utils\n\n\nDEFAULT_CAMERA_CONFIG = {\n    'trackbodyid': 2,\n    'distance': 3.0,\n    'lookat': np.array((0.0, 0.0, 1.15)),\n    'elevation': -20.0,\n}\n\n\nclass HopperEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    def __init__(self,\n                 xml_file='hopper.xml',\n                 forward_reward_weight=1.0,\n                 ctrl_cost_weight=1e-3,\n                 healthy_reward=1.0,\n                 terminate_when_unhealthy=True,\n                 healthy_state_range=(-100.0, 100.0),\n                 healthy_z_range=(0.7, float('inf')),\n                 healthy_angle_range=(-0.2, 0.2),\n                 reset_noise_scale=5e-3,\n                 exclude_current_positions_from_observation=True):\n        utils.EzPickle.__init__(**locals())\n\n        self._forward_reward_weight = forward_reward_weight\n\n        self._ctrl_cost_weight = ctrl_cost_weight\n\n        self._healthy_reward = healthy_reward\n        self._terminate_when_unhealthy = terminate_when_unhealthy\n\n        self._healthy_state_range = healthy_state_range\n        self._healthy_z_range = healthy_z_range\n        self._healthy_angle_range = healthy_angle_range\n\n        self._reset_noise_scale = reset_noise_scale\n\n        self._exclude_current_positions_from_observation = (\n            exclude_current_positions_from_observation)\n\n        mujoco_env.MujocoEnv.__init__(self, xml_file, 4)\n\n    @property\n    def healthy_reward(self):\n        return float(\n            self.is_healthy\n            or self._terminate_when_unhealthy\n        ) * self._healthy_reward\n\n    def control_cost(self, action):\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\n        return control_cost\n\n    @property\n    def is_healthy(self):\n        z, angle = self.sim.data.qpos[1:3]\n        state = self.state_vector()[2:]\n\n        min_state, max_state = self._healthy_state_range\n        min_z, max_z = self._healthy_z_range\n        min_angle, max_angle = self._healthy_angle_range\n\n        healthy_state = np.all(\n            np.logical_and(min_state < state, state < max_state))\n        healthy_z = min_z < z < max_z\n        healthy_angle = min_angle < angle < max_angle\n\n        is_healthy = all((healthy_state, healthy_z, healthy_angle))\n\n        return is_healthy\n\n    @property\n    def done(self):\n        done = (not self.is_healthy\n                if self._terminate_when_unhealthy\n                else False)\n        return done\n\n    def _get_obs(self):\n        position = self.sim.data.qpos.flat.copy()\n        velocity = np.clip(\n            self.sim.data.qvel.flat.copy(), -10, 10)\n\n        if self._exclude_current_positions_from_observation:\n            position = position[1:]\n\n        observation = np.concatenate((position, velocity)).ravel()\n        return observation\n\n    def step(self, action):\n        x_position_before = self.sim.data.qpos[0]\n        self.do_simulation(action, self.frame_skip)\n        x_position_after = self.sim.data.qpos[0]\n        x_velocity = ((x_position_after - x_position_before)\n                      / self.dt)\n\n        ctrl_cost = self.control_cost(action)\n\n        forward_reward = self._forward_reward_weight * x_velocity\n        healthy_reward = self.healthy_reward\n\n        rewards = forward_reward + healthy_reward\n        costs = ctrl_cost\n\n        observation = self._get_obs()\n        reward = rewards - costs\n        done = self.done\n        info = {\n            'x_position': x_position_after,\n            'x_velocity': x_velocity,\n        }\n\n        return observation, reward, done, info\n\n    def reset_model(self):\n        noise_low = -self._reset_noise_scale\n        noise_high = self._reset_noise_scale\n\n        qpos = self.init_qpos + self.np_random.uniform(\n            low=noise_low, high=noise_high, size=self.model.nq)\n        qvel = self.init_qvel + self.np_random.uniform(\n            low=noise_low, high=noise_high, size=self.model.nv)\n\n        self.set_state(qpos, qvel)\n\n        observation = self._get_obs()\n        return observation\n\n    def viewer_setup(self):\n        for key, value in DEFAULT_CAMERA_CONFIG.items():\n            if isinstance(value, np.ndarray):\n                getattr(self.viewer.cam, key)[:] = value\n            else:\n                setattr(self.viewer.cam, key, value)",
        "import numpy as np\nfrom gym import utils\nfrom gym.envs.mujoco import mujoco_env\n\nclass InvertedPendulumEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    def __init__(self):\n        utils.EzPickle.__init__(self)\n        mujoco_env.MujocoEnv.__init__(self, 'inverted_pendulum.xml', 2)\n\n    def step(self, a):\n        reward = 1.0\n        self.do_simulation(a, self.frame_skip)\n        ob = self._get_obs()\n        notdone = np.isfinite(ob).all() and (np.abs(ob[1]) <= .2)\n        done = not notdone\n        return ob, reward, done, {}\n\n    def reset_model(self):\n        qpos = self.init_qpos + self.np_random.uniform(size=self.model.nq, low=-0.01, high=0.01)\n        qvel = self.init_qvel + self.np_random.uniform(size=self.model.nv, low=-0.01, high=0.01)\n        self.set_state(qpos, qvel)\n        return self._get_obs()\n\n    def _get_obs(self):\n        return np.concatenate([self.sim.data.qpos, self.sim.data.qvel]).ravel()\n\n    def viewer_setup(self):\n        v = self.viewer\n        v.cam.trackbodyid = 0\n        v.cam.distance = self.model.stat.extent",
        "import numpy as np\nfrom gym.envs.mujoco import mujoco_env\nfrom gym import utils\n\n\nDEFAULT_CAMERA_CONFIG = {\n    'trackbodyid': 2,\n    'distance': 4.0,\n    'lookat': np.array((0.0, 0.0, 1.15)),\n    'elevation': -20.0,\n}\n\n\nclass Walker2dEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    def __init__(self,\n                 xml_file='walker2d.xml',\n                 forward_reward_weight=1.0,\n                 ctrl_cost_weight=1e-3,\n                 healthy_reward=1.0,\n                 terminate_when_unhealthy=True,\n                 healthy_z_range=(0.8, 2.0),\n                 healthy_angle_range=(-1.0, 1.0),\n                 reset_noise_scale=5e-3,\n                 exclude_current_positions_from_observation=True):\n        utils.EzPickle.__init__(**locals())\n\n        self._forward_reward_weight = forward_reward_weight\n        self._ctrl_cost_weight = ctrl_cost_weight\n\n        self._healthy_reward = healthy_reward\n        self._terminate_when_unhealthy = terminate_when_unhealthy\n\n        self._healthy_z_range = healthy_z_range\n        self._healthy_angle_range = healthy_angle_range\n\n        self._reset_noise_scale = reset_noise_scale\n\n        self._exclude_current_positions_from_observation = (\n            exclude_current_positions_from_observation)\n\n        mujoco_env.MujocoEnv.__init__(self, xml_file, 4)\n\n    @property\n    def healthy_reward(self):\n        return float(\n            self.is_healthy\n            or self._terminate_when_unhealthy\n        ) * self._healthy_reward\n\n    def control_cost(self, action):\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\n        return control_cost\n\n    @property\n    def is_healthy(self):\n        z, angle = self.sim.data.qpos[1:3]\n\n        min_z, max_z = self._healthy_z_range\n        min_angle, max_angle = self._healthy_angle_range\n\n        healthy_z = min_z < z < max_z\n        healthy_angle = min_angle < angle < max_angle\n        is_healthy = healthy_z and healthy_angle\n\n        return is_healthy\n\n    @property\n    def done(self):\n        done = (not self.is_healthy\n                if self._terminate_when_unhealthy\n                else False)\n        return done\n\n    def _get_obs(self):\n        position = self.sim.data.qpos.flat.copy()\n        velocity = np.clip(\n            self.sim.data.qvel.flat.copy(), -10, 10)\n\n        if self._exclude_current_positions_from_observation:\n            position = position[1:]\n\n        observation = np.concatenate((position, velocity)).ravel()\n        return observation\n\n    def step(self, action):\n        x_position_before = self.sim.data.qpos[0]\n        self.do_simulation(action, self.frame_skip)\n        x_position_after = self.sim.data.qpos[0]\n        x_velocity = ((x_position_after - x_position_before)\n                      / self.dt)\n\n        ctrl_cost = self.control_cost(action)\n\n        forward_reward = self._forward_reward_weight * x_velocity\n        healthy_reward = self.healthy_reward\n\n        rewards = forward_reward + healthy_reward\n        costs = ctrl_cost\n\n        observation = self._get_obs()\n        reward = rewards - costs\n        done = self.done\n        info = {\n            'x_position': x_position_after,\n            'x_velocity': x_velocity,\n        }\n\n        return observation, reward, done, info\n\n    def reset_model(self):\n        noise_low = -self._reset_noise_scale\n        noise_high = self._reset_noise_scale\n\n        qpos = self.init_qpos + self.np_random.uniform(\n            low=noise_low, high=noise_high, size=self.model.nq)\n        qvel = self.init_qvel + self.np_random.uniform(\n            low=noise_low, high=noise_high, size=self.model.nv)\n\n        self.set_state(qpos, qvel)\n\n        observation = self._get_obs()\n        return observation\n\n    def viewer_setup(self):\n        for key, value in DEFAULT_CAMERA_CONFIG.items():\n            if isinstance(value, np.ndarray):\n                getattr(self.viewer.cam, key)[:] = value\n            else:\n                setattr(self.viewer.cam, key, value)",
        "import numpy as np\nfrom gym import utils\nfrom gym.envs.mujoco import mujoco_env\n\nclass Walker2dEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n\n    def __init__(self):\n        mujoco_env.MujocoEnv.__init__(self, \"walker2d.xml\", 4)\n        utils.EzPickle.__init__(self)\n\n    def step(self, a):\n        posbefore = self.sim.data.qpos[0]\n        self.do_simulation(a, self.frame_skip)\n        posafter, height, ang = self.sim.data.qpos[0:3]\n        alive_bonus = 1.0\n        reward = ((posafter - posbefore) / self.dt)\n        reward += alive_bonus\n        reward -= 1e-3 * np.square(a).sum()\n        done = not (height > 0.8 and height < 2.0 and\n                    ang > -1.0 and ang < 1.0)\n        ob = self._get_obs()\n        return ob, reward, done, {}\n\n    def _get_obs(self):\n        qpos = self.sim.data.qpos\n        qvel = self.sim.data.qvel\n        return np.concatenate([qpos[1:], np.clip(qvel, -10, 10)]).ravel()\n\n    def reset_model(self):\n        self.set_state(\n            self.init_qpos + self.np_random.uniform(low=-.005, high=.005, size=self.model.nq),\n            self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)\n        )\n        return self._get_obs()\n\n    def viewer_setup(self):\n        self.viewer.cam.trackbodyid = 2\n        self.viewer.cam.distance = self.model.stat.extent * 0.5\n        self.viewer.cam.lookat[2] = 1.15\n        self.viewer.cam.elevation = -20",
        "import numpy as np\nfrom gym import utils\nfrom gym.envs.mujoco import mujoco_env\n\n\nDEFAULT_CAMERA_CONFIG = {\n    'distance': 4.0,\n}\n\n\nclass HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    def __init__(self,\n                 xml_file='half_cheetah.xml',\n                 forward_reward_weight=1.0,\n                 ctrl_cost_weight=0.1,\n                 reset_noise_scale=0.1,\n                 exclude_current_positions_from_observation=True):\n        utils.EzPickle.__init__(**locals())\n\n        self._forward_reward_weight = forward_reward_weight\n\n        self._ctrl_cost_weight = ctrl_cost_weight\n\n        self._reset_noise_scale = reset_noise_scale\n\n        self._exclude_current_positions_from_observation = (\n            exclude_current_positions_from_observation)\n\n        mujoco_env.MujocoEnv.__init__(self, xml_file, 5)\n\n    def control_cost(self, action):\n        control_cost = self._ctrl_cost_weight * np.sum(np.square(action))\n        return control_cost\n\n    def step(self, action):\n        x_position_before = self.sim.data.qpos[0]\n        self.do_simulation(action, self.frame_skip)\n        x_position_after = self.sim.data.qpos[0]\n        x_velocity = ((x_position_after - x_position_before)\n                      / self.dt)\n\n        ctrl_cost = self.control_cost(action)\n\n        forward_reward = self._forward_reward_weight * x_velocity\n\n        observation = self._get_obs()\n        reward = forward_reward - ctrl_cost\n        done = False\n        info = {\n            'x_position': x_position_after,\n            'x_velocity': x_velocity,\n\n            'reward_run': forward_reward,\n            'reward_ctrl': -ctrl_cost\n        }\n\n        return observation, reward, done, info\n\n    def _get_obs(self):\n        position = self.sim.data.qpos.flat.copy()\n        velocity = self.sim.data.qvel.flat.copy()\n\n        if self._exclude_current_positions_from_observation:\n            position = position[1:]\n\n        observation = np.concatenate((position, velocity)).ravel()\n        return observation\n\n    def reset_model(self):\n        noise_low = -self._reset_noise_scale\n        noise_high = self._reset_noise_scale\n\n        qpos = self.init_qpos + self.np_random.uniform(\n            low=noise_low, high=noise_high, size=self.model.nq)\n        qvel = self.init_qvel + self._reset_noise_scale * self.np_random.randn(\n            self.model.nv)\n\n        self.set_state(qpos, qvel)\n\n        observation = self._get_obs()\n        return observation\n\n    def viewer_setup(self):\n        for key, value in DEFAULT_CAMERA_CONFIG.items():\n            if isinstance(value, np.ndarray):\n                getattr(self.viewer.cam, key)[:] = value\n            else:\n                setattr(self.viewer.cam, key, value)",
        "import numpy as np\nfrom gym import utils\nfrom gym.envs.mujoco import mujoco_env\n\nclass InvertedDoublePendulumEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n\n    def __init__(self):\n        mujoco_env.MujocoEnv.__init__(self, 'inverted_double_pendulum.xml', 5)\n        utils.EzPickle.__init__(self)\n\n    def step(self, action):\n        self.do_simulation(action, self.frame_skip)\n        ob = self._get_obs()\n        x, _, y = self.sim.data.site_xpos[0]\n        dist_penalty = 0.01 * x ** 2 + (y - 2) ** 2\n        v1, v2 = self.sim.data.qvel[1:3]\n        vel_penalty = 1e-3 * v1**2 + 5e-3 * v2**2\n        alive_bonus = 10\n        r = alive_bonus - dist_penalty - vel_penalty\n        done = bool(y <= 1)\n        return ob, r, done, {}\n\n    def _get_obs(self):\n        return np.concatenate([\n            self.sim.data.qpos[:1],  # cart x pos\n            np.sin(self.sim.data.qpos[1:]),  # link angles\n            np.cos(self.sim.data.qpos[1:]),\n            np.clip(self.sim.data.qvel, -10, 10),\n            np.clip(self.sim.data.qfrc_constraint, -10, 10)\n        ]).ravel()\n\n    def reset_model(self):\n        self.set_state(\n            self.init_qpos + self.np_random.uniform(low=-.1, high=.1, size=self.model.nq),\n            self.init_qvel + self.np_random.randn(self.model.nv) * .1\n        )\n        return self._get_obs()\n\n    def viewer_setup(self):\n        v = self.viewer\n        v.cam.trackbodyid = 0\n        v.cam.distance = self.model.stat.extent * 0.5\n        v.cam.lookat[2] = 0.12250000000000005  # v.model.stat.center[2]",
        "import numpy as np\nfrom gym import utils\nfrom gym.envs.mujoco import mujoco_env\n\nclass SwimmerEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    def __init__(self):\n        mujoco_env.MujocoEnv.__init__(self, 'swimmer.xml', 4)\n        utils.EzPickle.__init__(self)\n\n    def step(self, a):\n        ctrl_cost_coeff = 0.0001\n        xposbefore = self.sim.data.qpos[0]\n        self.do_simulation(a, self.frame_skip)\n        xposafter = self.sim.data.qpos[0]\n        reward_fwd = (xposafter - xposbefore) / self.dt\n        reward_ctrl = - ctrl_cost_coeff * np.square(a).sum()\n        reward = reward_fwd + reward_ctrl\n        ob = self._get_obs()\n        return ob, reward, False, dict(reward_fwd=reward_fwd, reward_ctrl=reward_ctrl)\n\n    def _get_obs(self):\n        qpos = self.sim.data.qpos\n        qvel = self.sim.data.qvel\n        return np.concatenate([qpos.flat[2:], qvel.flat])\n\n    def reset_model(self):\n        self.set_state(\n            self.init_qpos + self.np_random.uniform(low=-.1, high=.1, size=self.model.nq),\n            self.init_qvel + self.np_random.uniform(low=-.1, high=.1, size=self.model.nv)\n        )\n        return self._get_obs()",
        "from gym.envs.registration import registry, register, make, spec\n\n# Algorithmic\n# ----------------------------------------\n\nregister(\n    id='Copy-v0',\n    entry_point='gym.envs.algorithmic:CopyEnv',\n    max_episode_steps=200,\n    reward_threshold=25.0,\n)\n\nregister(\n    id='RepeatCopy-v0',\n    entry_point='gym.envs.algorithmic:RepeatCopyEnv',\n    max_episode_steps=200,\n    reward_threshold=75.0,\n)\n\nregister(\n    id='ReversedAddition-v0',\n    entry_point='gym.envs.algorithmic:ReversedAdditionEnv',\n    kwargs={'rows' : 2},\n    max_episode_steps=200,\n    reward_threshold=25.0,\n)\n\nregister(\n    id='ReversedAddition3-v0',\n    entry_point='gym.envs.algorithmic:ReversedAdditionEnv',\n    kwargs={'rows' : 3},\n    max_episode_steps=200,\n    reward_threshold=25.0,\n)\n\nregister(\n    id='DuplicatedInput-v0',\n    entry_point='gym.envs.algorithmic:DuplicatedInputEnv',\n    max_episode_steps=200,\n    reward_threshold=9.0,\n)\n\nregister(\n    id='Reverse-v0',\n    entry_point='gym.envs.algorithmic:ReverseEnv',\n    max_episode_steps=200,\n    reward_threshold=25.0,\n)\n\n# Classic\n# ----------------------------------------\n\nregister(\n    id='CartPole-v0',\n    entry_point='gym.envs.classic_control:CartPoleEnv',\n    max_episode_steps=200,\n    reward_threshold=195.0,\n)\n\nregister(\n    id='CartPole-v1',\n    entry_point='gym.envs.classic_control:CartPoleEnv',\n    max_episode_steps=500,\n    reward_threshold=475.0,\n)\n\nregister(\n    id='MountainCar-v0',\n    entry_point='gym.envs.classic_control:MountainCarEnv',\n    max_episode_steps=200,\n    reward_threshold=-110.0,\n)\n\nregister(\n    id='MountainCarContinuous-v0',\n    entry_point='gym.envs.classic_control:Continuous_MountainCarEnv',\n    max_episode_steps=999,\n    reward_threshold=90.0,\n)\n\nregister(\n    id='Pendulum-v0',\n    entry_point='gym.envs.classic_control:PendulumEnv',\n    max_episode_steps=200,\n)\n\nregister(\n    id='Acrobot-v1',\n    entry_point='gym.envs.classic_control:AcrobotEnv',\n    reward_threshold=-100.0,\n    max_episode_steps=500,\n)\n\n# Box2d\n# ----------------------------------------\n\nregister(\n    id='LunarLander-v2',\n    entry_point='gym.envs.box2d:LunarLander',\n    max_episode_steps=1000,\n    reward_threshold=200,\n)\n\nregister(\n    id='LunarLanderContinuous-v2',\n    entry_point='gym.envs.box2d:LunarLanderContinuous',\n    max_episode_steps=1000,\n    reward_threshold=200,\n)\n\nregister(\n    id='BipedalWalker-v3',\n    entry_point='gym.envs.box2d:BipedalWalker',\n    max_episode_steps=1600,\n    reward_threshold=300,\n)\n\nregister(\n    id='BipedalWalkerHardcore-v3',\n    entry_point='gym.envs.box2d:BipedalWalkerHardcore',\n    max_episode_steps=2000,\n    reward_threshold=300,\n)\n\nregister(\n    id='CarRacing-v0',\n    entry_point='gym.envs.box2d:CarRacing',\n    max_episode_steps=1000,\n    reward_threshold=900,\n)\n\n# Toy Text\n# ----------------------------------------\n\nregister(\n    id='Blackjack-v0',\n    entry_point='gym.envs.toy_text:BlackjackEnv',\n)\n\nregister(\n    id='KellyCoinflip-v0',\n    entry_point='gym.envs.toy_text:KellyCoinflipEnv',\n    reward_threshold=246.61,\n)\nregister(\n    id='KellyCoinflipGeneralized-v0',\n    entry_point='gym.envs.toy_text:KellyCoinflipGeneralizedEnv',\n)\n\nregister(\n    id='FrozenLake-v0',\n    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n    kwargs={'map_name' : '4x4'},\n    max_episode_steps=100,\n    reward_threshold=0.78, # optimum = .8196\n)\n\nregister(\n    id='FrozenLake8x8-v0',\n    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n    kwargs={'map_name' : '8x8'},\n    max_episode_steps=200,\n    reward_threshold=0.99, # optimum = 1\n)\n\nregister(\n    id='CliffWalking-v0',\n    entry_point='gym.envs.toy_text:CliffWalkingEnv',\n)\n\nregister(\n    id='NChain-v0',\n    entry_point='gym.envs.toy_text:NChainEnv',\n    max_episode_steps=1000,\n)\n\nregister(\n    id='Roulette-v0',\n    entry_point='gym.envs.toy_text:RouletteEnv',\n    max_episode_steps=100,\n)\n\nregister(\n    id='Taxi-v3',\n    entry_point='gym.envs.toy_text:TaxiEnv',\n    reward_threshold=8, # optimum = 8.46\n    max_episode_steps=200,\n)\n\nregister(\n    id='GuessingGame-v0',\n    entry_point='gym.envs.toy_text:GuessingGame',\n    max_episode_steps=200,\n)\n\nregister(\n    id='HotterColder-v0',\n    entry_point='gym.envs.toy_text:HotterColder',\n    max_episode_steps=200,\n)\n\n# Mujoco\n# ----------------------------------------\n\n# 2D\n\nregister(\n    id='Reacher-v2',\n    entry_point='gym.envs.mujoco:ReacherEnv',\n    max_episode_steps=50,\n    reward_threshold=-3.75,\n)\n\nregister(\n    id='Pusher-v2',\n    entry_point='gym.envs.mujoco:PusherEnv',\n    max_episode_steps=100,\n    reward_threshold=0.0,\n)\n\nregister(\n    id='Thrower-v2',\n    entry_point='gym.envs.mujoco:ThrowerEnv',\n    max_episode_steps=100,\n    reward_threshold=0.0,\n)\n\nregister(\n    id='Striker-v2',\n    entry_point='gym.envs.mujoco:StrikerEnv',\n    max_episode_steps=100,\n    reward_threshold=0.0,\n)\n\nregister(\n    id='InvertedPendulum-v2',\n    entry_point='gym.envs.mujoco:InvertedPendulumEnv',\n    max_episode_steps=1000,\n    reward_threshold=950.0,\n)\n\nregister(\n    id='InvertedDoublePendulum-v2',\n    entry_point='gym.envs.mujoco:InvertedDoublePendulumEnv',\n    max_episode_steps=1000,\n    reward_threshold=9100.0,\n)\n\nregister(\n    id='HalfCheetah-v2',\n    entry_point='gym.envs.mujoco:HalfCheetahEnv',\n    max_episode_steps=1000,\n    reward_threshold=4800.0,\n)\n\nregister(\n    id='HalfCheetah-v3',\n    entry_point='gym.envs.mujoco.half_cheetah_v3:HalfCheetahEnv',\n    max_episode_steps=1000,\n    reward_threshold=4800.0,\n)\n\nregister(\n    id='Hopper-v2',\n    entry_point='gym.envs.mujoco:HopperEnv',\n    max_episode_steps=1000,\n    reward_threshold=3800.0,\n)\n\nregister(\n    id='Hopper-v3',\n    entry_point='gym.envs.mujoco.hopper_v3:HopperEnv',\n    max_episode_steps=1000,\n    reward_threshold=3800.0,\n)\n\nregister(\n    id='Swimmer-v2',\n    entry_point='gym.envs.mujoco:SwimmerEnv',\n    max_episode_steps=1000,\n    reward_threshold=360.0,\n)\n\nregister(\n    id='Swimmer-v3',\n    entry_point='gym.envs.mujoco.swimmer_v3:SwimmerEnv',\n    max_episode_steps=1000,\n    reward_threshold=360.0,\n)\n\nregister(\n    id='Walker2d-v2',\n    max_episode_steps=1000,\n    entry_point='gym.envs.mujoco:Walker2dEnv',\n)\n\nregister(\n    id='Walker2d-v3',\n    max_episode_steps=1000,\n    entry_point='gym.envs.mujoco.walker2d_v3:Walker2dEnv',\n)\n\nregister(\n    id='Ant-v2',\n    entry_point='gym.envs.mujoco:AntEnv',\n    max_episode_steps=1000,\n    reward_threshold=6000.0,\n)\n\nregister(\n    id='Ant-v3',\n    entry_point='gym.envs.mujoco.ant_v3:AntEnv',\n    max_episode_steps=1000,\n    reward_threshold=6000.0,\n)\n\nregister(\n    id='Humanoid-v2',\n    entry_point='gym.envs.mujoco:HumanoidEnv',\n    max_episode_steps=1000,\n)\n\nregister(\n    id='Humanoid-v3',\n    entry_point='gym.envs.mujoco.humanoid_v3:HumanoidEnv',\n    max_episode_steps=1000,\n)\n\nregister(\n    id='HumanoidStandup-v2',\n    entry_point='gym.envs.mujoco:HumanoidStandupEnv',\n    max_episode_steps=1000,\n)\n\n# Robotics\n# ----------------------------------------\n\ndef _merge(a, b):\n    a.update(b)\n    return a\n\nfor reward_type in ['sparse', 'dense']:\n    suffix = 'Dense' if reward_type == 'dense' else ''\n    kwargs = {\n        'reward_type': reward_type,\n    }\n\n    # Fetch\n    register(\n        id='FetchSlide{}-v1'.format(suffix),\n        entry_point='gym.envs.robotics:FetchSlideEnv',\n        kwargs=kwargs,\n        max_episode_steps=50,\n    )\n\n    register(\n        id='FetchPickAndPlace{}-v1'.format(suffix),\n        entry_point='gym.envs.robotics:FetchPickAndPlaceEnv',\n        kwargs=kwargs,\n        max_episode_steps=50,\n    )\n\n    register(\n        id='FetchReach{}-v1'.format(suffix),\n        entry_point='gym.envs.robotics:FetchReachEnv',\n        kwargs=kwargs,\n        max_episode_steps=50,\n    )\n\n    register(\n        id='FetchPush{}-v1'.format(suffix),\n        entry_point='gym.envs.robotics:FetchPushEnv',\n        kwargs=kwargs,\n        max_episode_steps=50,\n    )\n\n    # Hand\n    register(\n        id='HandReach{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandReachEnv',\n        kwargs=kwargs,\n        max_episode_steps=50,\n    )\n\n    register(\n        id='HandManipulateBlockRotateZ{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandBlockEnv',\n        kwargs=_merge({'target_position': 'ignore', 'target_rotation': 'z'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulateBlockRotateZTouchSensors{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandBlockTouchSensorsEnv',\n        kwargs=_merge({'target_position': 'ignore', 'target_rotation': 'z', 'touch_get_obs': 'boolean'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulateBlockRotateZTouchSensors{}-v1'.format(suffix),\n        entry_point='gym.envs.robotics:HandBlockTouchSensorsEnv',\n        kwargs=_merge({'target_position': 'ignore', 'target_rotation': 'z', 'touch_get_obs': 'sensordata'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulateBlockRotateParallel{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandBlockEnv',\n        kwargs=_merge({'target_position': 'ignore', 'target_rotation': 'parallel'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulateBlockRotateParallelTouchSensors{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandBlockTouchSensorsEnv',\n        kwargs=_merge({'target_position': 'ignore', 'target_rotation': 'parallel', 'touch_get_obs': 'boolean'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulateBlockRotateParallelTouchSensors{}-v1'.format(suffix),\n        entry_point='gym.envs.robotics:HandBlockTouchSensorsEnv',\n        kwargs=_merge({'target_position': 'ignore', 'target_rotation': 'parallel', 'touch_get_obs': 'sensordata'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulateBlockRotateXYZ{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandBlockEnv',\n        kwargs=_merge({'target_position': 'ignore', 'target_rotation': 'xyz'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulateBlockRotateXYZTouchSensors{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandBlockTouchSensorsEnv',\n        kwargs=_merge({'target_position': 'ignore', 'target_rotation': 'xyz', 'touch_get_obs': 'boolean'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulateBlockRotateXYZTouchSensors{}-v1'.format(suffix),\n        entry_point='gym.envs.robotics:HandBlockTouchSensorsEnv',\n        kwargs=_merge({'target_position': 'ignore', 'target_rotation': 'xyz', 'touch_get_obs': 'sensordata'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulateBlockFull{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandBlockEnv',\n        kwargs=_merge({'target_position': 'random', 'target_rotation': 'xyz'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    # Alias for \"Full\"\n    register(\n        id='HandManipulateBlock{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandBlockEnv',\n        kwargs=_merge({'target_position': 'random', 'target_rotation': 'xyz'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulateBlockTouchSensors{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandBlockTouchSensorsEnv',\n        kwargs=_merge({'target_position': 'random', 'target_rotation': 'xyz', 'touch_get_obs': 'boolean'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulateBlockTouchSensors{}-v1'.format(suffix),\n        entry_point='gym.envs.robotics:HandBlockTouchSensorsEnv',\n        kwargs=_merge({'target_position': 'random', 'target_rotation': 'xyz', 'touch_get_obs': 'sensordata'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulateEggRotate{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandEggEnv',\n        kwargs=_merge({'target_position': 'ignore', 'target_rotation': 'xyz'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulateEggRotateTouchSensors{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandEggTouchSensorsEnv',\n        kwargs=_merge({'target_position': 'ignore', 'target_rotation': 'xyz', 'touch_get_obs': 'boolean'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulateEggRotateTouchSensors{}-v1'.format(suffix),\n        entry_point='gym.envs.robotics:HandEggTouchSensorsEnv',\n        kwargs=_merge({'target_position': 'ignore', 'target_rotation': 'xyz', 'touch_get_obs': 'sensordata'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulateEggFull{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandEggEnv',\n        kwargs=_merge({'target_position': 'random', 'target_rotation': 'xyz'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    # Alias for \"Full\"\n    register(\n        id='HandManipulateEgg{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandEggEnv',\n        kwargs=_merge({'target_position': 'random', 'target_rotation': 'xyz'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulateEggTouchSensors{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandEggTouchSensorsEnv',\n        kwargs=_merge({'target_position': 'random', 'target_rotation': 'xyz', 'touch_get_obs': 'boolean'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulateEggTouchSensors{}-v1'.format(suffix),\n        entry_point='gym.envs.robotics:HandEggTouchSensorsEnv',\n        kwargs=_merge({'target_position': 'random', 'target_rotation': 'xyz', 'touch_get_obs': 'sensordata'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulatePenRotate{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandPenEnv',\n        kwargs=_merge({'target_position': 'ignore', 'target_rotation': 'xyz'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulatePenRotateTouchSensors{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandPenTouchSensorsEnv',\n        kwargs=_merge({'target_position': 'ignore', 'target_rotation': 'xyz', 'touch_get_obs': 'boolean'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulatePenRotateTouchSensors{}-v1'.format(suffix),\n        entry_point='gym.envs.robotics:HandPenTouchSensorsEnv',\n        kwargs=_merge({'target_position': 'ignore', 'target_rotation': 'xyz', 'touch_get_obs': 'sensordata'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulatePenFull{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandPenEnv',\n        kwargs=_merge({'target_position': 'random', 'target_rotation': 'xyz'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    # Alias for \"Full\"\n    register(\n        id='HandManipulatePen{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandPenEnv',\n        kwargs=_merge({'target_position': 'random', 'target_rotation': 'xyz'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulatePenTouchSensors{}-v0'.format(suffix),\n        entry_point='gym.envs.robotics:HandPenTouchSensorsEnv',\n        kwargs=_merge({'target_position': 'random', 'target_rotation': 'xyz', 'touch_get_obs': 'boolean'}, kwargs),\n        max_episode_steps=100,\n    )\n\n    register(\n        id='HandManipulatePenTouchSensors{}-v1'.format(suffix),\n        entry_point='gym.envs.robotics:HandPenTouchSensorsEnv',\n        kwargs=_merge({'target_position': 'random', 'target_rotation': 'xyz', 'touch_get_obs': 'sensordata'}, kwargs),\n        max_episode_steps=100,\n    )\n\n# Atari\n# ----------------------------------------\n\n# # print ', '.join([\"'{}'\".format(name.split('.')[0]) for name in atari_py.list_games()])\nfor game in ['adventure', 'air_raid', 'alien', 'amidar', 'assault', 'asterix', 'asteroids', 'atlantis',\n    'bank_heist', 'battle_zone', 'beam_rider', 'berzerk', 'bowling', 'boxing', 'breakout', 'carnival',\n    'centipede', 'chopper_command', 'crazy_climber', 'defender', 'demon_attack', 'double_dunk',\n    'elevator_action', 'enduro', 'fishing_derby', 'freeway', 'frostbite', 'gopher', 'gravitar',\n    'hero', 'ice_hockey', 'jamesbond', 'journey_escape', 'kangaroo', 'krull', 'kung_fu_master',\n    'montezuma_revenge', 'ms_pacman', 'name_this_game', 'phoenix', 'pitfall', 'pong', 'pooyan',\n    'private_eye', 'qbert', 'riverraid', 'road_runner', 'robotank', 'seaquest', 'skiing',\n    'solaris', 'space_invaders', 'star_gunner', 'tennis', 'time_pilot', 'tutankham', 'up_n_down',\n    'venture', 'video_pinball', 'wizard_of_wor', 'yars_revenge', 'zaxxon']:\n    for obs_type in ['image', 'ram']:\n        # space_invaders should yield SpaceInvaders-v0 and SpaceInvaders-ram-v0\n        name = ''.join([g.capitalize() for g in game.split('_')])\n        if obs_type == 'ram':\n            name = '{}-ram'.format(name)\n\n        nondeterministic = False\n        if game == 'elevator_action' and obs_type == 'ram':\n            # ElevatorAction-ram-v0 seems to yield slightly\n            # non-deterministic observations about 10% of the time. We\n            # should track this down eventually, but for now we just\n            # mark it as nondeterministic.\n            nondeterministic = True\n\n        register(\n            id='{}-v0'.format(name),\n            entry_point='gym.envs.atari:AtariEnv',\n            kwargs={'game': game, 'obs_type': obs_type, 'repeat_action_probability': 0.25},\n            max_episode_steps=10000,\n            nondeterministic=nondeterministic,\n        )\n\n        register(\n            id='{}-v4'.format(name),\n            entry_point='gym.envs.atari:AtariEnv',\n            kwargs={'game': game, 'obs_type': obs_type},\n            max_episode_steps=100000,\n            nondeterministic=nondeterministic,\n        )\n\n        # Standard Deterministic (as in the original DeepMind paper)\n        if game == 'space_invaders':\n            frameskip = 3\n        else:\n            frameskip = 4\n\n        # Use a deterministic frame skip.\n        register(\n            id='{}Deterministic-v0'.format(name),\n            entry_point='gym.envs.atari:AtariEnv',\n            kwargs={'game': game, 'obs_type': obs_type, 'frameskip': frameskip, 'repeat_action_probability': 0.25},\n            max_episode_steps=100000,\n            nondeterministic=nondeterministic,\n        )\n\n        register(\n            id='{}Deterministic-v4'.format(name),\n            entry_point='gym.envs.atari:AtariEnv',\n            kwargs={'game': game, 'obs_type': obs_type, 'frameskip': frameskip},\n            max_episode_steps=100000,\n            nondeterministic=nondeterministic,\n        )\n\n        register(\n            id='{}NoFrameskip-v0'.format(name),\n            entry_point='gym.envs.atari:AtariEnv',\n            kwargs={'game': game, 'obs_type': obs_type, 'frameskip': 1, 'repeat_action_probability': 0.25}, # A frameskip of 1 means we get every frame\n            max_episode_steps=frameskip * 100000,\n            nondeterministic=nondeterministic,\n        )\n\n        # No frameskip. (Atari has no entropy source, so these are\n        # deterministic environments.)\n        register(\n            id='{}NoFrameskip-v4'.format(name),\n            entry_point='gym.envs.atari:AtariEnv',\n            kwargs={'game': game, 'obs_type': obs_type, 'frameskip': 1}, # A frameskip of 1 means we get every frame\n            max_episode_steps=frameskip * 100000,\n            nondeterministic=nondeterministic,\n        )\n\n\n# Unit test\n# ---------\n\nregister(\n    id='CubeCrash-v0',\n    entry_point='gym.envs.unittest:CubeCrash',\n    reward_threshold=0.9,\n    )\nregister(\n    id='CubeCrashSparse-v0',\n    entry_point='gym.envs.unittest:CubeCrashSparse',\n    reward_threshold=0.9,\n    )\nregister(\n    id='CubeCrashScreenBecomesBlack-v0',\n    entry_point='gym.envs.unittest:CubeCrashScreenBecomesBlack',\n    reward_threshold=0.9,\n    )\n\nregister(\n    id='MemorizeDigits-v0',\n    entry_point='gym.envs.unittest:MemorizeDigits',\n    reward_threshold=20,\n    )",
        "# -*- coding: utf-8 -*-\n\"\"\"\n@author: Olivier Sigaud\n\nA merge between two sources:\n\n* Adaptation of the MountainCar Environment from the \"FAReinforcement\" library\nof Jose Antonio Martin H. (version 1.0), adapted by  'Tom Schaul, tom@idsia.ch'\nand then modified by Arnaud de Broissia\n\n* the OpenAI/gym MountainCar environment\nitself from\nhttp://incompleteideas.net/sutton/MountainCar/MountainCar1.cp\npermalink: https://perma.cc/6Z2N-PFWC\n\"\"\"\n\nimport math\n\nimport numpy as np\n\nimport gym\nfrom gym import spaces\nfrom gym.utils import seeding\n\n\nclass Continuous_MountainCarEnv(gym.Env):\n    metadata = {\n        'render.modes': ['human', 'rgb_array'],\n        'video.frames_per_second': 30\n    }\n\n    def __init__(self, goal_velocity=0):\n        self.min_action = -1.0\n        self.max_action = 1.0\n        self.min_position = -1.2\n        self.max_position = 0.6\n        self.max_speed = 0.07\n        self.goal_position = 0.45 # was 0.5 in gym, 0.45 in Arnaud de Broissia's version\n        self.goal_velocity = goal_velocity\n        self.power = 0.0015\n\n        self.low_state = np.array(\n            [self.min_position, -self.max_speed], dtype=np.float32\n        )\n        self.high_state = np.array(\n            [self.max_position, self.max_speed], dtype=np.float32\n        )\n\n        self.viewer = None\n\n        self.action_space = spaces.Box(\n            low=self.min_action,\n            high=self.max_action,\n            shape=(1,),\n            dtype=np.float32\n        )\n        self.observation_space = spaces.Box(\n            low=self.low_state,\n            high=self.high_state,\n            dtype=np.float32\n        )\n\n        self.seed()\n        self.reset()\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def step(self, action):\n\n        position = self.state[0]\n        velocity = self.state[1]\n        force = min(max(action[0], self.min_action), self.max_action)\n\n        velocity += force * self.power - 0.0025 * math.cos(3 * position)\n        if (velocity > self.max_speed): velocity = self.max_speed\n        if (velocity < -self.max_speed): velocity = -self.max_speed\n        position += velocity\n        if (position > self.max_position): position = self.max_position\n        if (position < self.min_position): position = self.min_position\n        if (position == self.min_position and velocity < 0): velocity = 0\n\n        # Convert a possible numpy bool to a Python bool.\n        done = bool(\n            position >= self.goal_position and velocity >= self.goal_velocity\n        )\n\n        reward = 0\n        if done:\n            reward = 100.0\n        reward -= math.pow(action[0], 2) * 0.1\n\n        self.state = np.array([position, velocity])\n        return self.state, reward, done, {}\n\n    def reset(self):\n        self.state = np.array([self.np_random.uniform(low=-0.6, high=-0.4), 0])\n        return np.array(self.state)\n\n    def _height(self, xs):\n        return np.sin(3 * xs)*.45+.55\n\n    def render(self, mode='human'):\n        screen_width = 600\n        screen_height = 400\n\n        world_width = self.max_position - self.min_position\n        scale = screen_width/world_width\n        carwidth = 40\n        carheight = 20\n\n        if self.viewer is None:\n            from gym.envs.classic_control import rendering\n            self.viewer = rendering.Viewer(screen_width, screen_height)\n            xs = np.linspace(self.min_position, self.max_position, 100)\n            ys = self._height(xs)\n            xys = list(zip((xs-self.min_position)*scale, ys*scale))\n\n            self.track = rendering.make_polyline(xys)\n            self.track.set_linewidth(4)\n            self.viewer.add_geom(self.track)\n\n            clearance = 10\n\n            l, r, t, b = -carwidth / 2, carwidth / 2, carheight, 0\n            car = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n            car.add_attr(rendering.Transform(translation=(0, clearance)))\n            self.cartrans = rendering.Transform()\n            car.add_attr(self.cartrans)\n            self.viewer.add_geom(car)\n            frontwheel = rendering.make_circle(carheight / 2.5)\n            frontwheel.set_color(.5, .5, .5)\n            frontwheel.add_attr(\n                rendering.Transform(translation=(carwidth / 4, clearance))\n            )\n            frontwheel.add_attr(self.cartrans)\n            self.viewer.add_geom(frontwheel)\n            backwheel = rendering.make_circle(carheight / 2.5)\n            backwheel.add_attr(\n                rendering.Transform(translation=(-carwidth / 4, clearance))\n            )\n            backwheel.add_attr(self.cartrans)\n            backwheel.set_color(.5, .5, .5)\n            self.viewer.add_geom(backwheel)\n            flagx = (self.goal_position-self.min_position)*scale\n            flagy1 = self._height(self.goal_position)*scale\n            flagy2 = flagy1 + 50\n            flagpole = rendering.Line((flagx, flagy1), (flagx, flagy2))\n            self.viewer.add_geom(flagpole)\n            flag = rendering.FilledPolygon(\n                [(flagx, flagy2), (flagx, flagy2 - 10), (flagx + 25, flagy2 - 5)]\n            )\n            flag.set_color(.8, .8, 0)\n            self.viewer.add_geom(flag)\n\n        pos = self.state[0]\n        self.cartrans.set_translation(\n            (pos-self.min_position) * scale, self._height(pos) * scale\n        )\n        self.cartrans.set_rotation(math.cos(3 * pos))\n\n        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n\n    def close(self):\n        if self.viewer:\n            self.viewer.close()\n            self.viewer = None",
        "\"\"\"\n2D rendering framework\n\"\"\"\nimport os\nimport sys\n\nif \"Apple\" in sys.version:\n    if 'DYLD_FALLBACK_LIBRARY_PATH' in os.environ:\n        os.environ['DYLD_FALLBACK_LIBRARY_PATH'] += ':/usr/lib'\n        # (JDS 2016/04/15): avoid bug on Anaconda 2.3.0 / Yosemite\n\nfrom gym import error\n\ntry:\n    import pyglet\nexcept ImportError as e:\n    raise ImportError('''\n    Cannot import pyglet.\n    HINT: you can install pyglet directly via 'pip install pyglet'.\n    But if you really just want to install all Gym dependencies and not have to think about it,\n    'pip install -e .[all]' or 'pip install gym[all]' will do it.\n    ''')\n\ntry:\n    from pyglet.gl import *\nexcept ImportError as e:\n    raise ImportError('''\n    Error occurred while running `from pyglet.gl import *`\n    HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'.\n    If you're running on a server, you may need a virtual frame buffer; something like this should work:\n    'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\n    ''')\n\nimport math\nimport numpy as np\n\nRAD2DEG = 57.29577951308232\n\ndef get_display(spec):\n    \"\"\"Convert a display specification (such as :0) into an actual Display\n    object.\n\n    Pyglet only supports multiple Displays on Linux.\n    \"\"\"\n    if spec is None:\n        return pyglet.canvas.get_display()\n        # returns already available pyglet_display,\n        # if there is no pyglet display available then it creates one\n    elif isinstance(spec, str):\n        return pyglet.canvas.Display(spec)\n    else:\n        raise error.Error('Invalid display specification: {}. (Must be a string like :0 or None.)'.format(spec))\n\ndef get_window(width, height, display):\n    \"\"\"\n    Will create a pyglet window from the display specification provided.\n    \"\"\"\n    screen = display.get_screens() #available screens\n    config = screen[0].get_best_config() #selecting the first screen\n    context = config.create_context(None) #create GL context\n\n    return pyglet.window.Window(width=width, height=height, display=display, config=config, context=context)\n\nclass Viewer(object):\n    def __init__(self, width, height, display=None):\n        display = get_display(display)\n\n        self.width = width\n        self.height = height\n        self.window = get_window(width=width, height=height, display=display)\n        self.window.on_close = self.window_closed_by_user\n        self.isopen = True\n        self.geoms = []\n        self.onetime_geoms = []\n        self.transform = Transform()\n\n        glEnable(GL_BLEND)\n        glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA)\n\n    def close(self):\n        self.window.close()\n\n    def window_closed_by_user(self):\n        self.isopen = False\n\n    def set_bounds(self, left, right, bottom, top):\n        assert right > left and top > bottom\n        scalex = self.width/(right-left)\n        scaley = self.height/(top-bottom)\n        self.transform = Transform(\n            translation=(-left*scalex, -bottom*scaley),\n            scale=(scalex, scaley))\n\n    def add_geom(self, geom):\n        self.geoms.append(geom)\n\n    def add_onetime(self, geom):\n        self.onetime_geoms.append(geom)\n\n    def render(self, return_rgb_array=False):\n        glClearColor(1,1,1,1)\n        self.window.clear()\n        self.window.switch_to()\n        self.window.dispatch_events()\n        self.transform.enable()\n        for geom in self.geoms:\n            geom.render()\n        for geom in self.onetime_geoms:\n            geom.render()\n        self.transform.disable()\n        arr = None\n        if return_rgb_array:\n            buffer = pyglet.image.get_buffer_manager().get_color_buffer()\n            image_data = buffer.get_image_data()\n            arr = np.frombuffer(image_data.get_data(), dtype=np.uint8)\n            # In https://github.com/openai/gym-http-api/issues/2, we\n            # discovered that someone using Xmonad on Arch was having\n            # a window of size 598 x 398, though a 600 x 400 window\n            # was requested. (Guess Xmonad was preserving a pixel for\n            # the boundary.) So we use the buffer height/width rather\n            # than the requested one.\n            arr = arr.reshape(buffer.height, buffer.width, 4)\n            arr = arr[::-1,:,0:3]\n        self.window.flip()\n        self.onetime_geoms = []\n        return arr if return_rgb_array else self.isopen\n\n    # Convenience\n    def draw_circle(self, radius=10, res=30, filled=True, **attrs):\n        geom = make_circle(radius=radius, res=res, filled=filled)\n        _add_attrs(geom, attrs)\n        self.add_onetime(geom)\n        return geom\n\n    def draw_polygon(self, v, filled=True, **attrs):\n        geom = make_polygon(v=v, filled=filled)\n        _add_attrs(geom, attrs)\n        self.add_onetime(geom)\n        return geom\n\n    def draw_polyline(self, v, **attrs):\n        geom = make_polyline(v=v)\n        _add_attrs(geom, attrs)\n        self.add_onetime(geom)\n        return geom\n\n    def draw_line(self, start, end, **attrs):\n        geom = Line(start, end)\n        _add_attrs(geom, attrs)\n        self.add_onetime(geom)\n        return geom\n\n    def get_array(self):\n        self.window.flip()\n        image_data = pyglet.image.get_buffer_manager().get_color_buffer().get_image_data()\n        self.window.flip()\n        arr = np.fromstring(image_data.get_data(), dtype=np.uint8, sep='')\n        arr = arr.reshape(self.height, self.width, 4)\n        return arr[::-1,:,0:3]\n\n    def __del__(self):\n        self.close()\n\ndef _add_attrs(geom, attrs):\n    if \"color\" in attrs:\n        geom.set_color(*attrs[\"color\"])\n    if \"linewidth\" in attrs:\n        geom.set_linewidth(attrs[\"linewidth\"])\n\nclass Geom(object):\n    def __init__(self):\n        self._color=Color((0, 0, 0, 1.0))\n        self.attrs = [self._color]\n    def render(self):\n        for attr in reversed(self.attrs):\n            attr.enable()\n        self.render1()\n        for attr in self.attrs:\n            attr.disable()\n    def render1(self):\n        raise NotImplementedError\n    def add_attr(self, attr):\n        self.attrs.append(attr)\n    def set_color(self, r, g, b):\n        self._color.vec4 = (r, g, b, 1)\n\nclass Attr(object):\n    def enable(self):\n        raise NotImplementedError\n    def disable(self):\n        pass\n\nclass Transform(Attr):\n    def __init__(self, translation=(0.0, 0.0), rotation=0.0, scale=(1,1)):\n        self.set_translation(*translation)\n        self.set_rotation(rotation)\n        self.set_scale(*scale)\n    def enable(self):\n        glPushMatrix()\n        glTranslatef(self.translation[0], self.translation[1], 0) # translate to GL loc ppint\n        glRotatef(RAD2DEG * self.rotation, 0, 0, 1.0)\n        glScalef(self.scale[0], self.scale[1], 1)\n    def disable(self):\n        glPopMatrix()\n    def set_translation(self, newx, newy):\n        self.translation = (float(newx), float(newy))\n    def set_rotation(self, new):\n        self.rotation = float(new)\n    def set_scale(self, newx, newy):\n        self.scale = (float(newx), float(newy))\n\nclass Color(Attr):\n    def __init__(self, vec4):\n        self.vec4 = vec4\n    def enable(self):\n        glColor4f(*self.vec4)\n\nclass LineStyle(Attr):\n    def __init__(self, style):\n        self.style = style\n    def enable(self):\n        glEnable(GL_LINE_STIPPLE)\n        glLineStipple(1, self.style)\n    def disable(self):\n        glDisable(GL_LINE_STIPPLE)\n\nclass LineWidth(Attr):\n    def __init__(self, stroke):\n        self.stroke = stroke\n    def enable(self):\n        glLineWidth(self.stroke)\n\nclass Point(Geom):\n    def __init__(self):\n        Geom.__init__(self)\n    def render1(self):\n        glBegin(GL_POINTS) # draw point\n        glVertex3f(0.0, 0.0, 0.0)\n        glEnd()\n\nclass FilledPolygon(Geom):\n    def __init__(self, v):\n        Geom.__init__(self)\n        self.v = v\n    def render1(self):\n        if   len(self.v) == 4 : glBegin(GL_QUADS)\n        elif len(self.v)  > 4 : glBegin(GL_POLYGON)\n        else: glBegin(GL_TRIANGLES)\n        for p in self.v:\n            glVertex3f(p[0], p[1],0)  # draw each vertex\n        glEnd()\n\ndef make_circle(radius=10, res=30, filled=True):\n    points = []\n    for i in range(res):\n        ang = 2*math.pi*i / res\n        points.append((math.cos(ang)*radius, math.sin(ang)*radius))\n    if filled:\n        return FilledPolygon(points)\n    else:\n        return PolyLine(points, True)\n\ndef make_polygon(v, filled=True):\n    if filled: return FilledPolygon(v)\n    else: return PolyLine(v, True)\n\ndef make_polyline(v):\n    return PolyLine(v, False)\n\ndef make_capsule(length, width):\n    l, r, t, b = 0, length, width/2, -width/2\n    box = make_polygon([(l,b), (l,t), (r,t), (r,b)])\n    circ0 = make_circle(width/2)\n    circ1 = make_circle(width/2)\n    circ1.add_attr(Transform(translation=(length, 0)))\n    geom = Compound([box, circ0, circ1])\n    return geom\n\nclass Compound(Geom):\n    def __init__(self, gs):\n        Geom.__init__(self)\n        self.gs = gs\n        for g in self.gs:\n            g.attrs = [a for a in g.attrs if not isinstance(a, Color)]\n    def render1(self):\n        for g in self.gs:\n            g.render()\n\nclass PolyLine(Geom):\n    def __init__(self, v, close):\n        Geom.__init__(self)\n        self.v = v\n        self.close = close\n        self.linewidth = LineWidth(1)\n        self.add_attr(self.linewidth)\n    def render1(self):\n        glBegin(GL_LINE_LOOP if self.close else GL_LINE_STRIP)\n        for p in self.v:\n            glVertex3f(p[0], p[1],0)  # draw each vertex\n        glEnd()\n    def set_linewidth(self, x):\n        self.linewidth.stroke = x\n\nclass Line(Geom):\n    def __init__(self, start=(0.0, 0.0), end=(0.0, 0.0)):\n        Geom.__init__(self)\n        self.start = start\n        self.end = end\n        self.linewidth = LineWidth(1)\n        self.add_attr(self.linewidth)\n\n    def render1(self):\n        glBegin(GL_LINES)\n        glVertex2f(*self.start)\n        glVertex2f(*self.end)\n        glEnd()\n\nclass Image(Geom):\n    def __init__(self, fname, width, height):\n        Geom.__init__(self)\n        self.width = width\n        self.height = height\n        img = pyglet.image.load(fname)\n        self.img = img\n        self.flip = False\n    def render1(self):\n        self.img.blit(-self.width/2, -self.height/2, width=self.width, height=self.height)\n\n# ================================================================\n\nclass SimpleImageViewer(object):\n    def __init__(self, display=None, maxwidth=500):\n        self.window = None\n        self.isopen = False\n        self.display = display\n        self.maxwidth = maxwidth\n    def imshow(self, arr):\n        if self.window is None:\n            height, width, _channels = arr.shape\n            if width > self.maxwidth:\n                scale = self.maxwidth / width\n                width = int(scale * width)\n                height = int(scale * height)\n            self.window = pyglet.window.Window(width=width, height=height,\n                display=self.display, vsync=False, resizable=True)\n            self.width = width\n            self.height = height\n            self.isopen = True\n\n            @self.window.event\n            def on_resize(width, height):\n                self.width = width\n                self.height = height\n\n            @self.window.event\n            def on_close():\n                self.isopen = False\n\n        assert len(arr.shape) == 3, \"You passed in an image with the wrong number shape\"\n        image = pyglet.image.ImageData(arr.shape[1], arr.shape[0],\n            'RGB', arr.tobytes(), pitch=arr.shape[1]*-3)\n        gl.glTexParameteri(gl.GL_TEXTURE_2D,\n            gl.GL_TEXTURE_MAG_FILTER, gl.GL_NEAREST)\n        texture = image.get_texture()\n        texture.width = self.width\n        texture.height = self.height\n        self.window.clear()\n        self.window.switch_to()\n        self.window.dispatch_events()\n        texture.blit(0, 0) # draw\n        self.window.flip()\n    def close(self):\n        if self.isopen and sys.meta_path:\n            # ^^^ check sys.meta_path to avoid 'ImportError: sys.meta_path is None, Python is likely shutting down'\n            self.window.close()\n            self.isopen = False\n\n    def __del__(self):\n        self.close()",
        "from gym.envs.classic_control.cartpole import CartPoleEnv\nfrom gym.envs.classic_control.mountain_car import MountainCarEnv\nfrom gym.envs.classic_control.continuous_mountain_car import Continuous_MountainCarEnv\nfrom gym.envs.classic_control.pendulum import PendulumEnv\nfrom gym.envs.classic_control.acrobot import AcrobotEnv\n",
        "\"\"\"\nClassic cart-pole system implemented by Rich Sutton et al.\nCopied from http://incompleteideas.net/sutton/book/code/pole.c\npermalink: https://perma.cc/C9ZM-652R\n\"\"\"\n\nimport math\nimport gym\nfrom gym import spaces, logger\nfrom gym.utils import seeding\nimport numpy as np\n\n\nclass CartPoleEnv(gym.Env):\n    \"\"\"\n    Description:\n        A pole is attached by an un-actuated joint to a cart, which moves along\n        a frictionless track. The pendulum starts upright, and the goal is to\n        prevent it from falling over by increasing and reducing the cart's\n        velocity.\n\n    Source:\n        This environment corresponds to the version of the cart-pole problem\n        described by Barto, Sutton, and Anderson\n\n    Observation:\n        Type: Box(4)\n        Num\tObservation               Min             Max\n        0\tCart Position             -4.8            4.8\n        1\tCart Velocity             -Inf            Inf\n        2\tPole Angle                -24 deg         24 deg\n        3\tPole Velocity At Tip      -Inf            Inf\n\n    Actions:\n        Type: Discrete(2)\n        Num\tAction\n        0\tPush cart to the left\n        1\tPush cart to the right\n\n        Note: The amount the velocity that is reduced or increased is not\n        fixed; it depends on the angle the pole is pointing. This is because\n        the center of gravity of the pole increases the amount of energy needed\n        to move the cart underneath it\n\n    Reward:\n        Reward is 1 for every step taken, including the termination step\n\n    Starting State:\n        All observations are assigned a uniform random value in [-0.05..0.05]\n\n    Episode Termination:\n        Pole Angle is more than 12 degrees.\n        Cart Position is more than 2.4 (center of the cart reaches the edge of\n        the display).\n        Episode length is greater than 200.\n        Solved Requirements:\n        Considered solved when the average reward is greater than or equal to\n        195.0 over 100 consecutive trials.\n    \"\"\"\n\n    metadata = {\n        'render.modes': ['human', 'rgb_array'],\n        'video.frames_per_second': 50\n    }\n\n    def __init__(self):\n        self.gravity = 9.8\n        self.masscart = 1.0\n        self.masspole = 0.1\n        self.total_mass = (self.masspole + self.masscart)\n        self.length = 0.5  # actually half the pole's length\n        self.polemass_length = (self.masspole * self.length)\n        self.force_mag = 10.0\n        self.tau = 0.02  # seconds between state updates\n        self.kinematics_integrator = 'euler'\n\n        # Angle at which to fail the episode\n        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n        self.x_threshold = 2.4\n\n        # Angle limit set to 2 * theta_threshold_radians so failing observation\n        # is still within bounds.\n        high = np.array([self.x_threshold * 2,\n                         np.finfo(np.float32).max,\n                         self.theta_threshold_radians * 2,\n                         np.finfo(np.float32).max],\n                        dtype=np.float32)\n\n        self.action_space = spaces.Discrete(2)\n        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n\n        self.seed()\n        self.viewer = None\n        self.state = None\n\n        self.steps_beyond_done = None\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def step(self, action):\n        err_msg = \"%r (%s) invalid\" % (action, type(action))\n        assert self.action_space.contains(action), err_msg\n\n        x, x_dot, theta, theta_dot = self.state\n        force = self.force_mag if action == 1 else -self.force_mag\n        costheta = math.cos(theta)\n        sintheta = math.sin(theta)\n\n        # For the interested reader:\n        # https://coneural.org/florian/papers/05_cart_pole.pdf\n        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass))\n        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n\n        if self.kinematics_integrator == 'euler':\n            x = x + self.tau * x_dot\n            x_dot = x_dot + self.tau * xacc\n            theta = theta + self.tau * theta_dot\n            theta_dot = theta_dot + self.tau * thetaacc\n        else:  # semi-implicit euler\n            x_dot = x_dot + self.tau * xacc\n            x = x + self.tau * x_dot\n            theta_dot = theta_dot + self.tau * thetaacc\n            theta = theta + self.tau * theta_dot\n\n        self.state = (x, x_dot, theta, theta_dot)\n\n        done = bool(\n            x < -self.x_threshold\n            or x > self.x_threshold\n            or theta < -self.theta_threshold_radians\n            or theta > self.theta_threshold_radians\n        )\n\n        if not done:\n            reward = 1.0\n        elif self.steps_beyond_done is None:\n            # Pole just fell!\n            self.steps_beyond_done = 0\n            reward = 1.0\n        else:\n            if self.steps_beyond_done == 0:\n                logger.warn(\n                    \"You are calling 'step()' even though this \"\n                    \"environment has already returned done = True. You \"\n                    \"should always call 'reset()' once you receive 'done = \"\n                    \"True' -- any further steps are undefined behavior.\"\n                )\n            self.steps_beyond_done += 1\n            reward = 0.0\n\n        return np.array(self.state), reward, done, {}\n\n    def reset(self):\n        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n        self.steps_beyond_done = None\n        return np.array(self.state)\n\n    def render(self, mode='human'):\n        screen_width = 600\n        screen_height = 400\n\n        world_width = self.x_threshold * 2\n        scale = screen_width/world_width\n        carty = 100  # TOP OF CART\n        polewidth = 10.0\n        polelen = scale * (2 * self.length)\n        cartwidth = 50.0\n        cartheight = 30.0\n\n        if self.viewer is None:\n            from gym.envs.classic_control import rendering\n            self.viewer = rendering.Viewer(screen_width, screen_height)\n            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n            axleoffset = cartheight / 4.0\n            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n            self.carttrans = rendering.Transform()\n            cart.add_attr(self.carttrans)\n            self.viewer.add_geom(cart)\n            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n            pole.set_color(.8, .6, .4)\n            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n            pole.add_attr(self.poletrans)\n            pole.add_attr(self.carttrans)\n            self.viewer.add_geom(pole)\n            self.axle = rendering.make_circle(polewidth/2)\n            self.axle.add_attr(self.poletrans)\n            self.axle.add_attr(self.carttrans)\n            self.axle.set_color(.5, .5, .8)\n            self.viewer.add_geom(self.axle)\n            self.track = rendering.Line((0, carty), (screen_width, carty))\n            self.track.set_color(0, 0, 0)\n            self.viewer.add_geom(self.track)\n\n            self._pole_geom = pole\n\n        if self.state is None:\n            return None\n\n        # Edit the pole polygon vertex\n        pole = self._pole_geom\n        l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n        pole.v = [(l, b), (l, t), (r, t), (r, b)]\n\n        x = self.state\n        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART\n        self.carttrans.set_translation(cartx, carty)\n        self.poletrans.set_rotation(-x[2])\n\n        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n\n    def close(self):\n        if self.viewer:\n            self.viewer.close()\n            self.viewer = None",
        "\"\"\"\nhttp://incompleteideas.net/sutton/MountainCar/MountainCar1.cp\npermalink: https://perma.cc/6Z2N-PFWC\n\"\"\"\n\nimport math\n\nimport numpy as np\n\nimport gym\nfrom gym import spaces\nfrom gym.utils import seeding\n\nclass MountainCarEnv(gym.Env):\n    metadata = {\n        'render.modes': ['human', 'rgb_array'],\n        'video.frames_per_second': 30\n    }\n\n    def __init__(self, goal_velocity = 0):\n        self.min_position = -1.2\n        self.max_position = 0.6\n        self.max_speed = 0.07\n        self.goal_position = 0.5\n        self.goal_velocity = goal_velocity\n        \n        self.force=0.001\n        self.gravity=0.0025\n\n        self.low = np.array([self.min_position, -self.max_speed], dtype=np.float32)\n        self.high = np.array([self.max_position, self.max_speed], dtype=np.float32)\n\n        self.viewer = None\n\n        self.action_space = spaces.Discrete(3)\n        self.observation_space = spaces.Box(self.low, self.high, dtype=np.float32)\n\n        self.seed()\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def step(self, action):\n        assert self.action_space.contains(action), \"%r (%s) invalid\" % (action, type(action))\n\n        position, velocity = self.state\n        velocity += (action-1)*self.force + math.cos(3*position)*(-self.gravity)\n        velocity = np.clip(velocity, -self.max_speed, self.max_speed)\n        position += velocity\n        position = np.clip(position, self.min_position, self.max_position)\n        if (position==self.min_position and velocity<0): velocity = 0\n\n        done = bool(position >= self.goal_position and velocity >= self.goal_velocity)\n        reward = -1.0\n\n        self.state = (position, velocity)\n        return np.array(self.state), reward, done, {}\n\n    def reset(self):\n        self.state = np.array([self.np_random.uniform(low=-0.6, high=-0.4), 0])\n        return np.array(self.state)\n\n    def _height(self, xs):\n        return np.sin(3 * xs)*.45+.55\n\n    def render(self, mode='human'):\n        screen_width = 600\n        screen_height = 400\n\n        world_width = self.max_position - self.min_position\n        scale = screen_width/world_width\n        carwidth=40\n        carheight=20\n\n\n        if self.viewer is None:\n            from gym.envs.classic_control import rendering\n            self.viewer = rendering.Viewer(screen_width, screen_height)\n            xs = np.linspace(self.min_position, self.max_position, 100)\n            ys = self._height(xs)\n            xys = list(zip((xs-self.min_position)*scale, ys*scale))\n\n            self.track = rendering.make_polyline(xys)\n            self.track.set_linewidth(4)\n            self.viewer.add_geom(self.track)\n\n            clearance = 10\n\n            l,r,t,b = -carwidth/2, carwidth/2, carheight, 0\n            car = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n            car.add_attr(rendering.Transform(translation=(0, clearance)))\n            self.cartrans = rendering.Transform()\n            car.add_attr(self.cartrans)\n            self.viewer.add_geom(car)\n            frontwheel = rendering.make_circle(carheight/2.5)\n            frontwheel.set_color(.5, .5, .5)\n            frontwheel.add_attr(rendering.Transform(translation=(carwidth/4,clearance)))\n            frontwheel.add_attr(self.cartrans)\n            self.viewer.add_geom(frontwheel)\n            backwheel = rendering.make_circle(carheight/2.5)\n            backwheel.add_attr(rendering.Transform(translation=(-carwidth/4,clearance)))\n            backwheel.add_attr(self.cartrans)\n            backwheel.set_color(.5, .5, .5)\n            self.viewer.add_geom(backwheel)\n            flagx = (self.goal_position-self.min_position)*scale\n            flagy1 = self._height(self.goal_position)*scale\n            flagy2 = flagy1 + 50\n            flagpole = rendering.Line((flagx, flagy1), (flagx, flagy2))\n            self.viewer.add_geom(flagpole)\n            flag = rendering.FilledPolygon([(flagx, flagy2), (flagx, flagy2-10), (flagx+25, flagy2-5)])\n            flag.set_color(.8,.8,0)\n            self.viewer.add_geom(flag)\n\n        pos = self.state[0]\n        self.cartrans.set_translation((pos-self.min_position)*scale, self._height(pos)*scale)\n        self.cartrans.set_rotation(math.cos(3 * pos))\n\n        return self.viewer.render(return_rgb_array = mode=='rgb_array')\n    \n    def get_keys_to_action(self):\n        return {():1,(276,):0,(275,):2,(275,276):1} #control with left and right arrow keys \n    \n    def close(self):\n        if self.viewer:\n            self.viewer.close()\n            self.viewer = None",
        "\"\"\"classic Acrobot task\"\"\"\nimport numpy as np\nfrom numpy import sin, cos, pi\n\nfrom gym import core, spaces\nfrom gym.utils import seeding\n\n__copyright__ = \"Copyright 2013, RLPy http://acl.mit.edu/RLPy\"\n__credits__ = [\"Alborz Geramifard\", \"Robert H. Klein\", \"Christoph Dann\",\n               \"William Dabney\", \"Jonathan P. How\"]\n__license__ = \"BSD 3-Clause\"\n__author__ = \"Christoph Dann <cdann@cdann.de>\"\n\n# SOURCE:\n# https://github.com/rlpy/rlpy/blob/master/rlpy/Domains/Acrobot.py\n\nclass AcrobotEnv(core.Env):\n\n    \"\"\"\n    Acrobot is a 2-link pendulum with only the second joint actuated.\n    Initially, both links point downwards. The goal is to swing the\n    end-effector at a height at least the length of one link above the base.\n    Both links can swing freely and can pass by each other, i.e., they don't\n    collide when they have the same angle.\n    **STATE:**\n    The state consists of the sin() and cos() of the two rotational joint\n    angles and the joint angular velocities :\n    [cos(theta1) sin(theta1) cos(theta2) sin(theta2) thetaDot1 thetaDot2].\n    For the first link, an angle of 0 corresponds to the link pointing downwards.\n    The angle of the second link is relative to the angle of the first link.\n    An angle of 0 corresponds to having the same angle between the two links.\n    A state of [1, 0, 1, 0, ..., ...] means that both links point downwards.\n    **ACTIONS:**\n    The action is either applying +1, 0 or -1 torque on the joint between\n    the two pendulum links.\n    .. note::\n        The dynamics equations were missing some terms in the NIPS paper which\n        are present in the book. R. Sutton confirmed in personal correspondence\n        that the experimental results shown in the paper and the book were\n        generated with the equations shown in the book.\n        However, there is the option to run the domain with the paper equations\n        by setting book_or_nips = 'nips'\n    **REFERENCE:**\n    .. seealso::\n        R. Sutton: Generalization in Reinforcement Learning:\n        Successful Examples Using Sparse Coarse Coding (NIPS 1996)\n    .. seealso::\n        R. Sutton and A. G. Barto:\n        Reinforcement learning: An introduction.\n        Cambridge: MIT press, 1998.\n    .. warning::\n        This version of the domain uses the Runge-Kutta method for integrating\n        the system dynamics and is more realistic, but also considerably harder\n        than the original version which employs Euler integration,\n        see the AcrobotLegacy class.\n    \"\"\"\n\n    metadata = {\n        'render.modes': ['human', 'rgb_array'],\n        'video.frames_per_second' : 15\n    }\n\n    dt = .2\n\n    LINK_LENGTH_1 = 1.  # [m]\n    LINK_LENGTH_2 = 1.  # [m]\n    LINK_MASS_1 = 1.  #: [kg] mass of link 1\n    LINK_MASS_2 = 1.  #: [kg] mass of link 2\n    LINK_COM_POS_1 = 0.5  #: [m] position of the center of mass of link 1\n    LINK_COM_POS_2 = 0.5  #: [m] position of the center of mass of link 2\n    LINK_MOI = 1.  #: moments of inertia for both links\n\n    MAX_VEL_1 = 4 * pi\n    MAX_VEL_2 = 9 * pi\n\n    AVAIL_TORQUE = [-1., 0., +1]\n\n    torque_noise_max = 0.\n\n    #: use dynamics equations from the nips paper or the book\n    book_or_nips = \"book\"\n    action_arrow = None\n    domain_fig = None\n    actions_num = 3\n\n    def __init__(self):\n        self.viewer = None\n        high = np.array([1.0, 1.0, 1.0, 1.0, self.MAX_VEL_1, self.MAX_VEL_2], dtype=np.float32)\n        low = -high\n        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float32)\n        self.action_space = spaces.Discrete(3)\n        self.state = None\n        self.seed()\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def reset(self):\n        self.state = self.np_random.uniform(low=-0.1, high=0.1, size=(4,))\n        return self._get_ob()\n\n    def step(self, a):\n        s = self.state\n        torque = self.AVAIL_TORQUE[a]\n\n        # Add noise to the force action\n        if self.torque_noise_max > 0:\n            torque += self.np_random.uniform(-self.torque_noise_max, self.torque_noise_max)\n\n        # Now, augment the state with our force action so it can be passed to\n        # _dsdt\n        s_augmented = np.append(s, torque)\n\n        ns = rk4(self._dsdt, s_augmented, [0, self.dt])\n        # only care about final timestep of integration returned by integrator\n        ns = ns[-1]\n        ns = ns[:4]  # omit action\n        # ODEINT IS TOO SLOW!\n        # ns_continuous = integrate.odeint(self._dsdt, self.s_continuous, [0, self.dt])\n        # self.s_continuous = ns_continuous[-1] # We only care about the state\n        # at the ''final timestep'', self.dt\n\n        ns[0] = wrap(ns[0], -pi, pi)\n        ns[1] = wrap(ns[1], -pi, pi)\n        ns[2] = bound(ns[2], -self.MAX_VEL_1, self.MAX_VEL_1)\n        ns[3] = bound(ns[3], -self.MAX_VEL_2, self.MAX_VEL_2)\n        self.state = ns\n        terminal = self._terminal()\n        reward = -1. if not terminal else 0.\n        return (self._get_ob(), reward, terminal, {})\n\n    def _get_ob(self):\n        s = self.state\n        return np.array([cos(s[0]), sin(s[0]), cos(s[1]), sin(s[1]), s[2], s[3]])\n\n    def _terminal(self):\n        s = self.state\n        return bool(-cos(s[0]) - cos(s[1] + s[0]) > 1.)\n\n    def _dsdt(self, s_augmented, t):\n        m1 = self.LINK_MASS_1\n        m2 = self.LINK_MASS_2\n        l1 = self.LINK_LENGTH_1\n        lc1 = self.LINK_COM_POS_1\n        lc2 = self.LINK_COM_POS_2\n        I1 = self.LINK_MOI\n        I2 = self.LINK_MOI\n        g = 9.8\n        a = s_augmented[-1]\n        s = s_augmented[:-1]\n        theta1 = s[0]\n        theta2 = s[1]\n        dtheta1 = s[2]\n        dtheta2 = s[3]\n        d1 = m1 * lc1 ** 2 + m2 * \\\n            (l1 ** 2 + lc2 ** 2 + 2 * l1 * lc2 * cos(theta2)) + I1 + I2\n        d2 = m2 * (lc2 ** 2 + l1 * lc2 * cos(theta2)) + I2\n        phi2 = m2 * lc2 * g * cos(theta1 + theta2 - pi / 2.)\n        phi1 = - m2 * l1 * lc2 * dtheta2 ** 2 * sin(theta2) \\\n               - 2 * m2 * l1 * lc2 * dtheta2 * dtheta1 * sin(theta2)  \\\n            + (m1 * lc1 + m2 * l1) * g * cos(theta1 - pi / 2) + phi2\n        if self.book_or_nips == \"nips\":\n            # the following line is consistent with the description in the\n            # paper\n            ddtheta2 = (a + d2 / d1 * phi1 - phi2) / \\\n                (m2 * lc2 ** 2 + I2 - d2 ** 2 / d1)\n        else:\n            # the following line is consistent with the java implementation and the\n            # book\n            ddtheta2 = (a + d2 / d1 * phi1 - m2 * l1 * lc2 * dtheta1 ** 2 * sin(theta2) - phi2) \\\n                / (m2 * lc2 ** 2 + I2 - d2 ** 2 / d1)\n        ddtheta1 = -(d2 * ddtheta2 + phi1) / d1\n        return (dtheta1, dtheta2, ddtheta1, ddtheta2, 0.)\n\n    def render(self, mode='human'):\n        from gym.envs.classic_control import rendering\n\n        s = self.state\n\n        if self.viewer is None:\n            self.viewer = rendering.Viewer(500,500)\n            bound = self.LINK_LENGTH_1 + self.LINK_LENGTH_2 + 0.2  # 2.2 for default\n            self.viewer.set_bounds(-bound,bound,-bound,bound)\n\n        if s is None: return None\n\n        p1 = [-self.LINK_LENGTH_1 *\n              cos(s[0]), self.LINK_LENGTH_1 * sin(s[0])]\n\n        p2 = [p1[0] - self.LINK_LENGTH_2 * cos(s[0] + s[1]),\n              p1[1] + self.LINK_LENGTH_2 * sin(s[0] + s[1])]\n\n        xys = np.array([[0,0], p1, p2])[:,::-1]\n        thetas = [s[0]- pi/2, s[0]+s[1]-pi/2]\n        link_lengths = [self.LINK_LENGTH_1, self.LINK_LENGTH_2]\n\n        self.viewer.draw_line((-2.2, 1), (2.2, 1))\n        for ((x,y),th,llen) in zip(xys, thetas, link_lengths):\n            l,r,t,b = 0, llen, .1, -.1\n            jtransform = rendering.Transform(rotation=th, translation=(x,y))\n            link = self.viewer.draw_polygon([(l,b), (l,t), (r,t), (r,b)])\n            link.add_attr(jtransform)\n            link.set_color(0,.8, .8)\n            circ = self.viewer.draw_circle(.1)\n            circ.set_color(.8, .8, 0)\n            circ.add_attr(jtransform)\n\n        return self.viewer.render(return_rgb_array = mode=='rgb_array')\n\n    def close(self):\n        if self.viewer:\n            self.viewer.close()\n            self.viewer = None\n\ndef wrap(x, m, M):\n    \"\"\"Wraps ``x`` so m <= x <= M; but unlike ``bound()`` which\n    truncates, ``wrap()`` wraps x around the coordinate system defined by m,M.\\n\n    For example, m = -180, M = 180 (degrees), x = 360 --> returns 0.\n\n    Args:\n        x: a scalar\n        m: minimum possible value in range\n        M: maximum possible value in range\n\n    Returns:\n        x: a scalar, wrapped\n    \"\"\"\n    diff = M - m\n    while x > M:\n        x = x - diff\n    while x < m:\n        x = x + diff\n    return x\n\ndef bound(x, m, M=None):\n    \"\"\"Either have m as scalar, so bound(x,m,M) which returns m <= x <= M *OR*\n    have m as length 2 vector, bound(x,m, <IGNORED>) returns m[0] <= x <= m[1].\n\n    Args:\n        x: scalar\n\n    Returns:\n        x: scalar, bound between min (m) and Max (M)\n    \"\"\"\n    if M is None:\n        M = m[1]\n        m = m[0]\n    # bound x between min (m) and Max (M)\n    return min(max(x, m), M)\n\n\ndef rk4(derivs, y0, t, *args, **kwargs):\n    \"\"\"\n    Integrate 1D or ND system of ODEs using 4-th order Runge-Kutta.\n    This is a toy implementation which may be useful if you find\n    yourself stranded on a system w/o scipy.  Otherwise use\n    :func:`scipy.integrate`.\n\n    Args:\n        derivs: the derivative of the system and has the signature ``dy = derivs(yi, ti)``\n        y0: initial state vector\n        t: sample times\n        args: additional arguments passed to the derivative function\n        kwargs: additional keyword arguments passed to the derivative function\n\n    Example 1 ::\n        ## 2D system\n        def derivs6(x,t):\n            d1 =  x[0] + 2*x[1]\n            d2 =  -3*x[0] + 4*x[1]\n            return (d1, d2)\n        dt = 0.0005\n        t = arange(0.0, 2.0, dt)\n        y0 = (1,2)\n        yout = rk4(derivs6, y0, t)\n    Example 2::\n        ## 1D system\n        alpha = 2\n        def derivs(x,t):\n            return -alpha*x + exp(-t)\n        y0 = 1\n        yout = rk4(derivs, y0, t)\n    If you have access to scipy, you should probably be using the\n    scipy.integrate tools rather than this function.\n\n    Returns:\n        yout: Runge-Kutta approximation of the ODE\n    \"\"\"\n\n    try:\n        Ny = len(y0)\n    except TypeError:\n        yout = np.zeros((len(t),), np.float_)\n    else:\n        yout = np.zeros((len(t), Ny), np.float_)\n\n    yout[0] = y0\n\n\n    for i in np.arange(len(t) - 1):\n\n        thist = t[i]\n        dt = t[i + 1] - thist\n        dt2 = dt / 2.0\n        y0 = yout[i]\n\n        k1 = np.asarray(derivs(y0, thist, *args, **kwargs))\n        k2 = np.asarray(derivs(y0 + dt2 * k1, thist + dt2, *args, **kwargs))\n        k3 = np.asarray(derivs(y0 + dt2 * k2, thist + dt2, *args, **kwargs))\n        k4 = np.asarray(derivs(y0 + dt * k3, thist + dt, *args, **kwargs))\n        yout[i + 1] = y0 + dt / 6.0 * (k1 + 2 * k2 + 2 * k3 + k4)\n    return yout",
        "import gym\nfrom gym import spaces\nfrom gym.utils import seeding\nimport numpy as np\nfrom os import path\n\n\nclass PendulumEnv(gym.Env):\n    metadata = {\n        'render.modes': ['human', 'rgb_array'],\n        'video.frames_per_second': 30\n    }\n\n    def __init__(self, g=10.0):\n        self.max_speed = 8\n        self.max_torque = 2.\n        self.dt = .05\n        self.g = g\n        self.m = 1.\n        self.l = 1.\n        self.viewer = None\n\n        high = np.array([1., 1., self.max_speed], dtype=np.float32)\n        self.action_space = spaces.Box(\n            low=-self.max_torque,\n            high=self.max_torque, shape=(1,),\n            dtype=np.float32\n        )\n        self.observation_space = spaces.Box(\n            low=-high,\n            high=high,\n            dtype=np.float32\n        )\n\n        self.seed()\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def step(self, u):\n        th, thdot = self.state  # th := theta\n\n        g = self.g\n        m = self.m\n        l = self.l\n        dt = self.dt\n\n        u = np.clip(u, -self.max_torque, self.max_torque)[0]\n        self.last_u = u  # for rendering\n        costs = angle_normalize(th) ** 2 + .1 * thdot ** 2 + .001 * (u ** 2)\n\n        newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt\n        newth = th + newthdot * dt\n        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)\n\n        self.state = np.array([newth, newthdot])\n        return self._get_obs(), -costs, False, {}\n\n    def reset(self):\n        high = np.array([np.pi, 1])\n        self.state = self.np_random.uniform(low=-high, high=high)\n        self.last_u = None\n        return self._get_obs()\n\n    def _get_obs(self):\n        theta, thetadot = self.state\n        return np.array([np.cos(theta), np.sin(theta), thetadot])\n\n    def render(self, mode='human'):\n        if self.viewer is None:\n            from gym.envs.classic_control import rendering\n            self.viewer = rendering.Viewer(500, 500)\n            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)\n            rod = rendering.make_capsule(1, .2)\n            rod.set_color(.8, .3, .3)\n            self.pole_transform = rendering.Transform()\n            rod.add_attr(self.pole_transform)\n            self.viewer.add_geom(rod)\n            axle = rendering.make_circle(.05)\n            axle.set_color(0, 0, 0)\n            self.viewer.add_geom(axle)\n            fname = path.join(path.dirname(__file__), \"assets/clockwise.png\")\n            self.img = rendering.Image(fname, 1., 1.)\n            self.imgtrans = rendering.Transform()\n            self.img.add_attr(self.imgtrans)\n\n        self.viewer.add_onetime(self.img)\n        self.pole_transform.set_rotation(self.state[0] + np.pi / 2)\n        if self.last_u:\n            self.imgtrans.scale = (-self.last_u / 2, np.abs(self.last_u) / 2)\n\n        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n\n    def close(self):\n        if self.viewer:\n            self.viewer.close()\n            self.viewer = None\n\n\ndef angle_normalize(x):\n    return (((x+np.pi) % (2*np.pi)) - np.pi)",
        "\"\"\"\nTask is to copy content from the input tape to\nthe output tape. http://arxiv.org/abs/1511.07275\n\"\"\"\nfrom gym.envs.algorithmic import algorithmic_env\n\n\nclass CopyEnv(algorithmic_env.TapeAlgorithmicEnv):\n    def __init__(self, base=5, chars=True):\n        super(CopyEnv, self).__init__(base=base, chars=chars)\n\n    def target_from_input_data(self, input_data):\n        return input_data",
        "from gym.envs.algorithmic import algorithmic_env\n\n\nclass ReversedAdditionEnv(algorithmic_env.GridAlgorithmicEnv):\n    def __init__(self, rows=2, base=3):\n        super(ReversedAdditionEnv, self).__init__(rows=rows, base=base, chars=False)\n\n    def target_from_input_data(self, input_strings):\n        curry = 0\n        target = []\n        for digits in input_strings:\n            total = sum(digits) + curry\n            target.append(total % self.base)\n            curry = total // self.base\n\n        if curry > 0:\n            target.append(curry)\n        return target\n\n    @property\n    def time_limit(self):\n        # Quirk preserved for the sake of consistency: add the length of the input\n        # rather than the length of the desired output (which may differ if there's\n        # an extra carried digit).\n        # TODO: It seems like this time limit is so strict as to make Addition3-v0\n        # unsolvable, since agents aren't even given enough time steps to look at\n        # all the digits. (The solutions on the scoreboard seem to only work by\n        # save-scumming.)\n        return self.input_width*2 + 4",
        "\"\"\"\nTask is to return every nth character from the input tape.\nhttp://arxiv.org/abs/1511.07275\n\"\"\"\nfrom gym.envs.algorithmic import algorithmic_env\n\n\nclass DuplicatedInputEnv(algorithmic_env.TapeAlgorithmicEnv):\n    def __init__(self, duplication=2, base=5):\n        self.duplication = duplication\n        super(DuplicatedInputEnv, self).__init__(base=base, chars=True)\n\n    def generate_input_data(self, size):\n        res = []\n        if size < self.duplication:\n            size = self.duplication\n        for i in range(size//self.duplication):\n            char = self.np_random.randint(self.base)\n            for _ in range(self.duplication):\n                res.append(char)\n        return res\n\n    def target_from_input_data(self, input_data):\n        return [input_data[i] for i in range(0, len(input_data), self.duplication)]",
        "\"\"\"\nTask is to reverse content over the input tape.\nhttp://arxiv.org/abs/1511.07275\n\"\"\"\nfrom gym.envs.algorithmic import algorithmic_env\n\n\nclass ReverseEnv(algorithmic_env.TapeAlgorithmicEnv):\n    MIN_REWARD_SHORTFALL_FOR_PROMOTION = -.1\n\n    def __init__(self, base=2):\n        super(ReverseEnv, self).__init__(base=base, chars=True, starting_min_length=1)\n        self.last = 50\n\n    def target_from_input_data(self, input_str):\n        return list(reversed(input_str))",
        "",
        "from gym.envs import algorithmic as alg\nimport unittest\n\n# All concrete subclasses of AlgorithmicEnv\nALL_ENVS = [\n    alg.copy_.CopyEnv, \n    alg.duplicated_input.DuplicatedInputEnv,\n    alg.repeat_copy.RepeatCopyEnv,\n    alg.reverse.ReverseEnv,\n    alg.reversed_addition.ReversedAdditionEnv,\n]\nALL_TAPE_ENVS = [env for env in ALL_ENVS \n    if issubclass(env, alg.algorithmic_env.TapeAlgorithmicEnv)]\nALL_GRID_ENVS = [env for env in ALL_ENVS \n    if issubclass(env, alg.algorithmic_env.GridAlgorithmicEnv)]\n\ndef imprint(env, input_arr):\n    \"\"\"Monkey-patch the given environment so that when reset() is called, the\n    input tape/grid will be set to the given data, rather than being randomly\n    generated.\"\"\"\n    env.generate_input_data = lambda _: input_arr\n\nclass TestAlgorithmicEnvInteractions(unittest.TestCase):\n    \"\"\"Test some generic behaviour not specific to any particular algorithmic\n    environment. Movement, allocation of rewards, etc.\"\"\"\n    CANNED_INPUT = [0, 1]\n    ENV_KLS = alg.copy_.CopyEnv\n    LEFT, RIGHT = ENV_KLS._movement_idx('left'), ENV_KLS._movement_idx('right')\n    def setUp(self):\n        self.env = self.ENV_KLS(base=2, chars=True)\n        imprint(self.env, self.CANNED_INPUT)\n\n    def test_successful_interaction(self):\n        obs = self.env.reset()\n        self.assertEqual(obs, 0)\n        obs, reward, done, _ = self.env.step([self.RIGHT, 1, 0])\n        self.assertEqual(obs, 1)\n        self.assertGreater(reward, 0)\n        self.assertFalse(done)\n        obs, reward, done, _ = self.env.step([self.LEFT, 1, 1])\n        self.assertTrue(done)\n        self.assertGreater(reward, 0)\n\n    def test_bad_output_fail_fast(self):\n        obs = self.env.reset()\n        obs, reward, done, _ = self.env.step([self.RIGHT, 1, 1])\n        self.assertTrue(done)\n        self.assertLess(reward, 0)\n\n    def test_levelup(self):\n        obs = self.env.reset()\n        # Kind of a hack\n        alg.algorithmic_env.AlgorithmicEnv.reward_shortfalls = []\n        min_length = self.env.min_length\n        for i in range(self.env.last):\n            obs, reward, done, _ = self.env.step([self.RIGHT, 1, 0])\n            self.assertFalse(done)\n            obs, reward, done, _ = self.env.step([self.RIGHT, 1, 1])\n            self.assertTrue(done)\n            self.env.reset()\n            if i < self.env.last-1:\n                self.assertEqual(len(alg.algorithmic_env.AlgorithmicEnv.reward_shortfalls), i+1)\n            else:\n                # Should have leveled up on the last iteration\n                self.assertEqual(self.env.min_length, min_length+1)\n                self.assertEqual(len(alg.algorithmic_env.AlgorithmicEnv.reward_shortfalls), 0)\n\n    def test_walk_off_the_end(self):\n        obs = self.env.reset()\n        # Walk off the end\n        obs, r, done, _ = self.env.step([self.LEFT, 0, 0])\n        self.assertEqual(obs, self.env.base)\n        self.assertEqual(r, 0)\n        self.assertFalse(done)\n        # Walk further off track\n        obs, r, done, _ = self.env.step([self.LEFT, 0, 0])\n        self.assertEqual(obs, self.env.base)\n        self.assertFalse(done)\n        # Return to the first input character\n        obs, r, done, _ = self.env.step([self.RIGHT, 0, 0])\n        self.assertEqual(obs, self.env.base)\n        self.assertFalse(done)\n        obs, r, done, _ = self.env.step([self.RIGHT, 0, 0])\n        self.assertEqual(obs, 0)\n\n    def test_grid_naviation(self):\n        env = alg.reversed_addition.ReversedAdditionEnv(rows=2, base=6)\n        N,S,E,W = [env._movement_idx(named_dir) for named_dir in ['up', 'down', 'right', 'left']]\n        # Corresponds to a grid that looks like...\n        #       0 1 2\n        #       3 4 5\n        canned = [ [0, 3], [1, 4], [2, 5] ]\n        imprint(env, canned)\n        obs = env.reset()\n        self.assertEqual(obs, 0)\n        navigation = [\n          (S, 3), (N, 0), (E, 1), (S, 4), (S, 6), (E, 6), (N, 5), (N, 2), (W, 1)\n        ]\n        for (movement, expected_obs) in navigation:\n            obs, reward, done, _ = env.step([movement, 0, 0])\n            self.assertEqual(reward, 0)\n            self.assertFalse(done)\n            self.assertEqual(obs, expected_obs)\n\n    def test_grid_success(self):\n        env = alg.reversed_addition.ReversedAdditionEnv(rows=2, base=3)\n        canned = [ [1, 2], [1, 0], [2, 2] ]\n        imprint(env, canned)\n        obs = env.reset()\n        target = [0, 2, 1, 1]\n        self.assertEqual(env.target, target)\n        self.assertEqual(obs, 1)\n        for i, target_digit in enumerate(target):\n            obs, reward, done, _ = env.step([0, 1, target_digit])\n            self.assertGreater(reward, 0)\n            self.assertEqual(done, i==len(target)-1)\n\n    def test_sane_time_limit(self):\n        obs = self.env.reset()\n        self.assertLess(self.env.time_limit, 100)\n        for _ in range(100):\n            obs, r, done, _ = self.env.step([self.LEFT, 0, 0])\n            if done:\n                return\n        self.fail(\"Time limit wasn't enforced\")\n\n    def test_rendering(self):\n        env = self.env\n        obs = env.reset()\n        self.assertEqual(env._get_str_obs(), 'A')\n        self.assertEqual(env._get_str_obs(1), 'B')\n        self.assertEqual(env._get_str_obs(-1), ' ')\n        self.assertEqual(env._get_str_obs(2), ' ')\n        self.assertEqual(env._get_str_target(0), 'A')\n        self.assertEqual(env._get_str_target(1), 'B')\n        # Test numerical alphabet rendering\n        env = self.ENV_KLS(base=3, chars=False)\n        imprint(env, self.CANNED_INPUT)\n        env.reset()\n        self.assertEqual(env._get_str_obs(), '0')\n        self.assertEqual(env._get_str_obs(1), '1')\n\n\nclass TestTargets(unittest.TestCase):\n    \"\"\"Test the rules mapping input strings/grids to target outputs.\"\"\"\n    def test_reverse_target(self):\n        input_expected = [\n            ([0], [0]),\n            ([0, 1], [1, 0]),\n            ([1, 1], [1, 1]),\n            ([1, 0, 1], [1, 0, 1]),\n            ([0, 0, 1, 1], [1, 1, 0, 0]),\n        ]\n        env = alg.reverse.ReverseEnv()\n        for input_arr, expected in input_expected:\n            target = env.target_from_input_data(input_arr)\n            self.assertEqual(target, expected)\n\n    def test_reversed_addition_target(self):\n        env = alg.reversed_addition.ReversedAdditionEnv(base=3)\n        input_expected = [\n            ([[1,1], [1,1]], [2, 2]),\n            ([[2,2], [0,1]], [1, 2]),\n            ([[2,1], [1,1], [1,1], [1,0]], [0, 0, 0, 2]),\n        ]\n        for (input_grid, expected_target) in input_expected:\n            self.assertEqual(env.target_from_input_data(input_grid), expected_target)\n\n    def test_reversed_addition_3rows(self):\n        env = alg.reversed_addition.ReversedAdditionEnv(base=3, rows=3)\n        input_expected = [\n            ([[1,1,0],[0,1,1]], [2, 2]),\n            ([[1,1,2],[0,1,1]], [1,0,1]),\n        ]\n        for (input_grid, expected_target) in input_expected:\n            self.assertEqual(env.target_from_input_data(input_grid), expected_target)\n\n    def test_copy_target(self):\n        env = alg.copy_.CopyEnv()\n        self.assertEqual(env.target_from_input_data([0, 1, 2]), [0, 1, 2])\n\n    def test_duplicated_input_target(self):\n        env = alg.duplicated_input.DuplicatedInputEnv(duplication=2)\n        self.assertEqual(env.target_from_input_data([0, 0, 0, 0, 1, 1]), [0, 0, 1])\n\n    def test_repeat_copy_target(self):\n        env = alg.repeat_copy.RepeatCopyEnv()\n        self.assertEqual(env.target_from_input_data([0, 1, 2]), [0, 1, 2, 2, 1, 0, 0, 1, 2])\n\nclass TestInputGeneration(unittest.TestCase):\n    \"\"\"Test random input generation.\n    \"\"\"\n    def test_tape_inputs(self):\n        for env_kls in ALL_TAPE_ENVS:\n            env = env_kls()\n            for size in range(2,5):\n                input_tape = env.generate_input_data(size)\n                self.assertTrue(all(0<=x<=env.base for x in input_tape),\n                \"Invalid input tape from env {}: {}\".format(env_kls, input_tape))\n                # DuplicatedInput needs to generate inputs with even length,\n                # so it may be short one\n                self.assertLessEqual(len(input_tape), size)\n\n    def test_grid_inputs(self):\n        for env_kls in ALL_GRID_ENVS:\n            env = env_kls()\n            for size in range(2, 5):\n                input_grid = env.generate_input_data(size)\n                # Should get \"size\" sublists, each of length self.rows (not the\n                # opposite, as you might expect)\n                self.assertEqual(len(input_grid), size)\n                self.assertTrue(all(len(col) == env.rows for col in input_grid))\n                self.assertTrue(all(0<=x<=env.base for x in input_grid[0]))\n\n    def test_duplicatedinput_inputs(self):\n        \"\"\"The duplicated_input env needs to generate strings with the appropriate\n        amount of repetiion.\"\"\"\n        env = alg.duplicated_input.DuplicatedInputEnv(duplication=2)\n        input_tape = env.generate_input_data(4)\n        self.assertEqual(len(input_tape), 4)\n        self.assertEqual(input_tape[0], input_tape[1])\n        self.assertEqual(input_tape[2], input_tape[3])\n        # If requested input size isn't a multiple of duplication, go lower\n        input_tape = env.generate_input_data(3)\n        self.assertEqual(len(input_tape), 2)\n        self.assertEqual(input_tape[0], input_tape[1])\n        # If requested input size is *less than* duplication, go up\n        input_tape = env.generate_input_data(1)\n        self.assertEqual(len(input_tape), 2)\n        self.assertEqual(input_tape[0], input_tape[1])\n        \n        env = alg.duplicated_input.DuplicatedInputEnv(duplication=3)\n        input_tape = env.generate_input_data(6)\n        self.assertEqual(len(input_tape), 6)\n        self.assertEqual(input_tape[0], input_tape[1])\n        self.assertEqual(input_tape[1], input_tape[2])\n\nif __name__ == '__main__':\n    unittest.main()",
        "from gym.envs.algorithmic.copy_ import CopyEnv\nfrom gym.envs.algorithmic.repeat_copy import RepeatCopyEnv\nfrom gym.envs.algorithmic.duplicated_input import DuplicatedInputEnv\nfrom gym.envs.algorithmic.reverse import ReverseEnv\nfrom gym.envs.algorithmic.reversed_addition import ReversedAdditionEnv",
        "\"\"\"\nAlgorithmic environments have the following traits in common:\n\n- A 1-d \"input tape\" or 2-d \"input grid\" of characters\n- A target string which is a deterministic function of the input characters\n\nAgents control a read head that moves over the input tape. Observations consist\nof the single character currently under the read head. The read head may fall\noff the end of the tape in any direction. When this happens, agents will observe\na special blank character (with index=env.base) until they get back in bounds.\n\nActions consist of 3 sub-actions:\n    - Direction to move the read head (left or right, plus up and down for 2-d envs)\n    - Whether to write to the output tape\n    - Which character to write (ignored if the above sub-action is 0)\n\nAn episode ends when:\n    - The agent writes the full target string to the output tape.\n    - The agent writes an incorrect character.\n    - The agent runs out the time limit. (Which is fairly conservative.)\n\nReward schedule:\n    write a correct character: +1\n    write a wrong character: -.5\n    run out the clock: -1\n    otherwise: 0\n\nIn the beginning, input strings will be fairly short. After an environment has\nbeen consistently solved over some window of episodes, the environment will\nincrease the average length of generated strings. Typical env specs require\nleveling up many times to reach their reward threshold.\n\"\"\"\nfrom gym import Env, logger\nfrom gym.spaces import Discrete, Tuple\nfrom gym.utils import colorize, seeding\nimport sys\nfrom contextlib import closing\nimport numpy as np\nfrom io import StringIO\n\n\nclass AlgorithmicEnv(Env):\n\n    metadata = {'render.modes': ['human', 'ansi']}\n    # Only 'promote' the length of generated input strings if the worst of the\n    # last n episodes was no more than this far from the maximum reward\n    MIN_REWARD_SHORTFALL_FOR_PROMOTION = -1.0\n\n    def __init__(self, base=10, chars=False, starting_min_length=2):\n        \"\"\"\n        base: Number of distinct characters.\n        chars: If True, use uppercase alphabet. Otherwise, digits. Only affects\n               rendering.\n        starting_min_length: Minimum input string length. Ramps up as episodes\n                             are consistently solved.\n        \"\"\"\n        self.base = base\n        # Keep track of this many past episodes\n        self.last = 10\n        # Cumulative reward earned this episode\n        self.episode_total_reward = None\n        # Running tally of reward shortfalls. e.g. if there were 10 points to\n        # earn and we got 8, we'd append -2\n        AlgorithmicEnv.reward_shortfalls = []\n        if chars:\n            self.charmap = [chr(ord('A')+i) for i in range(base)]\n        else:\n            self.charmap = [str(i) for i in range(base)]\n        self.charmap.append(' ')\n        # TODO: Not clear why this is a class variable rather than instance.\n        # Could lead to some spooky action at a distance if someone is working\n        # with multiple algorithmic envs at once. Also makes testing tricky.\n        AlgorithmicEnv.min_length = starting_min_length\n        # Three sub-actions:\n        #       1. Move read head left or right (or up/down)\n        #       2. Write or not\n        #       3. Which character to write. (Ignored if should_write=0)\n        self.action_space = Tuple(\n            [Discrete(len(self.MOVEMENTS)), Discrete(2), Discrete(self.base)]\n        )\n        # Can see just what is on the input tape (one of n characters, or\n        # nothing)\n        self.observation_space = Discrete(self.base + 1)\n        self.seed()\n        self.reset()\n\n    @classmethod\n    def _movement_idx(kls, movement_name):\n        return kls.MOVEMENTS.index(movement_name)\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def _get_obs(self, pos=None):\n        \"\"\"Return an observation corresponding to the given read head position\n        (or the current read head position, if none is given).\"\"\"\n        raise NotImplementedError\n\n    def _get_str_obs(self, pos=None):\n        ret = self._get_obs(pos)\n        return self.charmap[ret]\n\n    def _get_str_target(self, pos):\n        \"\"\"Return the ith character of the target string (or \" \" if index\n        out of bounds).\"\"\"\n        if pos < 0 or len(self.target) <= pos:\n            return \" \"\n        else:\n            return self.charmap[self.target[pos]]\n\n    def render_observation(self):\n        \"\"\"Return a string representation of the input tape/grid.\"\"\"\n        raise NotImplementedError\n\n    def render(self, mode='human'):\n        outfile = StringIO() if mode == 'ansi' else sys.stdout\n        inp = \"Total length of input instance: %d, step: %d\\n\" % (self.input_width, self.time)\n        outfile.write(inp)\n        x, y, action = self.read_head_position, self.write_head_position, self.last_action\n        if action is not None:\n            inp_act, out_act, pred = action\n        outfile.write(\"=\" * (len(inp) - 1) + \"\\n\")\n        y_str =      \"Output Tape         : \"\n        target_str = \"Targets             : \"\n        if action is not None:\n            pred_str = self.charmap[pred]\n        x_str = self.render_observation()\n        for i in range(-2, len(self.target) + 2):\n            target_str += self._get_str_target(i)\n            if i < y - 1:\n                y_str += self._get_str_target(i)\n            elif i == (y - 1):\n                if action is not None and out_act == 1:\n                    color = 'green' if pred == self.target[i] else 'red'\n                    y_str += colorize(pred_str, color, highlight=True)\n                else:\n                    y_str += self._get_str_target(i)\n        outfile.write(x_str)\n        outfile.write(y_str + \"\\n\")\n        outfile.write(target_str + \"\\n\\n\")\n\n        if action is not None:\n            outfile.write(\"Current reward      :   %.3f\\n\" % self.last_reward)\n            outfile.write(\"Cumulative reward   :   %.3f\\n\" % self.episode_total_reward)\n            move = self.MOVEMENTS[inp_act]\n            outfile.write(\"Action              :   Tuple(move over input: %s,\\n\" % move)\n            out_act = out_act == 1\n            outfile.write(\"                              write to the output tape: %s,\\n\" % out_act)\n            outfile.write(\"                              prediction: %s)\\n\" % pred_str)\n        else:\n            outfile.write(\"\\n\" * 5)\n\n        if mode != 'human':\n            with closing(outfile):\n                return outfile.getvalue()\n\n    @property\n    def input_width(self):\n        return len(self.input_data)\n\n    def step(self, action):\n        assert self.action_space.contains(action)\n        self.last_action = action\n        inp_act, out_act, pred = action\n        done = False\n        reward = 0.0\n        self.time += 1\n        assert 0 <= self.write_head_position\n        if out_act == 1:\n            try:\n                correct = pred == self.target[self.write_head_position]\n            except IndexError:\n                logger.warn(\n                    \"It looks like you're calling step() even though this \"\n                    \"environment has already returned done=True. You should \"\n                    \"always call reset() once you receive done=True. Any \"\n                    \"further steps are undefined behaviour.\")\n                correct = False\n            if correct:\n                reward = 1.0\n            else:\n                # Bail as soon as a wrong character is written to the tape\n                reward = -0.5\n                done = True\n            self.write_head_position += 1\n            if self.write_head_position >= len(self.target):\n                done = True\n        self._move(inp_act)\n        if self.time > self.time_limit:\n            reward = -1.0\n            done = True\n        obs = self._get_obs()\n        self.last_reward = reward\n        self.episode_total_reward += reward\n        return (obs, reward, done, {})\n\n    @property\n    def time_limit(self):\n        \"\"\"If an agent takes more than this many timesteps, end the episode\n        immediately and return a negative reward.\"\"\"\n        # (Seemingly arbitrary)\n        return self.input_width + len(self.target) + 4\n\n    def _check_levelup(self):\n        \"\"\"Called between episodes. Update our running record of episode rewards\n        and, if appropriate, 'level up' minimum input length.\"\"\"\n        if self.episode_total_reward is None:\n            # This is before the first episode/call to reset(). Nothing to do\n            return\n        AlgorithmicEnv.reward_shortfalls.append(self.episode_total_reward - len(self.target))\n        AlgorithmicEnv.reward_shortfalls = AlgorithmicEnv.reward_shortfalls[-self.last:]\n        if len(AlgorithmicEnv.reward_shortfalls) == self.last and \\\n                min(AlgorithmicEnv.reward_shortfalls) >= self.MIN_REWARD_SHORTFALL_FOR_PROMOTION and \\\n                AlgorithmicEnv.min_length < 30:\n            AlgorithmicEnv.min_length += 1\n            AlgorithmicEnv.reward_shortfalls = []\n\n    def reset(self):\n        self._check_levelup()\n        self.last_action = None\n        self.last_reward = 0\n        self.read_head_position = self.READ_HEAD_START\n        self.write_head_position = 0\n        self.episode_total_reward = 0.0\n        self.time = 0\n        length = self.np_random.randint(3) + AlgorithmicEnv.min_length\n        self.input_data = self.generate_input_data(length)\n        self.target = self.target_from_input_data(self.input_data)\n        return self._get_obs()\n\n    def generate_input_data(self, size):\n        raise NotImplementedError\n\n    def target_from_input_data(self, input_data):\n        raise NotImplementedError(\"Subclasses must implement\")\n\n    def _move(self, movement):\n        raise NotImplementedError\n\n\nclass TapeAlgorithmicEnv(AlgorithmicEnv):\n    \"\"\"An algorithmic env with a 1-d input tape.\"\"\"\n    MOVEMENTS = ['left', 'right']\n    READ_HEAD_START = 0\n\n    def _move(self, movement):\n        named = self.MOVEMENTS[movement]\n        self.read_head_position += 1 if named == 'right' else -1\n\n    def _get_obs(self, pos=None):\n        if pos is None:\n            pos = self.read_head_position\n        if pos < 0:\n            return self.base\n        if isinstance(pos, np.ndarray):\n            pos = pos.item()\n        try:\n            return self.input_data[pos]\n        except IndexError:\n            return self.base\n\n    def generate_input_data(self, size):\n        return [self.np_random.randint(self.base) for _ in range(size)]\n\n    def render_observation(self):\n        x = self.read_head_position\n        x_str = \"Observation Tape    : \"\n        for i in range(-2, self.input_width + 2):\n            if i == x:\n                x_str += colorize(self._get_str_obs(np.array([i])), 'green', highlight=True)\n            else:\n                x_str += self._get_str_obs(np.array([i]))\n        x_str += \"\\n\"\n        return x_str\n\n\nclass GridAlgorithmicEnv(AlgorithmicEnv):\n    \"\"\"An algorithmic env with a 2-d input grid.\"\"\"\n    MOVEMENTS = ['left', 'right', 'up', 'down']\n    READ_HEAD_START = (0, 0)\n\n    def __init__(self, rows, *args, **kwargs):\n        self.rows = rows\n        AlgorithmicEnv.__init__(self, *args, **kwargs)\n\n    def _move(self, movement):\n        named = self.MOVEMENTS[movement]\n        x, y = self.read_head_position\n        if named == 'left':\n            x -= 1\n        elif named == 'right':\n            x += 1\n        elif named == 'up':\n            y -= 1\n        elif named == 'down':\n            y += 1\n        else:\n            raise ValueError(\"Unrecognized direction: {}\".format(named))\n        self.read_head_position = x, y\n\n    def generate_input_data(self, size):\n        return [\n            [self.np_random.randint(self.base) for _ in range(self.rows)]\n            for __ in range(size)\n        ]\n\n    def _get_obs(self, pos=None):\n        if pos is None:\n            pos = self.read_head_position\n        x, y = pos\n        if any(idx < 0 for idx in pos):\n            return self.base\n        try:\n            return self.input_data[x][y]\n        except IndexError:\n            return self.base\n\n    def render_observation(self):\n        x = self.read_head_position\n        label = \"Observation Grid    : \"\n        x_str = \"\"\n        for j in range(-1, self.rows+1):\n            if j != -1:\n                x_str += \" \" * len(label)\n            for i in range(-2, self.input_width + 2):\n                if i == x[0] and j == x[1]:\n                    x_str += colorize(self._get_str_obs((i, j)), 'green', highlight=True)\n                else:\n                    x_str += self._get_str_obs((i, j))\n            x_str += \"\\n\"\n        x_str = label + x_str\n        return x_str",
        "\"\"\"\nTask is to copy content multiple times from the input tape to\nthe output tape. http://arxiv.org/abs/1511.07275\n\"\"\"\nfrom gym.envs.algorithmic import algorithmic_env\n\n\nclass RepeatCopyEnv(algorithmic_env.TapeAlgorithmicEnv):\n    MIN_REWARD_SHORTFALL_FOR_PROMOTION = -.1\n\n    def __init__(self, base=5):\n        super(RepeatCopyEnv, self).__init__(base=base, chars=True)\n        self.last = 50\n\n    def target_from_input_data(self, input_data):\n        return input_data + list(reversed(input_data)) + input_data",
        "import pytest\ntry:\n    import Box2D\n    from .lunar_lander import LunarLander, LunarLanderContinuous, demo_heuristic_lander\nexcept ImportError:\n    Box2D = None\n\n\n@pytest.mark.skipif(Box2D is None, reason='Box2D not installed')\ndef test_lunar_lander():\n    _test_lander(LunarLander(), seed=0)\n\n@pytest.mark.skipif(Box2D is None, reason='Box2D not installed')\ndef test_lunar_lander_continuous():\n    _test_lander(LunarLanderContinuous(), seed=0)\n\n@pytest.mark.skipif(Box2D is None, reason='Box2D not installed')\ndef _test_lander(env, seed=None, render=False):\n    total_reward = demo_heuristic_lander(env, seed=seed, render=render)\n    assert total_reward > 100\n\n",
        "try:\n    import Box2D\n    from gym.envs.box2d.lunar_lander import LunarLander\n    from gym.envs.box2d.lunar_lander import LunarLanderContinuous\n    from gym.envs.box2d.bipedal_walker import BipedalWalker, BipedalWalkerHardcore\n    from gym.envs.box2d.car_racing import CarRacing\nexcept ImportError:\n    Box2D = None",
        "\"\"\"\n\nEasiest continuous control task to learn from pixels, a top-down racing environment.\nDiscrete control is reasonable in this environment as well, on/off discretization is\nfine.\n\nState consists of STATE_W x STATE_H pixels.\n\nThe reward is -0.1 every frame and +1000/N for every track tile visited, where N is\nthe total number of tiles visited in the track. For example, if you have finished in 732 frames,\nyour reward is 1000 - 0.1*732 = 926.8 points.\n\nThe game is solved when the agent consistently gets 900+ points. The generated track is random every episode.\n\nThe episode finishes when all the tiles are visited. The car also can go outside of the PLAYFIELD -  that\nis far off the track, then it will get -100 and die.\n\nSome indicators are shown at the bottom of the window along with the state RGB buffer. From\nleft to right: the true speed, four ABS sensors, the steering wheel position and gyroscope.\n\nTo play yourself (it's rather fast for humans), type:\n\npython gym/envs/box2d/car_racing.py\n\nRemember it's a powerful rear-wheel drive car -  don't press the accelerator and turn at the\nsame time.\n\nCreated by Oleg Klimov. Licensed on the same terms as the rest of OpenAI Gym.\n\"\"\"\n\nimport sys, math\nimport numpy as np\n\nimport Box2D\nfrom Box2D.b2 import (edgeShape, circleShape, fixtureDef, polygonShape, revoluteJointDef, contactListener)\n\nimport gym\nfrom gym import spaces\nfrom gym.envs.box2d.car_dynamics import Car\nfrom gym.utils import colorize, seeding, EzPickle\n\nimport pyglet\nfrom pyglet import gl\n\nSTATE_W = 96   # less than Atari 160x192\nSTATE_H = 96\nVIDEO_W = 600\nVIDEO_H = 400\nWINDOW_W = 1000\nWINDOW_H = 800\n\nSCALE = 6.0             # Track scale\nTRACK_RAD = 900/SCALE   # Track is heavily morphed circle with this radius\nPLAYFIELD = 2000/SCALE  # Game over boundary\nFPS = 50                # Frames per second\nZOOM = 2.7              # Camera zoom\nZOOM_FOLLOW = True      # Set to False for fixed view (don't use zoom)\n\n\nTRACK_DETAIL_STEP = 21/SCALE\nTRACK_TURN_RATE = 0.31\nTRACK_WIDTH = 40/SCALE\nBORDER = 8/SCALE\nBORDER_MIN_COUNT = 4\n\nROAD_COLOR = [0.4, 0.4, 0.4]\n\n\nclass FrictionDetector(contactListener):\n    def __init__(self, env):\n        contactListener.__init__(self)\n        self.env = env\n\n    def BeginContact(self, contact):\n        self._contact(contact, True)\n\n    def EndContact(self, contact):\n        self._contact(contact, False)\n\n    def _contact(self, contact, begin):\n        tile = None\n        obj = None\n        u1 = contact.fixtureA.body.userData\n        u2 = contact.fixtureB.body.userData\n        if u1 and \"road_friction\" in u1.__dict__:\n            tile = u1\n            obj = u2\n        if u2 and \"road_friction\" in u2.__dict__:\n            tile = u2\n            obj = u1\n        if not tile:\n            return\n\n        tile.color[0] = ROAD_COLOR[0]\n        tile.color[1] = ROAD_COLOR[1]\n        tile.color[2] = ROAD_COLOR[2]\n        if not obj or \"tiles\" not in obj.__dict__:\n            return\n        if begin:\n            obj.tiles.add(tile)\n            if not tile.road_visited:\n                tile.road_visited = True\n                self.env.reward += 1000.0/len(self.env.track)\n                self.env.tile_visited_count += 1\n        else:\n            obj.tiles.remove(tile)\n\nclass CarRacing(gym.Env, EzPickle):\n    metadata = {\n        'render.modes': ['human', 'rgb_array', 'state_pixels'],\n        'video.frames_per_second' : FPS\n    }\n\n    def __init__(self, verbose=1):\n        EzPickle.__init__(self)\n        self.seed()\n        self.contactListener_keepref = FrictionDetector(self)\n        self.world = Box2D.b2World((0,0), contactListener=self.contactListener_keepref)\n        self.viewer = None\n        self.invisible_state_window = None\n        self.invisible_video_window = None\n        self.road = None\n        self.car = None\n        self.reward = 0.0\n        self.prev_reward = 0.0\n        self.verbose = verbose\n        self.fd_tile = fixtureDef(\n                shape=polygonShape(vertices=[(0, 0), (1, 0), (1, -1), (0, -1)]))\n\n        self.action_space = spaces.Box(np.array([-1, 0, 0]),\n                                       np.array([+1, +1, +1]),\n                                       dtype=np.float32)  # steer, gas, brake\n\n        self.observation_space = spaces.Box(low=0, high=255, shape=(STATE_H, STATE_W, 3), dtype=np.uint8)\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def _destroy(self):\n        if not self.road:\n            return\n        for t in self.road:\n            self.world.DestroyBody(t)\n        self.road = []\n        self.car.destroy()\n\n    def _create_track(self):\n        CHECKPOINTS = 12\n\n        # Create checkpoints\n        checkpoints = []\n        for c in range(CHECKPOINTS):\n            alpha = 2*math.pi*c/CHECKPOINTS + self.np_random.uniform(0, 2*math.pi*1/CHECKPOINTS)\n            rad = self.np_random.uniform(TRACK_RAD/3, TRACK_RAD)\n            if c == 0:\n                alpha = 0\n                rad = 1.5*TRACK_RAD\n            if c == CHECKPOINTS-1:\n                alpha = 2*math.pi*c/CHECKPOINTS\n                self.start_alpha = 2*math.pi*(-0.5)/CHECKPOINTS\n                rad = 1.5*TRACK_RAD\n            checkpoints.append((alpha, rad*math.cos(alpha), rad*math.sin(alpha)))\n        self.road = []\n\n        # Go from one checkpoint to another to create track\n        x, y, beta = 1.5*TRACK_RAD, 0, 0\n        dest_i = 0\n        laps = 0\n        track = []\n        no_freeze = 2500\n        visited_other_side = False\n        while True:\n            alpha = math.atan2(y, x)\n            if visited_other_side and alpha > 0:\n                laps += 1\n                visited_other_side = False\n            if alpha < 0:\n                visited_other_side = True\n                alpha += 2*math.pi\n            while True: # Find destination from checkpoints\n                failed = True\n                while True:\n                    dest_alpha, dest_x, dest_y = checkpoints[dest_i % len(checkpoints)]\n                    if alpha <= dest_alpha:\n                        failed = False\n                        break\n                    dest_i += 1\n                    if dest_i % len(checkpoints) == 0:\n                        break\n                if not failed:\n                    break\n                alpha -= 2*math.pi\n                continue\n            r1x = math.cos(beta)\n            r1y = math.sin(beta)\n            p1x = -r1y\n            p1y = r1x\n            dest_dx = dest_x - x  # vector towards destination\n            dest_dy = dest_y - y\n            proj = r1x*dest_dx + r1y*dest_dy  # destination vector projected on rad\n            while beta - alpha > 1.5*math.pi:\n                 beta -= 2*math.pi\n            while beta - alpha < -1.5*math.pi:\n                 beta += 2*math.pi\n            prev_beta = beta\n            proj *= SCALE\n            if proj > 0.3:\n                 beta -= min(TRACK_TURN_RATE, abs(0.001*proj))\n            if proj < -0.3:\n                 beta += min(TRACK_TURN_RATE, abs(0.001*proj))\n            x += p1x*TRACK_DETAIL_STEP\n            y += p1y*TRACK_DETAIL_STEP\n            track.append((alpha,prev_beta*0.5 + beta*0.5,x,y))\n            if laps > 4:\n                 break\n            no_freeze -= 1\n            if no_freeze == 0:\n                 break\n\n        # Find closed loop range i1..i2, first loop should be ignored, second is OK\n        i1, i2 = -1, -1\n        i = len(track)\n        while True:\n            i -= 1\n            if i == 0:\n                return False  # Failed\n            pass_through_start = track[i][0] > self.start_alpha and track[i-1][0] <= self.start_alpha\n            if pass_through_start and i2 == -1:\n                i2 = i\n            elif pass_through_start and i1 == -1:\n                i1 = i\n                break\n        if self.verbose == 1:\n            print(\"Track generation: %i..%i -> %i-tiles track\" % (i1, i2, i2-i1))\n        assert i1 != -1\n        assert i2 != -1\n\n        track = track[i1:i2-1]\n\n        first_beta = track[0][1]\n        first_perp_x = math.cos(first_beta)\n        first_perp_y = math.sin(first_beta)\n        # Length of perpendicular jump to put together head and tail\n        well_glued_together = np.sqrt(\n            np.square(first_perp_x*(track[0][2] - track[-1][2])) +\n            np.square(first_perp_y*(track[0][3] - track[-1][3])))\n        if well_glued_together > TRACK_DETAIL_STEP:\n            return False\n\n        # Red-white border on hard turns\n        border = [False]*len(track)\n        for i in range(len(track)):\n            good = True\n            oneside = 0\n            for neg in range(BORDER_MIN_COUNT):\n                beta1 = track[i-neg-0][1]\n                beta2 = track[i-neg-1][1]\n                good &= abs(beta1 - beta2) > TRACK_TURN_RATE*0.2\n                oneside += np.sign(beta1 - beta2)\n            good &= abs(oneside) == BORDER_MIN_COUNT\n            border[i] = good\n        for i in range(len(track)):\n            for neg in range(BORDER_MIN_COUNT):\n                border[i-neg] |= border[i]\n\n        # Create tiles\n        for i in range(len(track)):\n            alpha1, beta1, x1, y1 = track[i]\n            alpha2, beta2, x2, y2 = track[i-1]\n            road1_l = (x1 - TRACK_WIDTH*math.cos(beta1), y1 - TRACK_WIDTH*math.sin(beta1))\n            road1_r = (x1 + TRACK_WIDTH*math.cos(beta1), y1 + TRACK_WIDTH*math.sin(beta1))\n            road2_l = (x2 - TRACK_WIDTH*math.cos(beta2), y2 - TRACK_WIDTH*math.sin(beta2))\n            road2_r = (x2 + TRACK_WIDTH*math.cos(beta2), y2 + TRACK_WIDTH*math.sin(beta2))\n            vertices = [road1_l, road1_r, road2_r, road2_l]\n            self.fd_tile.shape.vertices = vertices\n            t = self.world.CreateStaticBody(fixtures=self.fd_tile)\n            t.userData = t\n            c = 0.01*(i%3)\n            t.color = [ROAD_COLOR[0] + c, ROAD_COLOR[1] + c, ROAD_COLOR[2] + c]\n            t.road_visited = False\n            t.road_friction = 1.0\n            t.fixtures[0].sensor = True\n            self.road_poly.append(( [road1_l, road1_r, road2_r, road2_l], t.color ))\n            self.road.append(t)\n            if border[i]:\n                side = np.sign(beta2 - beta1)\n                b1_l = (x1 + side * TRACK_WIDTH * math.cos(beta1), y1 + side * TRACK_WIDTH * math.sin(beta1))\n                b1_r = (x1 + side * (TRACK_WIDTH+BORDER) * math.cos(beta1),\n                        y1 + side * (TRACK_WIDTH+BORDER)*math.sin(beta1))\n                b2_l = (x2 + side * TRACK_WIDTH * math.cos(beta2), y2 + side * TRACK_WIDTH * math.sin(beta2))\n                b2_r = (x2 + side * (TRACK_WIDTH+BORDER) * math.cos(beta2),\n                        y2 + side * (TRACK_WIDTH+BORDER) * math.sin(beta2))\n                self.road_poly.append(([b1_l, b1_r, b2_r, b2_l], (1, 1, 1) if i % 2 == 0 else (1, 0, 0)))\n        self.track = track\n        return True\n\n    def reset(self):\n        self._destroy()\n        self.reward = 0.0\n        self.prev_reward = 0.0\n        self.tile_visited_count = 0\n        self.t = 0.0\n        self.road_poly = []\n\n        while True:\n            success = self._create_track()\n            if success:\n                break\n            if self.verbose == 1:\n                print(\"retry to generate track (normal if there are not many instances of this message)\")\n        self.car = Car(self.world, *self.track[0][1:4])\n\n        return self.step(None)[0]\n\n    def step(self, action):\n        if action is not None:\n            self.car.steer(-action[0])\n            self.car.gas(action[1])\n            self.car.brake(action[2])\n\n        self.car.step(1.0/FPS)\n        self.world.Step(1.0/FPS, 6*30, 2*30)\n        self.t += 1.0/FPS\n\n        self.state = self.render(\"state_pixels\")\n\n        step_reward = 0\n        done = False\n        if action is not None: # First step without action, called from reset()\n            self.reward -= 0.1\n            # We actually don't want to count fuel spent, we want car to be faster.\n            # self.reward -=  10 * self.car.fuel_spent / ENGINE_POWER\n            self.car.fuel_spent = 0.0\n            step_reward = self.reward - self.prev_reward\n            self.prev_reward = self.reward\n            if self.tile_visited_count == len(self.track):\n                done = True\n            x, y = self.car.hull.position\n            if abs(x) > PLAYFIELD or abs(y) > PLAYFIELD:\n                done = True\n                step_reward = -100\n\n        return self.state, step_reward, done, {}\n\n    def render(self, mode='human'):\n        assert mode in ['human', 'state_pixels', 'rgb_array']\n        if self.viewer is None:\n            from gym.envs.classic_control import rendering\n            self.viewer = rendering.Viewer(WINDOW_W, WINDOW_H)\n            self.score_label = pyglet.text.Label('0000', font_size=36,\n                x=20, y=WINDOW_H*2.5/40.00, anchor_x='left', anchor_y='center',\n                color=(255,255,255,255))\n            self.transform = rendering.Transform()\n\n        if \"t\" not in self.__dict__: return  # reset() not called yet\n\n        zoom = 0.1*SCALE*max(1-self.t, 0) + ZOOM*SCALE*min(self.t, 1)   # Animate zoom first second\n        scroll_x = self.car.hull.position[0]\n        scroll_y = self.car.hull.position[1]\n        angle = -self.car.hull.angle\n        vel = self.car.hull.linearVelocity\n        if np.linalg.norm(vel) > 0.5:\n            angle = math.atan2(vel[0], vel[1])\n        self.transform.set_scale(zoom, zoom)\n        self.transform.set_translation(\n            WINDOW_W/2 - (scroll_x*zoom*math.cos(angle) - scroll_y*zoom*math.sin(angle)),\n            WINDOW_H/4 - (scroll_x*zoom*math.sin(angle) + scroll_y*zoom*math.cos(angle)))\n        self.transform.set_rotation(angle)\n\n        self.car.draw(self.viewer, mode != \"state_pixels\")\n\n        arr = None\n        win = self.viewer.window\n        win.switch_to()\n        win.dispatch_events()\n\n        win.clear()\n        t = self.transform\n        if mode == 'rgb_array':\n            VP_W = VIDEO_W\n            VP_H = VIDEO_H\n        elif mode == 'state_pixels':\n            VP_W = STATE_W\n            VP_H = STATE_H\n        else:\n            pixel_scale = 1\n            if hasattr(win.context, '_nscontext'):\n                pixel_scale = win.context._nscontext.view().backingScaleFactor()  # pylint: disable=protected-access\n            VP_W = int(pixel_scale * WINDOW_W)\n            VP_H = int(pixel_scale * WINDOW_H)\n\n        gl.glViewport(0, 0, VP_W, VP_H)\n        t.enable()\n        self.render_road()\n        for geom in self.viewer.onetime_geoms:\n            geom.render()\n        self.viewer.onetime_geoms = []\n        t.disable()\n        self.render_indicators(WINDOW_W, WINDOW_H)\n\n        if mode == 'human':\n            win.flip()\n            return self.viewer.isopen\n\n        image_data = pyglet.image.get_buffer_manager().get_color_buffer().get_image_data()\n        arr = np.fromstring(image_data.get_data(), dtype=np.uint8, sep='')\n        arr = arr.reshape(VP_H, VP_W, 4)\n        arr = arr[::-1, :, 0:3]\n\n        return arr\n\n    def close(self):\n        if self.viewer is not None:\n            self.viewer.close()\n            self.viewer = None\n\n    def render_road(self):\n        gl.glBegin(gl.GL_QUADS)\n        gl.glColor4f(0.4, 0.8, 0.4, 1.0)\n        gl.glVertex3f(-PLAYFIELD, +PLAYFIELD, 0)\n        gl.glVertex3f(+PLAYFIELD, +PLAYFIELD, 0)\n        gl.glVertex3f(+PLAYFIELD, -PLAYFIELD, 0)\n        gl.glVertex3f(-PLAYFIELD, -PLAYFIELD, 0)\n        gl.glColor4f(0.4, 0.9, 0.4, 1.0)\n        k = PLAYFIELD/20.0\n        for x in range(-20, 20, 2):\n            for y in range(-20, 20, 2):\n                gl.glVertex3f(k*x + k, k*y + 0, 0)\n                gl.glVertex3f(k*x + 0, k*y + 0, 0)\n                gl.glVertex3f(k*x + 0, k*y + k, 0)\n                gl.glVertex3f(k*x + k, k*y + k, 0)\n        for poly, color in self.road_poly:\n            gl.glColor4f(color[0], color[1], color[2], 1)\n            for p in poly:\n                gl.glVertex3f(p[0], p[1], 0)\n        gl.glEnd()\n\n    def render_indicators(self, W, H):\n        gl.glBegin(gl.GL_QUADS)\n        s = W/40.0\n        h = H/40.0\n        gl.glColor4f(0, 0, 0, 1)\n        gl.glVertex3f(W, 0, 0)\n        gl.glVertex3f(W, 5*h, 0)\n        gl.glVertex3f(0, 5*h, 0)\n        gl.glVertex3f(0, 0, 0)\n\n        def vertical_ind(place, val, color):\n            gl.glColor4f(color[0], color[1], color[2], 1)\n            gl.glVertex3f((place+0)*s, h + h*val, 0)\n            gl.glVertex3f((place+1)*s, h + h*val, 0)\n            gl.glVertex3f((place+1)*s, h, 0)\n            gl.glVertex3f((place+0)*s, h, 0)\n\n        def horiz_ind(place, val, color):\n            gl.glColor4f(color[0], color[1], color[2], 1)\n            gl.glVertex3f((place+0)*s, 4*h , 0)\n            gl.glVertex3f((place+val)*s, 4*h, 0)\n            gl.glVertex3f((place+val)*s, 2*h, 0)\n            gl.glVertex3f((place+0)*s, 2*h, 0)\n        true_speed = np.sqrt(np.square(self.car.hull.linearVelocity[0]) + np.square(self.car.hull.linearVelocity[1]))\n        vertical_ind(5, 0.02*true_speed, (1, 1, 1))\n        vertical_ind(7, 0.01*self.car.wheels[0].omega, (0.0, 0, 1)) # ABS sensors\n        vertical_ind(8, 0.01*self.car.wheels[1].omega, (0.0, 0, 1))\n        vertical_ind(9, 0.01*self.car.wheels[2].omega, (0.2, 0, 1))\n        vertical_ind(10,0.01*self.car.wheels[3].omega, (0.2, 0, 1))\n        horiz_ind(20, -10.0*self.car.wheels[0].joint.angle, (0, 1, 0))\n        horiz_ind(30, -0.8*self.car.hull.angularVelocity, (1, 0, 0))\n        gl.glEnd()\n        self.score_label.text = \"%04i\" % self.reward\n        self.score_label.draw()\n\n\nif __name__==\"__main__\":\n    from pyglet.window import key\n    a = np.array([0.0, 0.0, 0.0])\n\n    def key_press(k, mod):\n        global restart\n        if k == 0xff0d: restart = True\n        if k == key.LEFT:  a[0] = -1.0\n        if k == key.RIGHT: a[0] = +1.0\n        if k == key.UP:    a[1] = +1.0\n        if k == key.DOWN:  a[2] = +0.8   # set 1.0 for wheels to block to zero rotation\n\n    def key_release(k, mod):\n        if k == key.LEFT  and a[0] == -1.0: a[0] = 0\n        if k == key.RIGHT and a[0] == +1.0: a[0] = 0\n        if k == key.UP:    a[1] = 0\n        if k == key.DOWN:  a[2] = 0\n    env = CarRacing()\n    env.render()\n    env.viewer.window.on_key_press = key_press\n    env.viewer.window.on_key_release = key_release\n    record_video = False\n    if record_video:\n        from gym.wrappers.monitor import Monitor\n        env = Monitor(env, '/tmp/video-test', force=True)\n    isopen = True\n    while isopen:\n        env.reset()\n        total_reward = 0.0\n        steps = 0\n        restart = False\n        while True:\n            s, r, done, info = env.step(a)\n            total_reward += r\n            if steps % 200 == 0 or done:\n                print(\"\\naction \" + str([\"{:+0.2f}\".format(x) for x in a]))\n                print(\"step {} total_reward {:+0.2f}\".format(steps, total_reward))\n            steps += 1\n            isopen = env.render()\n            if done or restart or isopen == False:\n                break\n    env.close()",
        "import sys\nimport math\n\nimport numpy as np\nimport Box2D\nfrom Box2D.b2 import (edgeShape, circleShape, fixtureDef, polygonShape, revoluteJointDef, contactListener)\n\nimport gym\nfrom gym import spaces\nfrom gym.utils import colorize, seeding, EzPickle\n\n# This is simple 4-joints walker robot environment.\n#\n# There are two versions:\n#\n# - Normal, with slightly uneven terrain.\n#\n# - Hardcore with ladders, stumps, pitfalls.\n#\n# Reward is given for moving forward, total 300+ points up to the far end. If the robot falls,\n# it gets -100. Applying motor torque costs a small amount of points, more optimal agent\n# will get better score.\n#\n# Heuristic is provided for testing, it's also useful to get demonstrations to\n# learn from. To run heuristic:\n#\n# python gym/envs/box2d/bipedal_walker.py\n#\n# State consists of hull angle speed, angular velocity, horizontal speed, vertical speed,\n# position of joints and joints angular speed, legs contact with ground, and 10 lidar\n# rangefinder measurements to help to deal with the hardcore version. There's no coordinates\n# in the state vector. Lidar is less useful in normal version, but it works.\n#\n# To solve the game you need to get 300 points in 1600 time steps.\n#\n# To solve hardcore version you need 300 points in 2000 time steps.\n#\n# Created by Oleg Klimov. Licensed on the same terms as the rest of OpenAI Gym.\n\nFPS    = 50\nSCALE  = 30.0   # affects how fast-paced the game is, forces should be adjusted as well\n\nMOTORS_TORQUE = 80\nSPEED_HIP     = 4\nSPEED_KNEE    = 6\nLIDAR_RANGE   = 160/SCALE\n\nINITIAL_RANDOM = 5\n\nHULL_POLY =[\n    (-30,+9), (+6,+9), (+34,+1),\n    (+34,-8), (-30,-8)\n    ]\nLEG_DOWN = -8/SCALE\nLEG_W, LEG_H = 8/SCALE, 34/SCALE\n\nVIEWPORT_W = 600\nVIEWPORT_H = 400\n\nTERRAIN_STEP   = 14/SCALE\nTERRAIN_LENGTH = 200     # in steps\nTERRAIN_HEIGHT = VIEWPORT_H/SCALE/4\nTERRAIN_GRASS    = 10    # low long are grass spots, in steps\nTERRAIN_STARTPAD = 20    # in steps\nFRICTION = 2.5\n\nHULL_FD = fixtureDef(\n                shape=polygonShape(vertices=[ (x/SCALE,y/SCALE) for x,y in HULL_POLY ]),\n                density=5.0,\n                friction=0.1,\n                categoryBits=0x0020,\n                maskBits=0x001,  # collide only with ground\n                restitution=0.0) # 0.99 bouncy\n\nLEG_FD = fixtureDef(\n                    shape=polygonShape(box=(LEG_W/2, LEG_H/2)),\n                    density=1.0,\n                    restitution=0.0,\n                    categoryBits=0x0020,\n                    maskBits=0x001)\n\nLOWER_FD = fixtureDef(\n                    shape=polygonShape(box=(0.8*LEG_W/2, LEG_H/2)),\n                    density=1.0,\n                    restitution=0.0,\n                    categoryBits=0x0020,\n                    maskBits=0x001)\n\nclass ContactDetector(contactListener):\n    def __init__(self, env):\n        contactListener.__init__(self)\n        self.env = env\n    def BeginContact(self, contact):\n        if self.env.hull==contact.fixtureA.body or self.env.hull==contact.fixtureB.body:\n            self.env.game_over = True\n        for leg in [self.env.legs[1], self.env.legs[3]]:\n            if leg in [contact.fixtureA.body, contact.fixtureB.body]:\n                leg.ground_contact = True\n    def EndContact(self, contact):\n        for leg in [self.env.legs[1], self.env.legs[3]]:\n            if leg in [contact.fixtureA.body, contact.fixtureB.body]:\n                leg.ground_contact = False\n\nclass BipedalWalker(gym.Env, EzPickle):\n    metadata = {\n        'render.modes': ['human', 'rgb_array'],\n        'video.frames_per_second' : FPS\n    }\n\n    hardcore = False\n\n    def __init__(self):\n        EzPickle.__init__(self)\n        self.seed()\n        self.viewer = None\n\n        self.world = Box2D.b2World()\n        self.terrain = None\n        self.hull = None\n\n        self.prev_shaping = None\n\n        self.fd_polygon = fixtureDef(\n                        shape = polygonShape(vertices=\n                        [(0, 0),\n                         (1, 0),\n                         (1, -1),\n                         (0, -1)]),\n                        friction = FRICTION)\n\n        self.fd_edge = fixtureDef(\n                    shape = edgeShape(vertices=\n                    [(0, 0),\n                     (1, 1)]),\n                    friction = FRICTION,\n                    categoryBits=0x0001,\n                )\n\n        self.reset()\n\n        high = np.array([np.inf] * 24)\n        self.action_space = spaces.Box(np.array([-1, -1, -1, -1]), np.array([1, 1, 1, 1]), dtype=np.float32)\n        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def _destroy(self):\n        if not self.terrain: return\n        self.world.contactListener = None\n        for t in self.terrain:\n            self.world.DestroyBody(t)\n        self.terrain = []\n        self.world.DestroyBody(self.hull)\n        self.hull = None\n        for leg in self.legs:\n            self.world.DestroyBody(leg)\n        self.legs = []\n        self.joints = []\n\n    def _generate_terrain(self, hardcore):\n        GRASS, STUMP, STAIRS, PIT, _STATES_ = range(5)\n        state    = GRASS\n        velocity = 0.0\n        y        = TERRAIN_HEIGHT\n        counter  = TERRAIN_STARTPAD\n        oneshot  = False\n        self.terrain   = []\n        self.terrain_x = []\n        self.terrain_y = []\n        for i in range(TERRAIN_LENGTH):\n            x = i*TERRAIN_STEP\n            self.terrain_x.append(x)\n\n            if state==GRASS and not oneshot:\n                velocity = 0.8*velocity + 0.01*np.sign(TERRAIN_HEIGHT - y)\n                if i > TERRAIN_STARTPAD: velocity += self.np_random.uniform(-1, 1)/SCALE   #1\n                y += velocity\n\n            elif state==PIT and oneshot:\n                counter = self.np_random.randint(3, 5)\n                poly = [\n                    (x,              y),\n                    (x+TERRAIN_STEP, y),\n                    (x+TERRAIN_STEP, y-4*TERRAIN_STEP),\n                    (x,              y-4*TERRAIN_STEP),\n                    ]\n                self.fd_polygon.shape.vertices=poly\n                t = self.world.CreateStaticBody(\n                    fixtures = self.fd_polygon)\n                t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n                self.terrain.append(t)\n\n                self.fd_polygon.shape.vertices=[(p[0]+TERRAIN_STEP*counter,p[1]) for p in poly]\n                t = self.world.CreateStaticBody(\n                    fixtures = self.fd_polygon)\n                t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n                self.terrain.append(t)\n                counter += 2\n                original_y = y\n\n            elif state==PIT and not oneshot:\n                y = original_y\n                if counter > 1:\n                    y -= 4*TERRAIN_STEP\n\n            elif state==STUMP and oneshot:\n                counter = self.np_random.randint(1, 3)\n                poly = [\n                    (x,                      y),\n                    (x+counter*TERRAIN_STEP, y),\n                    (x+counter*TERRAIN_STEP, y+counter*TERRAIN_STEP),\n                    (x,                      y+counter*TERRAIN_STEP),\n                    ]\n                self.fd_polygon.shape.vertices=poly\n                t = self.world.CreateStaticBody(\n                    fixtures = self.fd_polygon)\n                t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n                self.terrain.append(t)\n\n            elif state==STAIRS and oneshot:\n                stair_height = +1 if self.np_random.rand() > 0.5 else -1\n                stair_width = self.np_random.randint(4, 5)\n                stair_steps = self.np_random.randint(3, 5)\n                original_y = y\n                for s in range(stair_steps):\n                    poly = [\n                        (x+(    s*stair_width)*TERRAIN_STEP, y+(   s*stair_height)*TERRAIN_STEP),\n                        (x+((1+s)*stair_width)*TERRAIN_STEP, y+(   s*stair_height)*TERRAIN_STEP),\n                        (x+((1+s)*stair_width)*TERRAIN_STEP, y+(-1+s*stair_height)*TERRAIN_STEP),\n                        (x+(    s*stair_width)*TERRAIN_STEP, y+(-1+s*stair_height)*TERRAIN_STEP),\n                        ]\n                    self.fd_polygon.shape.vertices=poly\n                    t = self.world.CreateStaticBody(\n                        fixtures = self.fd_polygon)\n                    t.color1, t.color2 = (1,1,1), (0.6,0.6,0.6)\n                    self.terrain.append(t)\n                counter = stair_steps*stair_width\n\n            elif state==STAIRS and not oneshot:\n                s = stair_steps*stair_width - counter - stair_height\n                n = s/stair_width\n                y = original_y + (n*stair_height)*TERRAIN_STEP\n\n            oneshot = False\n            self.terrain_y.append(y)\n            counter -= 1\n            if counter==0:\n                counter = self.np_random.randint(TERRAIN_GRASS/2, TERRAIN_GRASS)\n                if state==GRASS and hardcore:\n                    state = self.np_random.randint(1, _STATES_)\n                    oneshot = True\n                else:\n                    state = GRASS\n                    oneshot = True\n\n        self.terrain_poly = []\n        for i in range(TERRAIN_LENGTH-1):\n            poly = [\n                (self.terrain_x[i],   self.terrain_y[i]),\n                (self.terrain_x[i+1], self.terrain_y[i+1])\n                ]\n            self.fd_edge.shape.vertices=poly\n            t = self.world.CreateStaticBody(\n                fixtures = self.fd_edge)\n            color = (0.3, 1.0 if i%2==0 else 0.8, 0.3)\n            t.color1 = color\n            t.color2 = color\n            self.terrain.append(t)\n            color = (0.4, 0.6, 0.3)\n            poly += [ (poly[1][0], 0), (poly[0][0], 0) ]\n            self.terrain_poly.append( (poly, color) )\n        self.terrain.reverse()\n\n    def _generate_clouds(self):\n        # Sorry for the clouds, couldn't resist\n        self.cloud_poly   = []\n        for i in range(TERRAIN_LENGTH//20):\n            x = self.np_random.uniform(0, TERRAIN_LENGTH)*TERRAIN_STEP\n            y = VIEWPORT_H/SCALE*3/4\n            poly = [\n                (x+15*TERRAIN_STEP*math.sin(3.14*2*a/5)+self.np_random.uniform(0,5*TERRAIN_STEP),\n                 y+ 5*TERRAIN_STEP*math.cos(3.14*2*a/5)+self.np_random.uniform(0,5*TERRAIN_STEP) )\n                for a in range(5) ]\n            x1 = min( [p[0] for p in poly] )\n            x2 = max( [p[0] for p in poly] )\n            self.cloud_poly.append( (poly,x1,x2) )\n\n    def reset(self):\n        self._destroy()\n        self.world.contactListener_bug_workaround = ContactDetector(self)\n        self.world.contactListener = self.world.contactListener_bug_workaround\n        self.game_over = False\n        self.prev_shaping = None\n        self.scroll = 0.0\n        self.lidar_render = 0\n\n        W = VIEWPORT_W/SCALE\n        H = VIEWPORT_H/SCALE\n\n        self._generate_terrain(self.hardcore)\n        self._generate_clouds()\n\n        init_x = TERRAIN_STEP*TERRAIN_STARTPAD/2\n        init_y = TERRAIN_HEIGHT+2*LEG_H\n        self.hull = self.world.CreateDynamicBody(\n            position = (init_x, init_y),\n            fixtures = HULL_FD\n                )\n        self.hull.color1 = (0.5,0.4,0.9)\n        self.hull.color2 = (0.3,0.3,0.5)\n        self.hull.ApplyForceToCenter((self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM), 0), True)\n\n        self.legs = []\n        self.joints = []\n        for i in [-1,+1]:\n            leg = self.world.CreateDynamicBody(\n                position = (init_x, init_y - LEG_H/2 - LEG_DOWN),\n                angle = (i*0.05),\n                fixtures = LEG_FD\n                )\n            leg.color1 = (0.6-i/10., 0.3-i/10., 0.5-i/10.)\n            leg.color2 = (0.4-i/10., 0.2-i/10., 0.3-i/10.)\n            rjd = revoluteJointDef(\n                bodyA=self.hull,\n                bodyB=leg,\n                localAnchorA=(0, LEG_DOWN),\n                localAnchorB=(0, LEG_H/2),\n                enableMotor=True,\n                enableLimit=True,\n                maxMotorTorque=MOTORS_TORQUE,\n                motorSpeed = i,\n                lowerAngle = -0.8,\n                upperAngle = 1.1,\n                )\n            self.legs.append(leg)\n            self.joints.append(self.world.CreateJoint(rjd))\n\n            lower = self.world.CreateDynamicBody(\n                position = (init_x, init_y - LEG_H*3/2 - LEG_DOWN),\n                angle = (i*0.05),\n                fixtures = LOWER_FD\n                )\n            lower.color1 = (0.6-i/10., 0.3-i/10., 0.5-i/10.)\n            lower.color2 = (0.4-i/10., 0.2-i/10., 0.3-i/10.)\n            rjd = revoluteJointDef(\n                bodyA=leg,\n                bodyB=lower,\n                localAnchorA=(0, -LEG_H/2),\n                localAnchorB=(0, LEG_H/2),\n                enableMotor=True,\n                enableLimit=True,\n                maxMotorTorque=MOTORS_TORQUE,\n                motorSpeed = 1,\n                lowerAngle = -1.6,\n                upperAngle = -0.1,\n                )\n            lower.ground_contact = False\n            self.legs.append(lower)\n            self.joints.append(self.world.CreateJoint(rjd))\n\n        self.drawlist = self.terrain + self.legs + [self.hull]\n\n        class LidarCallback(Box2D.b2.rayCastCallback):\n            def ReportFixture(self, fixture, point, normal, fraction):\n                if (fixture.filterData.categoryBits & 1) == 0:\n                    return -1\n                self.p2 = point\n                self.fraction = fraction\n                return fraction\n        self.lidar = [LidarCallback() for _ in range(10)]\n\n        return self.step(np.array([0,0,0,0]))[0]\n\n    def step(self, action):\n        #self.hull.ApplyForceToCenter((0, 20), True) -- Uncomment this to receive a bit of stability help\n        control_speed = False  # Should be easier as well\n        if control_speed:\n            self.joints[0].motorSpeed = float(SPEED_HIP  * np.clip(action[0], -1, 1))\n            self.joints[1].motorSpeed = float(SPEED_KNEE * np.clip(action[1], -1, 1))\n            self.joints[2].motorSpeed = float(SPEED_HIP  * np.clip(action[2], -1, 1))\n            self.joints[3].motorSpeed = float(SPEED_KNEE * np.clip(action[3], -1, 1))\n        else:\n            self.joints[0].motorSpeed     = float(SPEED_HIP     * np.sign(action[0]))\n            self.joints[0].maxMotorTorque = float(MOTORS_TORQUE * np.clip(np.abs(action[0]), 0, 1))\n            self.joints[1].motorSpeed     = float(SPEED_KNEE    * np.sign(action[1]))\n            self.joints[1].maxMotorTorque = float(MOTORS_TORQUE * np.clip(np.abs(action[1]), 0, 1))\n            self.joints[2].motorSpeed     = float(SPEED_HIP     * np.sign(action[2]))\n            self.joints[2].maxMotorTorque = float(MOTORS_TORQUE * np.clip(np.abs(action[2]), 0, 1))\n            self.joints[3].motorSpeed     = float(SPEED_KNEE    * np.sign(action[3]))\n            self.joints[3].maxMotorTorque = float(MOTORS_TORQUE * np.clip(np.abs(action[3]), 0, 1))\n\n        self.world.Step(1.0/FPS, 6*30, 2*30)\n\n        pos = self.hull.position\n        vel = self.hull.linearVelocity\n\n        for i in range(10):\n            self.lidar[i].fraction = 1.0\n            self.lidar[i].p1 = pos\n            self.lidar[i].p2 = (\n                pos[0] + math.sin(1.5*i/10.0)*LIDAR_RANGE,\n                pos[1] - math.cos(1.5*i/10.0)*LIDAR_RANGE)\n            self.world.RayCast(self.lidar[i], self.lidar[i].p1, self.lidar[i].p2)\n\n        state = [\n            self.hull.angle,        # Normal angles up to 0.5 here, but sure more is possible.\n            2.0*self.hull.angularVelocity/FPS,\n            0.3*vel.x*(VIEWPORT_W/SCALE)/FPS,  # Normalized to get -1..1 range\n            0.3*vel.y*(VIEWPORT_H/SCALE)/FPS,\n            self.joints[0].angle,   # This will give 1.1 on high up, but it's still OK (and there should be spikes on hiting the ground, that's normal too)\n            self.joints[0].speed / SPEED_HIP,\n            self.joints[1].angle + 1.0,\n            self.joints[1].speed / SPEED_KNEE,\n            1.0 if self.legs[1].ground_contact else 0.0,\n            self.joints[2].angle,\n            self.joints[2].speed / SPEED_HIP,\n            self.joints[3].angle + 1.0,\n            self.joints[3].speed / SPEED_KNEE,\n            1.0 if self.legs[3].ground_contact else 0.0\n            ]\n        state += [l.fraction for l in self.lidar]\n        assert len(state)==24\n\n        self.scroll = pos.x - VIEWPORT_W/SCALE/5\n\n        shaping  = 130*pos[0]/SCALE   # moving forward is a way to receive reward (normalized to get 300 on completion)\n        shaping -= 5.0*abs(state[0])  # keep head straight, other than that and falling, any behavior is unpunished\n\n        reward = 0\n        if self.prev_shaping is not None:\n            reward = shaping - self.prev_shaping\n        self.prev_shaping = shaping\n\n        for a in action:\n            reward -= 0.00035 * MOTORS_TORQUE * np.clip(np.abs(a), 0, 1)\n            # normalized to about -50.0 using heuristic, more optimal agent should spend less\n\n        done = False\n        if self.game_over or pos[0] < 0:\n            reward = -100\n            done   = True\n        if pos[0] > (TERRAIN_LENGTH-TERRAIN_GRASS)*TERRAIN_STEP:\n            done   = True\n        return np.array(state), reward, done, {}\n\n    def render(self, mode='human'):\n        from gym.envs.classic_control import rendering\n        if self.viewer is None:\n            self.viewer = rendering.Viewer(VIEWPORT_W, VIEWPORT_H)\n        self.viewer.set_bounds(self.scroll, VIEWPORT_W/SCALE + self.scroll, 0, VIEWPORT_H/SCALE)\n\n        self.viewer.draw_polygon( [\n            (self.scroll,                  0),\n            (self.scroll+VIEWPORT_W/SCALE, 0),\n            (self.scroll+VIEWPORT_W/SCALE, VIEWPORT_H/SCALE),\n            (self.scroll,                  VIEWPORT_H/SCALE),\n            ], color=(0.9, 0.9, 1.0) )\n        for poly,x1,x2 in self.cloud_poly:\n            if x2 < self.scroll/2: continue\n            if x1 > self.scroll/2 + VIEWPORT_W/SCALE: continue\n            self.viewer.draw_polygon( [(p[0]+self.scroll/2, p[1]) for p in poly], color=(1,1,1))\n        for poly, color in self.terrain_poly:\n            if poly[1][0] < self.scroll: continue\n            if poly[0][0] > self.scroll + VIEWPORT_W/SCALE: continue\n            self.viewer.draw_polygon(poly, color=color)\n\n        self.lidar_render = (self.lidar_render+1) % 100\n        i = self.lidar_render\n        if i < 2*len(self.lidar):\n            l = self.lidar[i] if i < len(self.lidar) else self.lidar[len(self.lidar)-i-1]\n            self.viewer.draw_polyline( [l.p1, l.p2], color=(1,0,0), linewidth=1 )\n\n        for obj in self.drawlist:\n            for f in obj.fixtures:\n                trans = f.body.transform\n                if type(f.shape) is circleShape:\n                    t = rendering.Transform(translation=trans*f.shape.pos)\n                    self.viewer.draw_circle(f.shape.radius, 30, color=obj.color1).add_attr(t)\n                    self.viewer.draw_circle(f.shape.radius, 30, color=obj.color2, filled=False, linewidth=2).add_attr(t)\n                else:\n                    path = [trans*v for v in f.shape.vertices]\n                    self.viewer.draw_polygon(path, color=obj.color1)\n                    path.append(path[0])\n                    self.viewer.draw_polyline(path, color=obj.color2, linewidth=2)\n\n        flagy1 = TERRAIN_HEIGHT\n        flagy2 = flagy1 + 50/SCALE\n        x = TERRAIN_STEP*3\n        self.viewer.draw_polyline( [(x, flagy1), (x, flagy2)], color=(0,0,0), linewidth=2 )\n        f = [(x, flagy2), (x, flagy2-10/SCALE), (x+25/SCALE, flagy2-5/SCALE)]\n        self.viewer.draw_polygon(f, color=(0.9,0.2,0) )\n        self.viewer.draw_polyline(f + [f[0]], color=(0,0,0), linewidth=2 )\n\n        return self.viewer.render(return_rgb_array = mode=='rgb_array')\n\n    def close(self):\n        if self.viewer is not None:\n            self.viewer.close()\n            self.viewer = None\n\nclass BipedalWalkerHardcore(BipedalWalker):\n    hardcore = True\n\nif __name__==\"__main__\":\n    # Heurisic: suboptimal, have no notion of balance.\n    env = BipedalWalker()\n    env.reset()\n    steps = 0\n    total_reward = 0\n    a = np.array([0.0, 0.0, 0.0, 0.0])\n    STAY_ON_ONE_LEG, PUT_OTHER_DOWN, PUSH_OFF = 1,2,3\n    SPEED = 0.29  # Will fall forward on higher speed\n    state = STAY_ON_ONE_LEG\n    moving_leg = 0\n    supporting_leg = 1 - moving_leg\n    SUPPORT_KNEE_ANGLE = +0.1\n    supporting_knee_angle = SUPPORT_KNEE_ANGLE\n    while True:\n        s, r, done, info = env.step(a)\n        total_reward += r\n        if steps % 20 == 0 or done:\n            print(\"\\naction \" + str([\"{:+0.2f}\".format(x) for x in a]))\n            print(\"step {} total_reward {:+0.2f}\".format(steps, total_reward))\n            print(\"hull \" + str([\"{:+0.2f}\".format(x) for x in s[0:4] ]))\n            print(\"leg0 \" + str([\"{:+0.2f}\".format(x) for x in s[4:9] ]))\n            print(\"leg1 \" + str([\"{:+0.2f}\".format(x) for x in s[9:14]]))\n        steps += 1\n\n        contact0 = s[8]\n        contact1 = s[13]\n        moving_s_base = 4 + 5*moving_leg\n        supporting_s_base = 4 + 5*supporting_leg\n\n        hip_targ  = [None,None]   # -0.8 .. +1.1\n        knee_targ = [None,None]   # -0.6 .. +0.9\n        hip_todo  = [0.0, 0.0]\n        knee_todo = [0.0, 0.0]\n\n        if state==STAY_ON_ONE_LEG:\n            hip_targ[moving_leg]  = 1.1\n            knee_targ[moving_leg] = -0.6\n            supporting_knee_angle += 0.03\n            if s[2] > SPEED: supporting_knee_angle += 0.03\n            supporting_knee_angle = min( supporting_knee_angle, SUPPORT_KNEE_ANGLE )\n            knee_targ[supporting_leg] = supporting_knee_angle\n            if s[supporting_s_base+0] < 0.10: # supporting leg is behind\n                state = PUT_OTHER_DOWN\n        if state==PUT_OTHER_DOWN:\n            hip_targ[moving_leg]  = +0.1\n            knee_targ[moving_leg] = SUPPORT_KNEE_ANGLE\n            knee_targ[supporting_leg] = supporting_knee_angle\n            if s[moving_s_base+4]:\n                state = PUSH_OFF\n                supporting_knee_angle = min( s[moving_s_base+2], SUPPORT_KNEE_ANGLE )\n        if state==PUSH_OFF:\n            knee_targ[moving_leg] = supporting_knee_angle\n            knee_targ[supporting_leg] = +1.0\n            if s[supporting_s_base+2] > 0.88 or s[2] > 1.2*SPEED:\n                state = STAY_ON_ONE_LEG\n                moving_leg = 1 - moving_leg\n                supporting_leg = 1 - moving_leg\n\n        if hip_targ[0]: hip_todo[0] = 0.9*(hip_targ[0] - s[4]) - 0.25*s[5]\n        if hip_targ[1]: hip_todo[1] = 0.9*(hip_targ[1] - s[9]) - 0.25*s[10]\n        if knee_targ[0]: knee_todo[0] = 4.0*(knee_targ[0] - s[6])  - 0.25*s[7]\n        if knee_targ[1]: knee_todo[1] = 4.0*(knee_targ[1] - s[11]) - 0.25*s[12]\n\n        hip_todo[0] -= 0.9*(0-s[0]) - 1.5*s[1] # PID to keep head strait\n        hip_todo[1] -= 0.9*(0-s[0]) - 1.5*s[1]\n        knee_todo[0] -= 15.0*s[3]  # vertical speed, to damp oscillations\n        knee_todo[1] -= 15.0*s[3]\n\n        a[0] = hip_todo[0]\n        a[1] = knee_todo[0]\n        a[2] = hip_todo[1]\n        a[3] = knee_todo[1]\n        a = np.clip(0.5*a, -1.0, 1.0)\n\n        env.render()\n        if done: break",
        "\"\"\"\nRocket trajectory optimization is a classic topic in Optimal Control.\n\nAccording to Pontryagin's maximum principle it's optimal to fire engine full throttle or\nturn it off. That's the reason this environment is OK to have discreet actions (engine on or off).\n\nThe landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector.\nReward for moving from the top of the screen to the landing pad and zero speed is about 100..140 points.\nIf the lander moves away from the landing pad it loses reward. The episode finishes if the lander crashes or\ncomes to rest, receiving an additional -100 or +100 points. Each leg with ground contact is +10 points.\nFiring the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame.\nSolved is 200 points.\n\nLanding outside the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land\non its first attempt. Please see the source code for details.\n\nTo see a heuristic landing, run:\n\npython gym/envs/box2d/lunar_lander.py\n\nTo play yourself, run:\n\npython examples/agents/keyboard_agent.py LunarLander-v2\n\nCreated by Oleg Klimov. Licensed on the same terms as the rest of OpenAI Gym.\n\"\"\"\n\n\nimport sys, math\nimport numpy as np\n\nimport Box2D\nfrom Box2D.b2 import (edgeShape, circleShape, fixtureDef, polygonShape, revoluteJointDef, contactListener)\n\nimport gym\nfrom gym import spaces\nfrom gym.utils import seeding, EzPickle\n\nFPS = 50\nSCALE = 30.0   # affects how fast-paced the game is, forces should be adjusted as well\n\nMAIN_ENGINE_POWER = 13.0\nSIDE_ENGINE_POWER = 0.6\n\nINITIAL_RANDOM = 1000.0   # Set 1500 to make game harder\n\nLANDER_POLY =[\n    (-14, +17), (-17, 0), (-17 ,-10),\n    (+17, -10), (+17, 0), (+14, +17)\n    ]\nLEG_AWAY = 20\nLEG_DOWN = 18\nLEG_W, LEG_H = 2, 8\nLEG_SPRING_TORQUE = 40\n\nSIDE_ENGINE_HEIGHT = 14.0\nSIDE_ENGINE_AWAY = 12.0\n\nVIEWPORT_W = 600\nVIEWPORT_H = 400\n\n\nclass ContactDetector(contactListener):\n    def __init__(self, env):\n        contactListener.__init__(self)\n        self.env = env\n\n    def BeginContact(self, contact):\n        if self.env.lander == contact.fixtureA.body or self.env.lander == contact.fixtureB.body:\n            self.env.game_over = True\n        for i in range(2):\n            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n                self.env.legs[i].ground_contact = True\n\n    def EndContact(self, contact):\n        for i in range(2):\n            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n                self.env.legs[i].ground_contact = False\n\n\nclass LunarLander(gym.Env, EzPickle):\n    metadata = {\n        'render.modes': ['human', 'rgb_array'],\n        'video.frames_per_second' : FPS\n    }\n\n    continuous = False\n\n    def __init__(self):\n        EzPickle.__init__(self)\n        self.seed()\n        self.viewer = None\n\n        self.world = Box2D.b2World()\n        self.moon = None\n        self.lander = None\n        self.particles = []\n\n        self.prev_reward = None\n\n        # useful range is -1 .. +1, but spikes can be higher\n        self.observation_space = spaces.Box(-np.inf, np.inf, shape=(8,), dtype=np.float32)\n\n        if self.continuous:\n            # Action is two floats [main engine, left-right engines].\n            # Main engine: -1..0 off, 0..+1 throttle from 50% to 100% power. Engine can't work with less than 50% power.\n            # Left-right:  -1.0..-0.5 fire left engine, +0.5..+1.0 fire right engine, -0.5..0.5 off\n            self.action_space = spaces.Box(-1, +1, (2,), dtype=np.float32)\n        else:\n            # Nop, fire left engine, main engine, right engine\n            self.action_space = spaces.Discrete(4)\n\n        self.reset()\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def _destroy(self):\n        if not self.moon: return\n        self.world.contactListener = None\n        self._clean_particles(True)\n        self.world.DestroyBody(self.moon)\n        self.moon = None\n        self.world.DestroyBody(self.lander)\n        self.lander = None\n        self.world.DestroyBody(self.legs[0])\n        self.world.DestroyBody(self.legs[1])\n\n    def reset(self):\n        self._destroy()\n        self.world.contactListener_keepref = ContactDetector(self)\n        self.world.contactListener = self.world.contactListener_keepref\n        self.game_over = False\n        self.prev_shaping = None\n\n        W = VIEWPORT_W/SCALE\n        H = VIEWPORT_H/SCALE\n\n        # terrain\n        CHUNKS = 11\n        height = self.np_random.uniform(0, H/2, size=(CHUNKS+1,))\n        chunk_x = [W/(CHUNKS-1)*i for i in range(CHUNKS)]\n        self.helipad_x1 = chunk_x[CHUNKS//2-1]\n        self.helipad_x2 = chunk_x[CHUNKS//2+1]\n        self.helipad_y = H/4\n        height[CHUNKS//2-2] = self.helipad_y\n        height[CHUNKS//2-1] = self.helipad_y\n        height[CHUNKS//2+0] = self.helipad_y\n        height[CHUNKS//2+1] = self.helipad_y\n        height[CHUNKS//2+2] = self.helipad_y\n        smooth_y = [0.33*(height[i-1] + height[i+0] + height[i+1]) for i in range(CHUNKS)]\n\n        self.moon = self.world.CreateStaticBody(shapes=edgeShape(vertices=[(0, 0), (W, 0)]))\n        self.sky_polys = []\n        for i in range(CHUNKS-1):\n            p1 = (chunk_x[i], smooth_y[i])\n            p2 = (chunk_x[i+1], smooth_y[i+1])\n            self.moon.CreateEdgeFixture(\n                vertices=[p1,p2],\n                density=0,\n                friction=0.1)\n            self.sky_polys.append([p1, p2, (p2[0], H), (p1[0], H)])\n\n        self.moon.color1 = (0.0, 0.0, 0.0)\n        self.moon.color2 = (0.0, 0.0, 0.0)\n\n        initial_y = VIEWPORT_H/SCALE\n        self.lander = self.world.CreateDynamicBody(\n            position=(VIEWPORT_W/SCALE/2, initial_y),\n            angle=0.0,\n            fixtures = fixtureDef(\n                shape=polygonShape(vertices=[(x/SCALE, y/SCALE) for x, y in LANDER_POLY]),\n                density=5.0,\n                friction=0.1,\n                categoryBits=0x0010,\n                maskBits=0x001,   # collide only with ground\n                restitution=0.0)  # 0.99 bouncy\n                )\n        self.lander.color1 = (0.5, 0.4, 0.9)\n        self.lander.color2 = (0.3, 0.3, 0.5)\n        self.lander.ApplyForceToCenter( (\n            self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n            self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM)\n            ), True)\n\n        self.legs = []\n        for i in [-1, +1]:\n            leg = self.world.CreateDynamicBody(\n                position=(VIEWPORT_W/SCALE/2 - i*LEG_AWAY/SCALE, initial_y),\n                angle=(i * 0.05),\n                fixtures=fixtureDef(\n                    shape=polygonShape(box=(LEG_W/SCALE, LEG_H/SCALE)),\n                    density=1.0,\n                    restitution=0.0,\n                    categoryBits=0x0020,\n                    maskBits=0x001)\n                )\n            leg.ground_contact = False\n            leg.color1 = (0.5, 0.4, 0.9)\n            leg.color2 = (0.3, 0.3, 0.5)\n            rjd = revoluteJointDef(\n                bodyA=self.lander,\n                bodyB=leg,\n                localAnchorA=(0, 0),\n                localAnchorB=(i * LEG_AWAY/SCALE, LEG_DOWN/SCALE),\n                enableMotor=True,\n                enableLimit=True,\n                maxMotorTorque=LEG_SPRING_TORQUE,\n                motorSpeed=+0.3 * i  # low enough not to jump back into the sky\n                )\n            if i == -1:\n                rjd.lowerAngle = +0.9 - 0.5  # The most esoteric numbers here, angled legs have freedom to travel within\n                rjd.upperAngle = +0.9\n            else:\n                rjd.lowerAngle = -0.9\n                rjd.upperAngle = -0.9 + 0.5\n            leg.joint = self.world.CreateJoint(rjd)\n            self.legs.append(leg)\n\n        self.drawlist = [self.lander] + self.legs\n\n        return self.step(np.array([0, 0]) if self.continuous else 0)[0]\n\n    def _create_particle(self, mass, x, y, ttl):\n        p = self.world.CreateDynamicBody(\n            position = (x, y),\n            angle=0.0,\n            fixtures = fixtureDef(\n                shape=circleShape(radius=2/SCALE, pos=(0, 0)),\n                density=mass,\n                friction=0.1,\n                categoryBits=0x0100,\n                maskBits=0x001,  # collide only with ground\n                restitution=0.3)\n                )\n        p.ttl = ttl\n        self.particles.append(p)\n        self._clean_particles(False)\n        return p\n\n    def _clean_particles(self, all):\n        while self.particles and (all or self.particles[0].ttl < 0):\n            self.world.DestroyBody(self.particles.pop(0))\n\n    def step(self, action):\n        if self.continuous:\n            action = np.clip(action, -1, +1).astype(np.float32)\n        else:\n            assert self.action_space.contains(action), \"%r (%s) invalid \" % (action, type(action))\n\n        # Engines\n        tip  = (math.sin(self.lander.angle), math.cos(self.lander.angle))\n        side = (-tip[1], tip[0])\n        dispersion = [self.np_random.uniform(-1.0, +1.0) / SCALE for _ in range(2)]\n\n        m_power = 0.0\n        if (self.continuous and action[0] > 0.0) or (not self.continuous and action == 2):\n            # Main engine\n            if self.continuous:\n                m_power = (np.clip(action[0], 0.0,1.0) + 1.0)*0.5   # 0.5..1.0\n                assert m_power >= 0.5 and m_power <= 1.0\n            else:\n                m_power = 1.0\n            ox = (tip[0] * (4/SCALE + 2 * dispersion[0]) +\n                  side[0] * dispersion[1])  # 4 is move a bit downwards, +-2 for randomness\n            oy = -tip[1] * (4/SCALE + 2 * dispersion[0]) - side[1] * dispersion[1]\n            impulse_pos = (self.lander.position[0] + ox, self.lander.position[1] + oy)\n            p = self._create_particle(3.5,  # 3.5 is here to make particle speed adequate\n                                      impulse_pos[0],\n                                      impulse_pos[1],\n                                      m_power)  # particles are just a decoration\n            p.ApplyLinearImpulse((ox * MAIN_ENGINE_POWER * m_power, oy * MAIN_ENGINE_POWER * m_power),\n                                 impulse_pos,\n                                 True)\n            self.lander.ApplyLinearImpulse((-ox * MAIN_ENGINE_POWER * m_power, -oy * MAIN_ENGINE_POWER * m_power),\n                                           impulse_pos,\n                                           True)\n\n        s_power = 0.0\n        if (self.continuous and np.abs(action[1]) > 0.5) or (not self.continuous and action in [1, 3]):\n            # Orientation engines\n            if self.continuous:\n                direction = np.sign(action[1])\n                s_power = np.clip(np.abs(action[1]), 0.5, 1.0)\n                assert s_power >= 0.5 and s_power <= 1.0\n            else:\n                direction = action-2\n                s_power = 1.0\n            ox = tip[0] * dispersion[0] + side[0] * (3 * dispersion[1] + direction * SIDE_ENGINE_AWAY/SCALE)\n            oy = -tip[1] * dispersion[0] - side[1] * (3 * dispersion[1] + direction * SIDE_ENGINE_AWAY/SCALE)\n            impulse_pos = (self.lander.position[0] + ox - tip[0] * 17/SCALE,\n                           self.lander.position[1] + oy + tip[1] * SIDE_ENGINE_HEIGHT/SCALE)\n            p = self._create_particle(0.7, impulse_pos[0], impulse_pos[1], s_power)\n            p.ApplyLinearImpulse((ox * SIDE_ENGINE_POWER * s_power, oy * SIDE_ENGINE_POWER * s_power),\n                                 impulse_pos\n                                 , True)\n            self.lander.ApplyLinearImpulse((-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),\n                                           impulse_pos,\n                                           True)\n\n        self.world.Step(1.0/FPS, 6*30, 2*30)\n\n        pos = self.lander.position\n        vel = self.lander.linearVelocity\n        state = [\n            (pos.x - VIEWPORT_W/SCALE/2) / (VIEWPORT_W/SCALE/2),\n            (pos.y - (self.helipad_y+LEG_DOWN/SCALE)) / (VIEWPORT_H/SCALE/2),\n            vel.x*(VIEWPORT_W/SCALE/2)/FPS,\n            vel.y*(VIEWPORT_H/SCALE/2)/FPS,\n            self.lander.angle,\n            20.0*self.lander.angularVelocity/FPS,\n            1.0 if self.legs[0].ground_contact else 0.0,\n            1.0 if self.legs[1].ground_contact else 0.0\n            ]\n        assert len(state) == 8\n\n        reward = 0\n        shaping = \\\n            - 100*np.sqrt(state[0]*state[0] + state[1]*state[1]) \\\n            - 100*np.sqrt(state[2]*state[2] + state[3]*state[3]) \\\n            - 100*abs(state[4]) + 10*state[6] + 10*state[7]  # And ten points for legs contact, the idea is if you\n                                                             # lose contact again after landing, you get negative reward\n        if self.prev_shaping is not None:\n            reward = shaping - self.prev_shaping\n        self.prev_shaping = shaping\n\n        reward -= m_power*0.30  # less fuel spent is better, about -30 for heuristic landing\n        reward -= s_power*0.03\n\n        done = False\n        if self.game_over or abs(state[0]) >= 1.0:\n            done = True\n            reward = -100\n        if not self.lander.awake:\n            done = True\n            reward = +100\n        return np.array(state, dtype=np.float32), reward, done, {}\n\n    def render(self, mode='human'):\n        from gym.envs.classic_control import rendering\n        if self.viewer is None:\n            self.viewer = rendering.Viewer(VIEWPORT_W, VIEWPORT_H)\n            self.viewer.set_bounds(0, VIEWPORT_W/SCALE, 0, VIEWPORT_H/SCALE)\n\n        for obj in self.particles:\n            obj.ttl -= 0.15\n            obj.color1 = (max(0.2, 0.2+obj.ttl), max(0.2, 0.5*obj.ttl), max(0.2, 0.5*obj.ttl))\n            obj.color2 = (max(0.2, 0.2+obj.ttl), max(0.2, 0.5*obj.ttl), max(0.2, 0.5*obj.ttl))\n\n        self._clean_particles(False)\n\n        for p in self.sky_polys:\n            self.viewer.draw_polygon(p, color=(0, 0, 0))\n\n        for obj in self.particles + self.drawlist:\n            for f in obj.fixtures:\n                trans = f.body.transform\n                if type(f.shape) is circleShape:\n                    t = rendering.Transform(translation=trans*f.shape.pos)\n                    self.viewer.draw_circle(f.shape.radius, 20, color=obj.color1).add_attr(t)\n                    self.viewer.draw_circle(f.shape.radius, 20, color=obj.color2, filled=False, linewidth=2).add_attr(t)\n                else:\n                    path = [trans*v for v in f.shape.vertices]\n                    self.viewer.draw_polygon(path, color=obj.color1)\n                    path.append(path[0])\n                    self.viewer.draw_polyline(path, color=obj.color2, linewidth=2)\n\n        for x in [self.helipad_x1, self.helipad_x2]:\n            flagy1 = self.helipad_y\n            flagy2 = flagy1 + 50/SCALE\n            self.viewer.draw_polyline([(x, flagy1), (x, flagy2)], color=(1, 1, 1))\n            self.viewer.draw_polygon([(x, flagy2), (x, flagy2-10/SCALE), (x + 25/SCALE, flagy2 - 5/SCALE)],\n                                     color=(0.8, 0.8, 0))\n\n        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n\n    def close(self):\n        if self.viewer is not None:\n            self.viewer.close()\n            self.viewer = None\n\n\nclass LunarLanderContinuous(LunarLander):\n    continuous = True\n\ndef heuristic(env, s):\n    \"\"\"\n    The heuristic for\n    1. Testing\n    2. Demonstration rollout.\n\n    Args:\n        env: The environment\n        s (list): The state. Attributes:\n                  s[0] is the horizontal coordinate\n                  s[1] is the vertical coordinate\n                  s[2] is the horizontal speed\n                  s[3] is the vertical speed\n                  s[4] is the angle\n                  s[5] is the angular speed\n                  s[6] 1 if first leg has contact, else 0\n                  s[7] 1 if second leg has contact, else 0\n    returns:\n         a: The heuristic to be fed into the step function defined above to determine the next step and reward.\n    \"\"\"\n\n    angle_targ = s[0]*0.5 + s[2]*1.0         # angle should point towards center\n    if angle_targ > 0.4: angle_targ = 0.4    # more than 0.4 radians (22 degrees) is bad\n    if angle_targ < -0.4: angle_targ = -0.4\n    hover_targ = 0.55*np.abs(s[0])           # target y should be proportional to horizontal offset\n\n    angle_todo = (angle_targ - s[4]) * 0.5 - (s[5])*1.0\n    hover_todo = (hover_targ - s[1])*0.5 - (s[3])*0.5\n\n    if s[6] or s[7]:  # legs have contact\n        angle_todo = 0\n        hover_todo = -(s[3])*0.5  # override to reduce fall speed, that's all we need after contact\n\n    if env.continuous:\n        a = np.array([hover_todo*20 - 1, -angle_todo*20])\n        a = np.clip(a, -1, +1)\n    else:\n        a = 0\n        if hover_todo > np.abs(angle_todo) and hover_todo > 0.05: a = 2\n        elif angle_todo < -0.05: a = 3\n        elif angle_todo > +0.05: a = 1\n    return a\n\ndef demo_heuristic_lander(env, seed=None, render=False):\n    env.seed(seed)\n    total_reward = 0\n    steps = 0\n    s = env.reset()\n    while True:\n        a = heuristic(env, s)\n        s, r, done, info = env.step(a)\n        total_reward += r\n\n        if render:\n            still_open = env.render()\n            if still_open == False: break\n\n        if steps % 20 == 0 or done:\n            print(\"observations:\", \" \".join([\"{:+0.2f}\".format(x) for x in s]))\n            print(\"step {} total_reward {:+0.2f}\".format(steps, total_reward))\n        steps += 1\n        if done: break\n    return total_reward\n\n\nif __name__ == '__main__':\n    demo_heuristic_lander(LunarLander(), render=True)",
        "\"\"\"\nTop-down car dynamics simulation.\n\nSome ideas are taken from this great tutorial http://www.iforce2d.net/b2dtut/top-down-car by Chris Campbell.\nThis simulation is a bit more detailed, with wheels rotation.\n\nCreated by Oleg Klimov. Licensed on the same terms as the rest of OpenAI Gym.\n\"\"\"\n\nimport numpy as np\nimport math\nimport Box2D\nfrom Box2D.b2 import (edgeShape, circleShape, fixtureDef, polygonShape, revoluteJointDef, contactListener, shape)\n\nSIZE = 0.02\nENGINE_POWER = 100000000*SIZE*SIZE\nWHEEL_MOMENT_OF_INERTIA = 4000*SIZE*SIZE\nFRICTION_LIMIT = 1000000*SIZE*SIZE     # friction ~= mass ~= size^2 (calculated implicitly using density)\nWHEEL_R = 27\nWHEEL_W = 14\nWHEELPOS = [\n    (-55, +80), (+55, +80),\n    (-55, -82), (+55, -82)\n    ]\nHULL_POLY1 = [\n    (-60, +130), (+60, +130),\n    (+60, +110), (-60, +110)\n    ]\nHULL_POLY2 = [\n    (-15, +120), (+15, +120),\n    (+20, +20), (-20, 20)\n    ]\nHULL_POLY3 = [\n    (+25, +20),\n    (+50, -10),\n    (+50, -40),\n    (+20, -90),\n    (-20, -90),\n    (-50, -40),\n    (-50, -10),\n    (-25, +20)\n    ]\nHULL_POLY4 = [\n    (-50, -120), (+50, -120),\n    (+50, -90),  (-50, -90)\n    ]\nWHEEL_COLOR = (0.0,  0.0, 0.0)\nWHEEL_WHITE = (0.3, 0.3, 0.3)\nMUD_COLOR = (0.4, 0.4, 0.0)\n\n\nclass Car:\n    def __init__(self, world, init_angle, init_x, init_y):\n        self.world = world\n        self.hull = self.world.CreateDynamicBody(\n            position=(init_x, init_y),\n            angle=init_angle,\n            fixtures=[\n                fixtureDef(shape=polygonShape(vertices=[(x*SIZE, y*SIZE) for x, y in HULL_POLY1]), density=1.0),\n                fixtureDef(shape=polygonShape(vertices=[(x*SIZE, y*SIZE) for x, y in HULL_POLY2]), density=1.0),\n                fixtureDef(shape=polygonShape(vertices=[(x*SIZE, y*SIZE) for x, y in HULL_POLY3]), density=1.0),\n                fixtureDef(shape=polygonShape(vertices=[(x*SIZE, y*SIZE) for x, y in HULL_POLY4]), density=1.0)\n                ]\n            )\n        self.hull.color = (0.8, 0.0, 0.0)\n        self.wheels = []\n        self.fuel_spent = 0.0\n        WHEEL_POLY = [\n            (-WHEEL_W, +WHEEL_R), (+WHEEL_W, +WHEEL_R),\n            (+WHEEL_W, -WHEEL_R), (-WHEEL_W, -WHEEL_R)\n            ]\n        for wx, wy in WHEELPOS:\n            front_k = 1.0 if wy > 0 else 1.0\n            w = self.world.CreateDynamicBody(\n                position=(init_x+wx*SIZE, init_y+wy*SIZE),\n                angle=init_angle,\n                fixtures=fixtureDef(\n                    shape=polygonShape(vertices=[(x*front_k*SIZE,y*front_k*SIZE) for x, y in WHEEL_POLY]),\n                    density=0.1,\n                    categoryBits=0x0020,\n                    maskBits=0x001,\n                    restitution=0.0)\n                    )\n            w.wheel_rad = front_k*WHEEL_R*SIZE\n            w.color = WHEEL_COLOR\n            w.gas = 0.0\n            w.brake = 0.0\n            w.steer = 0.0\n            w.phase = 0.0  # wheel angle\n            w.omega = 0.0  # angular velocity\n            w.skid_start = None\n            w.skid_particle = None\n            rjd = revoluteJointDef(\n                bodyA=self.hull,\n                bodyB=w,\n                localAnchorA=(wx*SIZE, wy*SIZE),\n                localAnchorB=(0,0),\n                enableMotor=True,\n                enableLimit=True,\n                maxMotorTorque=180*900*SIZE*SIZE,\n                motorSpeed=0,\n                lowerAngle=-0.4,\n                upperAngle=+0.4,\n                )\n            w.joint = self.world.CreateJoint(rjd)\n            w.tiles = set()\n            w.userData = w\n            self.wheels.append(w)\n        self.drawlist = self.wheels + [self.hull]\n        self.particles = []\n\n    def gas(self, gas):\n        \"\"\"control: rear wheel drive\n\n        Args:\n            gas (float): How much gas gets applied. Gets clipped between 0 and 1.\n        \"\"\"\n        gas = np.clip(gas, 0, 1)\n        for w in self.wheels[2:4]:\n            diff = gas - w.gas\n            if diff > 0.1: diff = 0.1  # gradually increase, but stop immediately\n            w.gas += diff\n\n    def brake(self, b):\n        \"\"\"control: brake\n\n        Args:\n            b (0..1): Degree to which the brakes are applied. More than 0.9 blocks the wheels to zero rotation\"\"\"\n        for w in self.wheels:\n            w.brake = b\n\n    def steer(self, s):\n        \"\"\"control: steer\n\n        Args:\n            s (-1..1): target position, it takes time to rotate steering wheel from side-to-side\"\"\"\n        self.wheels[0].steer = s\n        self.wheels[1].steer = s\n\n    def step(self, dt):\n        for w in self.wheels:\n            # Steer each wheel\n            dir = np.sign(w.steer - w.joint.angle)\n            val = abs(w.steer - w.joint.angle)\n            w.joint.motorSpeed = dir*min(50.0*val, 3.0)\n\n            # Position => friction_limit\n            grass = True\n            friction_limit = FRICTION_LIMIT*0.6  # Grass friction if no tile\n            for tile in w.tiles:\n                friction_limit = max(friction_limit, FRICTION_LIMIT*tile.road_friction)\n                grass = False\n\n            # Force\n            forw = w.GetWorldVector( (0,1) )\n            side = w.GetWorldVector( (1,0) )\n            v = w.linearVelocity\n            vf = forw[0]*v[0] + forw[1]*v[1]  # forward speed\n            vs = side[0]*v[0] + side[1]*v[1]  # side speed\n\n            # WHEEL_MOMENT_OF_INERTIA*np.square(w.omega)/2 = E -- energy\n            # WHEEL_MOMENT_OF_INERTIA*w.omega * domega/dt = dE/dt = W -- power\n            # domega = dt*W/WHEEL_MOMENT_OF_INERTIA/w.omega\n\n            # add small coef not to divide by zero\n            w.omega += dt*ENGINE_POWER*w.gas/WHEEL_MOMENT_OF_INERTIA/(abs(w.omega)+5.0)\n            self.fuel_spent += dt*ENGINE_POWER*w.gas\n\n            if w.brake >= 0.9:\n                w.omega = 0\n            elif w.brake > 0:\n                BRAKE_FORCE = 15    # radians per second\n                dir = -np.sign(w.omega)\n                val = BRAKE_FORCE*w.brake\n                if abs(val) > abs(w.omega): val = abs(w.omega)  # low speed => same as = 0\n                w.omega += dir*val\n            w.phase += w.omega*dt\n\n            vr = w.omega*w.wheel_rad  # rotating wheel speed\n            f_force = -vf + vr        # force direction is direction of speed difference\n            p_force = -vs\n\n            # Physically correct is to always apply friction_limit until speed is equal.\n            # But dt is finite, that will lead to oscillations if difference is already near zero.\n\n            # Random coefficient to cut oscillations in few steps (have no effect on friction_limit)\n            f_force *= 205000*SIZE*SIZE\n            p_force *= 205000*SIZE*SIZE\n            force = np.sqrt(np.square(f_force) + np.square(p_force))\n\n            # Skid trace\n            if abs(force) > 2.0*friction_limit:\n                if w.skid_particle and w.skid_particle.grass == grass and len(w.skid_particle.poly) < 30:\n                    w.skid_particle.poly.append( (w.position[0], w.position[1]) )\n                elif w.skid_start is None:\n                    w.skid_start = w.position\n                else:\n                    w.skid_particle = self._create_particle( w.skid_start, w.position, grass )\n                    w.skid_start = None\n            else:\n                w.skid_start = None\n                w.skid_particle = None\n\n            if abs(force) > friction_limit:\n                f_force /= force\n                p_force /= force\n                force = friction_limit  # Correct physics here\n                f_force *= force\n                p_force *= force\n\n            w.omega -= dt*f_force*w.wheel_rad/WHEEL_MOMENT_OF_INERTIA\n\n            w.ApplyForceToCenter( (\n                p_force*side[0] + f_force*forw[0],\n                p_force*side[1] + f_force*forw[1]), True )\n\n    def draw(self, viewer, draw_particles=True):\n        if draw_particles:\n            for p in self.particles:\n                viewer.draw_polyline(p.poly, color=p.color, linewidth=5)\n        for obj in self.drawlist:\n            for f in obj.fixtures:\n                trans = f.body.transform\n                path = [trans*v for v in f.shape.vertices]\n                viewer.draw_polygon(path, color=obj.color)\n                if \"phase\" not in obj.__dict__: continue\n                a1 = obj.phase\n                a2 = obj.phase + 1.2  # radians\n                s1 = math.sin(a1)\n                s2 = math.sin(a2)\n                c1 = math.cos(a1)\n                c2 = math.cos(a2)\n                if s1 > 0 and s2 > 0: continue\n                if s1 > 0: c1 = np.sign(c1)\n                if s2 > 0: c2 = np.sign(c2)\n                white_poly = [\n                    (-WHEEL_W*SIZE, +WHEEL_R*c1*SIZE), (+WHEEL_W*SIZE, +WHEEL_R*c1*SIZE),\n                    (+WHEEL_W*SIZE, +WHEEL_R*c2*SIZE), (-WHEEL_W*SIZE, +WHEEL_R*c2*SIZE)\n                    ]\n                viewer.draw_polygon([trans*v for v in white_poly], color=WHEEL_WHITE)\n\n    def _create_particle(self, point1, point2, grass):\n        class Particle:\n            pass\n        p = Particle()\n        p.color = WHEEL_COLOR if not grass else MUD_COLOR\n        p.ttl = 1\n        p.poly = [(point1[0], point1[1]), (point2[0], point2[1])]\n        p.grass = grass\n        self.particles.append(p)\n        while len(self.particles) > 30:\n            self.particles.pop(0)\n        return p\n\n    def destroy(self):\n        self.world.DestroyBody(self.hull)\n        self.hull = None\n        for w in self.wheels:\n            self.world.DestroyBody(w)\n        self.wheels = []\n",
        "import numpy as np\nfrom copy import deepcopy\n\nfrom gym import logger\nfrom gym.vector.vector_env import VectorEnv\nfrom gym.vector.utils import concatenate, create_empty_array\n\n__all__ = ['SyncVectorEnv']\n\n\nclass SyncVectorEnv(VectorEnv):\n    \"\"\"Vectorized environment that serially runs multiple environments.\n\n    Parameters\n    ----------\n    env_fns : iterable of callable\n        Functions that create the environments.\n\n    observation_space : `gym.spaces.Space` instance, optional\n        Observation space of a single environment. If `None`, then the\n        observation space of the first environment is taken.\n\n    action_space : `gym.spaces.Space` instance, optional\n        Action space of a single environment. If `None`, then the action space\n        of the first environment is taken.\n\n    copy : bool (default: `True`)\n        If `True`, then the `reset` and `step` methods return a copy of the\n        observations.\n    \"\"\"\n    def __init__(self, env_fns, observation_space=None, action_space=None,\n                 copy=True):\n        self.env_fns = env_fns\n        self.envs = [env_fn() for env_fn in env_fns]\n        self.copy = copy\n        \n        if (observation_space is None) or (action_space is None):\n            observation_space = observation_space or self.envs[0].observation_space\n            action_space = action_space or self.envs[0].action_space\n        super(SyncVectorEnv, self).__init__(num_envs=len(env_fns),\n            observation_space=observation_space, action_space=action_space)\n\n        self._check_observation_spaces()\n        self.observations = create_empty_array(self.single_observation_space,\n            n=self.num_envs, fn=np.zeros)\n        self._rewards = np.zeros((self.num_envs,), dtype=np.float64)\n        self._dones = np.zeros((self.num_envs,), dtype=np.bool_)\n        self._actions = None\n\n    def seed(self, seeds=None):\n        if seeds is None:\n            seeds = [None for _ in range(self.num_envs)]\n        if isinstance(seeds, int):\n            seeds = [seeds + i for i in range(self.num_envs)]\n        assert len(seeds) == self.num_envs\n\n        for env, seed in zip(self.envs, seeds):\n            env.seed(seed)\n\n    def reset_wait(self):\n        self._dones[:] = False\n        observations = []\n        for env in self.envs:\n            observation = env.reset()\n            observations.append(observation)\n        concatenate(observations, self.observations, self.single_observation_space)\n\n        return np.copy(self.observations) if self.copy else self.observations\n\n    def step_async(self, actions):\n        self._actions = actions\n\n    def step_wait(self):\n        observations, infos = [], []\n        for i, (env, action) in enumerate(zip(self.envs, self._actions)):\n            observation, self._rewards[i], self._dones[i], info = env.step(action)\n            if self._dones[i]:\n                observation = env.reset()\n            observations.append(observation)\n            infos.append(info)\n        concatenate(observations, self.observations, self.single_observation_space)\n\n        return (deepcopy(self.observations) if self.copy else self.observations,\n            np.copy(self._rewards), np.copy(self._dones), infos)\n\n    def close_extras(self, **kwargs):\n        [env.close() for env in self.envs]\n\n    def _check_observation_spaces(self):\n        for env in self.envs:\n            if not (env.observation_space == self.single_observation_space):\n                break\n        else:\n            return True\n        raise RuntimeError('Some environments have an observation space '\n            'different from `{0}`. In order to batch observations, the '\n            'observation spaces from all environments must be '\n            'equal.'.format(self.single_observation_space))",
        "import gym\nfrom gym.spaces import Tuple\nfrom gym.vector.utils.spaces import batch_space\n\n__all__ = ['VectorEnv']\n\n\nclass VectorEnv(gym.Env):\n    r\"\"\"Base class for vectorized environments.\n\n    Each observation returned from vectorized environment is a batch of observations \n    for each sub-environment. And :meth:`step` is also expected to receive a batch of \n    actions for each sub-environment.\n    \n    .. note::\n    \n        All sub-environments should share the identical observation and action spaces.\n        In other words, a vector of multiple different environments is not supported. \n\n    Parameters\n    ----------\n    num_envs : int\n        Number of environments in the vectorized environment.\n\n    observation_space : `gym.spaces.Space` instance\n        Observation space of a single environment.\n\n    action_space : `gym.spaces.Space` instance\n        Action space of a single environment.\n    \"\"\"\n    def __init__(self, num_envs, observation_space, action_space):\n        super(VectorEnv, self).__init__()\n        self.num_envs = num_envs\n        self.observation_space = batch_space(observation_space, n=num_envs)\n        self.action_space = Tuple((action_space,) * num_envs)\n\n        self.closed = False\n        self.viewer = None\n\n        # The observation and action spaces of a single environment are\n        # kept in separate properties\n        self.single_observation_space = observation_space\n        self.single_action_space = action_space\n\n    def reset_async(self):\n        pass\n\n    def reset_wait(self, **kwargs):\n        raise NotImplementedError()\n\n    def reset(self):\n        r\"\"\"Reset all sub-environments and return a batch of initial observations.\n        \n        Returns\n        -------\n        observations : sample from `observation_space`\n            A batch of observations from the vectorized environment.\n        \"\"\"\n        self.reset_async()\n        return self.reset_wait()\n\n    def step_async(self, actions):\n        pass\n\n    def step_wait(self, **kwargs):\n        raise NotImplementedError()\n\n    def step(self, actions):\n        r\"\"\"Take an action for each sub-environments. \n\n        Parameters\n        ----------\n        actions : iterable of samples from `action_space`\n            List of actions.\n\n        Returns\n        -------\n        observations : sample from `observation_space`\n            A batch of observations from the vectorized environment.\n\n        rewards : `np.ndarray` instance (dtype `np.float_`)\n            A vector of rewards from the vectorized environment.\n\n        dones : `np.ndarray` instance (dtype `np.bool_`)\n            A vector whose entries indicate whether the episode has ended.\n\n        infos : list of dict\n            A list of auxiliary diagnostic information dicts from sub-environments.\n        \"\"\"\n\n        self.step_async(actions)\n        return self.step_wait()\n\n    def close_extras(self, **kwargs):\n        r\"\"\"Clean up the extra resources e.g. beyond what's in this base class. \"\"\"\n        raise NotImplementedError()\n\n    def close(self, **kwargs):\n        r\"\"\"Close all sub-environments and release resources.\n        \n        It also closes all the existing image viewers, then calls :meth:`close_extras` and set\n        :attr:`closed` as ``True``. \n        \n        .. warning::\n        \n            This function itself does not close the environments, it should be handled\n            in :meth:`close_extras`. This is generic for both synchronous and asynchronous \n            vectorized environments. \n        \n        .. note::\n        \n            This will be automatically called when garbage collected or program exited. \n            \n        \"\"\"\n        if self.closed:\n            return\n        if self.viewer is not None:\n            self.viewer.close()\n        self.close_extras(**kwargs)\n        self.closed = True\n\n    def seed(self, seeds=None):\n        \"\"\"\n        Parameters\n        ----------\n        seeds : list of int, or int, optional\n            Random seed for each individual environment. If `seeds` is a list of\n            length `num_envs`, then the items of the list are chosen as random\n            seeds. If `seeds` is an int, then each environment uses the random\n            seed `seeds + n`, where `n` is the index of the environment (between\n            `0` and `num_envs - 1`).\n        \"\"\"\n        pass\n\n    def __del__(self):\n        if not getattr(self, 'closed', True):\n            self.close(terminate=True)\n\n    def __repr__(self):\n        if self.spec is None:\n            return '{}({})'.format(self.__class__.__name__, self.num_envs)\n        else:\n            return '{}({}, {})'.format(self.__class__.__name__, self.spec.id, self.num_envs)\n\n\nclass VectorEnvWrapper(VectorEnv):\n    r\"\"\"Wraps the vectorized environment to allow a modular transformation. \n    \n    This class is the base class for all wrappers for vectorized environments. The subclass\n    could override some methods to change the behavior of the original vectorized environment\n    without touching the original code. \n    \n    .. note::\n    \n        Don't forget to call ``super().__init__(env)`` if the subclass overrides :meth:`__init__`.\n    \n    \"\"\"\n    def __init__(self, env):\n        assert isinstance(env, VectorEnv)\n        self.env = env\n\n    def __getattr__(self, name):\n        if name.startswith('_'):\n            raise AttributeError(\"attempted to get missing private attribute '{}'\".format(name))\n        return getattr(self.env, name)\n\n    @property\n    def unwrapped(self):\n        return self.env.unwrapped\n\n    def __repr__(self):\n        return '<{}, {}>'.format(self.__class__.__name__, self.env)",
        "import pytest\nimport numpy as np\n\nfrom collections import OrderedDict\n\nfrom gym.spaces import Tuple, Dict\nfrom gym.vector.utils.spaces import _BaseGymSpaces\nfrom gym.vector.tests.utils import spaces\n\nfrom gym.vector.utils.numpy_utils import concatenate, create_empty_array\n\n@pytest.mark.parametrize('space', spaces,\n    ids=[space.__class__.__name__ for space in spaces])\ndef test_concatenate(space):\n    def assert_type(lhs, rhs, n):\n        # Special case: if rhs is a list of scalars, lhs must be an np.ndarray\n        if np.isscalar(rhs[0]):\n            assert isinstance(lhs, np.ndarray)\n            assert all([np.isscalar(rhs[i]) for i in range(n)])\n        else:\n            assert all([isinstance(rhs[i], type(lhs)) for i in range(n)])\n\n    def assert_nested_equal(lhs, rhs, n):\n        assert isinstance(rhs, list)\n        assert (n > 0) and (len(rhs) == n)\n        assert_type(lhs, rhs, n)\n        if isinstance(lhs, np.ndarray):\n            assert lhs.shape[0] == n\n            for i in range(n):\n                assert np.all(lhs[i] == rhs[i])\n\n        elif isinstance(lhs, tuple):\n            for i in range(len(lhs)):\n                rhs_T_i = [rhs[j][i] for j in range(n)]\n                assert_nested_equal(lhs[i], rhs_T_i, n)\n\n        elif isinstance(lhs, OrderedDict):\n            for key in lhs.keys():\n                rhs_T_key = [rhs[j][key] for j in range(n)]\n                assert_nested_equal(lhs[key], rhs_T_key, n)\n\n        else:\n            raise TypeError('Got unknown type `{0}`.'.format(type(lhs)))\n\n    samples = [space.sample() for _ in range(8)]\n    array = create_empty_array(space, n=8)\n    concatenated = concatenate(samples, array, space)\n\n    assert np.all(concatenated == array)\n    assert_nested_equal(array, samples, n=8)\n\n\n@pytest.mark.parametrize('n', [1, 8])\n@pytest.mark.parametrize('space', spaces,\n    ids=[space.__class__.__name__ for space in spaces])\ndef test_create_empty_array(space, n):\n\n    def assert_nested_type(arr, space, n):\n        if isinstance(space, _BaseGymSpaces):\n            assert isinstance(arr, np.ndarray)\n            assert arr.dtype == space.dtype\n            assert arr.shape == (n,) + space.shape\n\n        elif isinstance(space, Tuple):\n            assert isinstance(arr, tuple)\n            assert len(arr) == len(space.spaces)\n            for i in range(len(arr)):\n                assert_nested_type(arr[i], space.spaces[i], n)\n\n        elif isinstance(space, Dict):\n            assert isinstance(arr, OrderedDict)\n            assert set(arr.keys()) ^ set(space.spaces.keys()) == set()\n            for key in arr.keys():\n                assert_nested_type(arr[key], space.spaces[key], n)\n\n        else:\n            raise TypeError('Got unknown type `{0}`.'.format(type(arr)))\n\n    array = create_empty_array(space, n=n, fn=np.empty)\n    assert_nested_type(array, space, n=n)\n\n\n@pytest.mark.parametrize('n', [1, 8])\n@pytest.mark.parametrize('space', spaces,\n    ids=[space.__class__.__name__ for space in spaces])\ndef test_create_empty_array_zeros(space, n):\n\n    def assert_nested_type(arr, space, n):\n        if isinstance(space, _BaseGymSpaces):\n            assert isinstance(arr, np.ndarray)\n            assert arr.dtype == space.dtype\n            assert arr.shape == (n,) + space.shape\n            assert np.all(arr == 0)\n\n        elif isinstance(space, Tuple):\n            assert isinstance(arr, tuple)\n            assert len(arr) == len(space.spaces)\n            for i in range(len(arr)):\n                assert_nested_type(arr[i], space.spaces[i], n)\n\n        elif isinstance(space, Dict):\n            assert isinstance(arr, OrderedDict)\n            assert set(arr.keys()) ^ set(space.spaces.keys()) == set()\n            for key in arr.keys():\n                assert_nested_type(arr[key], space.spaces[key], n)\n\n        else:\n            raise TypeError('Got unknown type `{0}`.'.format(type(arr)))\n\n    array = create_empty_array(space, n=n, fn=np.zeros)\n    assert_nested_type(array, space, n=n)\n\n\n@pytest.mark.parametrize('space', spaces,\n    ids=[space.__class__.__name__ for space in spaces])\ndef test_create_empty_array_none_shape_ones(space):\n\n    def assert_nested_type(arr, space):\n        if isinstance(space, _BaseGymSpaces):\n            assert isinstance(arr, np.ndarray)\n            assert arr.dtype == space.dtype\n            assert arr.shape == space.shape\n            assert np.all(arr == 1)\n\n        elif isinstance(space, Tuple):\n            assert isinstance(arr, tuple)\n            assert len(arr) == len(space.spaces)\n            for i in range(len(arr)):\n                assert_nested_type(arr[i], space.spaces[i])\n\n        elif isinstance(space, Dict):\n            assert isinstance(arr, OrderedDict)\n            assert set(arr.keys()) ^ set(space.spaces.keys()) == set()\n            for key in arr.keys():\n                assert_nested_type(arr[key], space.spaces[key])\n\n        else:\n            raise TypeError('Got unknown type `{0}`.'.format(type(arr)))\n\n    array = create_empty_array(space, n=None, fn=np.ones)\n    assert_nested_type(array, space)",
        "import pytest\nimport sys\nimport numpy as np\n\nimport multiprocessing as mp\nfrom multiprocessing.sharedctypes import SynchronizedArray\nfrom multiprocessing import Array, Process\nfrom collections import OrderedDict\n\nfrom gym.spaces import Tuple, Dict\nfrom gym.vector.utils.spaces import _BaseGymSpaces\nfrom gym.vector.tests.utils import spaces\n\nfrom gym.vector.utils.shared_memory import (create_shared_memory,\n    read_from_shared_memory, write_to_shared_memory)\n\nis_python_2 = (sys.version_info < (3, 0))\n\nexpected_types = [\n    Array('d', 1), Array('f', 1), Array('f', 3), Array('f', 4), Array('B', 1), Array('B', 32 * 32 * 3),\n    Array('i', 1), (Array('i', 1), Array('i', 1)), (Array('i', 1), Array('f', 2)),\n    Array('B', 3), Array('B', 19),\n    OrderedDict([\n        ('position', Array('i', 1)),\n        ('velocity', Array('f', 1))\n    ]),\n    OrderedDict([\n        ('position', OrderedDict([('x', Array('i', 1)), ('y', Array('i', 1))])),\n        ('velocity', (Array('i', 1), Array('B', 1)))\n    ])\n]\n\n@pytest.mark.parametrize('n', [1, 8])\n@pytest.mark.parametrize('space,expected_type', list(zip(spaces, expected_types)),\n    ids=[space.__class__.__name__ for space in spaces])\n@pytest.mark.parametrize('ctx', [None,\n    pytest.param('fork', marks=pytest.mark.skipif(is_python_2, reason='Requires Python 3')),\n    pytest.param('spawn', marks=pytest.mark.skipif(is_python_2, reason='Requires Python 3'))],\n    ids=['default', 'fork', 'spawn'])\ndef test_create_shared_memory(space, expected_type, n, ctx):\n    def assert_nested_type(lhs, rhs, n):\n        assert type(lhs) == type(rhs)\n        if isinstance(lhs, (list, tuple)):\n            assert len(lhs) == len(rhs)\n            for lhs_, rhs_ in zip(lhs, rhs):\n                assert_nested_type(lhs_, rhs_, n)\n\n        elif isinstance(lhs, (dict, OrderedDict)):\n            assert set(lhs.keys()) ^ set(rhs.keys()) == set()\n            for key in lhs.keys():\n                assert_nested_type(lhs[key], rhs[key], n)\n\n        elif isinstance(lhs, SynchronizedArray):\n            # Assert the length of the array\n            assert len(lhs[:]) == n * len(rhs[:])\n            # Assert the data type\n            assert type(lhs[0]) == type(rhs[0])\n\n        else:\n            raise TypeError('Got unknown type `{0}`.'.format(type(lhs)))\n\n    ctx = mp if (ctx is None) else mp.get_context(ctx)\n    shared_memory = create_shared_memory(space, n=n, ctx=ctx)\n    assert_nested_type(shared_memory, expected_type, n=n)\n\n\n@pytest.mark.parametrize('space', spaces,\n    ids=[space.__class__.__name__ for space in spaces])\ndef test_write_to_shared_memory(space):\n\n    def assert_nested_equal(lhs, rhs):\n        assert isinstance(rhs, list)\n        if isinstance(lhs, (list, tuple)):\n            for i in range(len(lhs)):\n                assert_nested_equal(lhs[i], [rhs_[i] for rhs_ in rhs])\n\n        elif isinstance(lhs, (dict, OrderedDict)):\n            for key in lhs.keys():\n                assert_nested_equal(lhs[key], [rhs_[key] for rhs_ in rhs])\n\n        elif isinstance(lhs, SynchronizedArray):\n            assert np.all(np.array(lhs[:]) == np.stack(rhs, axis=0).flatten())\n\n        else:\n            raise TypeError('Got unknown type `{0}`.'.format(type(lhs)))\n\n    def write(i, shared_memory, sample):\n        write_to_shared_memory(i, sample, shared_memory, space)\n\n    shared_memory_n8 = create_shared_memory(space, n=8)\n    samples = [space.sample() for _ in range(8)]\n\n    processes = [Process(target=write, args=(i, shared_memory_n8,\n        samples[i])) for i in range(8)]\n\n    for process in processes:\n        process.start()\n    for process in processes:\n        process.join()\n\n    assert_nested_equal(shared_memory_n8, samples)\n\n\n@pytest.mark.parametrize('space', spaces,\n    ids=[space.__class__.__name__ for space in spaces])\ndef test_read_from_shared_memory(space):\n\n    def assert_nested_equal(lhs, rhs, space, n):\n        assert isinstance(rhs, list)\n        if isinstance(space, Tuple):\n            assert isinstance(lhs, tuple)\n            for i in range(len(lhs)):\n                assert_nested_equal(lhs[i], [rhs_[i] for rhs_ in rhs],\n                    space.spaces[i], n)\n\n        elif isinstance(space, Dict):\n            assert isinstance(lhs, OrderedDict)\n            for key in lhs.keys():\n                assert_nested_equal(lhs[key], [rhs_[key] for rhs_ in rhs],\n                    space.spaces[key], n)\n\n        elif isinstance(space, _BaseGymSpaces):\n            assert isinstance(lhs, np.ndarray)\n            assert lhs.shape == ((n,) + space.shape)\n            assert lhs.dtype == space.dtype\n            assert np.all(lhs == np.stack(rhs, axis=0))\n\n        else:\n            raise TypeError('Got unknown type `{0}`'.format(type(space)))\n\n    def write(i, shared_memory, sample):\n        write_to_shared_memory(i, sample, shared_memory, space)\n\n    shared_memory_n8 = create_shared_memory(space, n=8)\n    memory_view_n8 = read_from_shared_memory(shared_memory_n8, space, n=8)\n    samples = [space.sample() for _ in range(8)]\n\n    processes = [Process(target=write, args=(i, shared_memory_n8,\n        samples[i])) for i in range(8)]\n\n    for process in processes:\n        process.start()\n    for process in processes:\n        process.join()\n\n    assert_nested_equal(memory_view_n8, samples, space, n=8)",
        "import pytest\nimport numpy as np\n\nfrom multiprocessing import TimeoutError\nfrom gym.spaces import Box\nfrom gym.error import (AlreadyPendingCallError, NoAsyncCallError,\n                       ClosedEnvironmentError)\nfrom gym.vector.tests.utils import make_env, make_slow_env\n\nfrom gym.vector.async_vector_env import AsyncVectorEnv\n\n@pytest.mark.parametrize('shared_memory', [True, False])\ndef test_create_async_vector_env(shared_memory):\n    env_fns = [make_env('CubeCrash-v0', i) for i in range(8)]\n    try:\n        env = AsyncVectorEnv(env_fns, shared_memory=shared_memory)\n    finally:\n        env.close()\n\n    assert env.num_envs == 8\n\n\n@pytest.mark.parametrize('shared_memory', [True, False])\ndef test_reset_async_vector_env(shared_memory):\n    env_fns = [make_env('CubeCrash-v0', i) for i in range(8)]\n    try:\n        env = AsyncVectorEnv(env_fns, shared_memory=shared_memory)\n        observations = env.reset()\n    finally:\n        env.close()\n\n    assert isinstance(env.observation_space, Box)\n    assert isinstance(observations, np.ndarray)\n    assert observations.dtype == env.observation_space.dtype\n    assert observations.shape == (8,) + env.single_observation_space.shape\n    assert observations.shape == env.observation_space.shape\n\n\n@pytest.mark.parametrize('shared_memory', [True, False])\n@pytest.mark.parametrize('use_single_action_space', [True, False])\ndef test_step_async_vector_env(shared_memory, use_single_action_space):\n    env_fns = [make_env('CubeCrash-v0', i) for i in range(8)]\n    try:\n        env = AsyncVectorEnv(env_fns, shared_memory=shared_memory)\n        observations = env.reset()\n        if use_single_action_space:\n            actions = [env.single_action_space.sample() for _ in range(8)]\n        else:\n            actions = env.action_space.sample()\n        observations, rewards, dones, _ = env.step(actions)\n    finally:\n        env.close()\n\n    assert isinstance(env.observation_space, Box)\n    assert isinstance(observations, np.ndarray)\n    assert observations.dtype == env.observation_space.dtype\n    assert observations.shape == (8,) + env.single_observation_space.shape\n    assert observations.shape == env.observation_space.shape\n\n    assert isinstance(rewards, np.ndarray)\n    assert isinstance(rewards[0], (float, np.floating))\n    assert rewards.ndim == 1\n    assert rewards.size == 8\n\n    assert isinstance(dones, np.ndarray)\n    assert dones.dtype == np.bool_\n    assert dones.ndim == 1\n    assert dones.size == 8\n\n\n@pytest.mark.parametrize('shared_memory', [True, False])\ndef test_copy_async_vector_env(shared_memory):\n    env_fns = [make_env('CubeCrash-v0', i) for i in range(8)]\n    try:\n        env = AsyncVectorEnv(env_fns, shared_memory=shared_memory,\n                             copy=True)\n        observations = env.reset()\n        observations[0] = 128\n        assert not np.all(env.observations[0] == 128)\n    finally:\n        env.close()\n\n\n@pytest.mark.parametrize('shared_memory', [True, False])\ndef test_no_copy_async_vector_env(shared_memory):\n    env_fns = [make_env('CubeCrash-v0', i) for i in range(8)]\n    try:\n        env = AsyncVectorEnv(env_fns, shared_memory=shared_memory,\n                             copy=False)\n        observations = env.reset()\n        observations[0] = 128\n        assert np.all(env.observations[0] == 128)\n    finally:\n        env.close()\n\n\n@pytest.mark.parametrize('shared_memory', [True, False])\ndef test_reset_timeout_async_vector_env(shared_memory):\n    env_fns = [make_slow_env(0.3, i) for i in range(4)]\n    with pytest.raises(TimeoutError):\n        try:\n            env = AsyncVectorEnv(env_fns, shared_memory=shared_memory)\n            env.reset_async()\n            observations = env.reset_wait(timeout=0.1)\n        finally:\n            env.close(terminate=True)\n\n\n@pytest.mark.parametrize('shared_memory', [True, False])\ndef test_step_timeout_async_vector_env(shared_memory):\n    env_fns = [make_slow_env(0., i) for i in range(4)]\n    with pytest.raises(TimeoutError):\n        try:\n            env = AsyncVectorEnv(env_fns, shared_memory=shared_memory)\n            observations = env.reset()\n            env.step_async([0.1, 0.1, 0.3, 0.1])\n            observations, rewards, dones, _ = env.step_wait(timeout=0.1)\n        finally:\n            env.close(terminate=True)\n\n\n@pytest.mark.filterwarnings('ignore::UserWarning')\n@pytest.mark.parametrize('shared_memory', [True, False])\ndef test_reset_out_of_order_async_vector_env(shared_memory):\n    env_fns = [make_env('CubeCrash-v0', i) for i in range(4)]\n    with pytest.raises(NoAsyncCallError):\n        try:\n            env = AsyncVectorEnv(env_fns, shared_memory=shared_memory)\n            observations = env.reset_wait()\n        except NoAsyncCallError as exception:\n            assert exception.name == 'reset'\n            raise\n        finally:\n            env.close(terminate=True)\n\n    with pytest.raises(AlreadyPendingCallError):\n        try:\n            env = AsyncVectorEnv(env_fns, shared_memory=shared_memory)\n            actions = env.action_space.sample()\n            observations = env.reset()\n            env.step_async(actions)\n            env.reset_async()\n        except NoAsyncCallError as exception:\n            assert exception.name == 'step'\n            raise\n        finally:\n            env.close(terminate=True)\n\n\n@pytest.mark.filterwarnings('ignore::UserWarning')\n@pytest.mark.parametrize('shared_memory', [True, False])\ndef test_step_out_of_order_async_vector_env(shared_memory):\n    env_fns = [make_env('CubeCrash-v0', i) for i in range(4)]\n    with pytest.raises(NoAsyncCallError):\n        try:\n            env = AsyncVectorEnv(env_fns, shared_memory=shared_memory)\n            actions = env.action_space.sample()\n            observations = env.reset()\n            observations, rewards, dones, infos = env.step_wait()\n        except AlreadyPendingCallError as exception:\n            assert exception.name == 'step'\n            raise\n        finally:\n            env.close(terminate=True)\n\n    with pytest.raises(AlreadyPendingCallError):\n        try:\n            env = AsyncVectorEnv(env_fns, shared_memory=shared_memory)\n            actions = env.action_space.sample()\n            env.reset_async()\n            env.step_async(actions)\n        except AlreadyPendingCallError as exception:\n            assert exception.name == 'reset'\n            raise\n        finally:\n            env.close(terminate=True)\n\n\n@pytest.mark.parametrize('shared_memory', [True, False])\ndef test_already_closed_async_vector_env(shared_memory):\n    env_fns = [make_env('CubeCrash-v0', i) for i in range(4)]\n    with pytest.raises(ClosedEnvironmentError):\n        env = AsyncVectorEnv(env_fns, shared_memory=shared_memory)\n        env.close()\n        observations = env.reset()\n\n\n@pytest.mark.parametrize('shared_memory', [True, False])\ndef test_check_observations_async_vector_env(shared_memory):\n    # CubeCrash-v0 - observation_space: Box(40, 32, 3)\n    env_fns = [make_env('CubeCrash-v0', i) for i in range(8)]\n    # MemorizeDigits-v0 - observation_space: Box(24, 32, 3)\n    env_fns[1] = make_env('MemorizeDigits-v0', 1)\n    with pytest.raises(RuntimeError):\n        env = AsyncVectorEnv(env_fns, shared_memory=shared_memory)\n        env.close(terminate=True)",
        "",
        "import pytest\nimport numpy as np\n\nfrom gym.spaces import Box\nfrom gym.vector.tests.utils import make_env\n\nfrom gym.vector.sync_vector_env import SyncVectorEnv\n\ndef test_create_sync_vector_env():\n    env_fns = [make_env('CubeCrash-v0', i) for i in range(8)]\n    try:\n        env = SyncVectorEnv(env_fns)\n    finally:\n        env.close()\n\n    assert env.num_envs == 8\n\n\ndef test_reset_sync_vector_env():\n    env_fns = [make_env('CubeCrash-v0', i) for i in range(8)]\n    try:\n        env = SyncVectorEnv(env_fns)\n        observations = env.reset()\n    finally:\n        env.close()\n\n    assert isinstance(env.observation_space, Box)\n    assert isinstance(observations, np.ndarray)\n    assert observations.dtype == env.observation_space.dtype\n    assert observations.shape == (8,) + env.single_observation_space.shape\n    assert observations.shape == env.observation_space.shape\n\n\n@pytest.mark.parametrize('use_single_action_space', [True, False])\ndef test_step_sync_vector_env(use_single_action_space):\n    env_fns = [make_env('CubeCrash-v0', i) for i in range(8)]\n    try:\n        env = SyncVectorEnv(env_fns)\n        observations = env.reset()\n        if use_single_action_space:\n            actions = [env.single_action_space.sample() for _ in range(8)]\n        else:\n            actions = env.action_space.sample()\n        observations, rewards, dones, _ = env.step(actions)\n    finally:\n        env.close()\n\n    assert isinstance(env.observation_space, Box)\n    assert isinstance(observations, np.ndarray)\n    assert observations.dtype == env.observation_space.dtype\n    assert observations.shape == (8,) + env.single_observation_space.shape\n    assert observations.shape == env.observation_space.shape\n\n    assert isinstance(rewards, np.ndarray)\n    assert isinstance(rewards[0], (float, np.floating))\n    assert rewards.ndim == 1\n    assert rewards.size == 8\n\n    assert isinstance(dones, np.ndarray)\n    assert dones.dtype == np.bool_\n    assert dones.ndim == 1\n    assert dones.size == 8\n\n\ndef test_check_observations_sync_vector_env():\n    # CubeCrash-v0 - observation_space: Box(40, 32, 3)\n    env_fns = [make_env('CubeCrash-v0', i) for i in range(8)]\n    # MemorizeDigits-v0 - observation_space: Box(24, 32, 3)\n    env_fns[1] = make_env('MemorizeDigits-v0', 1)\n    with pytest.raises(RuntimeError):\n        env = SyncVectorEnv(env_fns)\n        env.close()",
        "import numpy as np\nimport gym\nimport time\n\nfrom gym.spaces import Box, Discrete, MultiDiscrete, MultiBinary, Tuple, Dict\n\nspaces = [\n    Box(low=np.array(-1.), high=np.array(1.), dtype=np.float64),\n    Box(low=np.array([0.]), high=np.array([10.]), dtype=np.float32),\n    Box(low=np.array([-1., 0., 0.]), high=np.array([1., 1., 1.]), dtype=np.float32),\n    Box(low=np.array([[-1., 0.], [0., -1.]]), high=np.ones((2, 2)), dtype=np.float32),\n    Box(low=0, high=255, shape=(), dtype=np.uint8),\n    Box(low=0, high=255, shape=(32, 32, 3), dtype=np.uint8),\n    Discrete(2),\n    Tuple((Discrete(3), Discrete(5))),\n    Tuple((Discrete(7), Box(low=np.array([0., -1.]), high=np.array([1., 1.]), dtype=np.float32))),\n    MultiDiscrete([11, 13, 17]),\n    MultiBinary(19),\n    Dict({\n        'position': Discrete(23),\n        'velocity': Box(low=np.array([0.]), high=np.array([1.]), dtype=np.float32)\n    }),\n    Dict({\n        'position': Dict({'x': Discrete(29), 'y': Discrete(31)}),\n        'velocity': Tuple((Discrete(37), Box(low=0, high=255, shape=(), dtype=np.uint8)))\n    })\n]\n\nHEIGHT, WIDTH = 64, 64\n\nclass UnittestSlowEnv(gym.Env):\n    def __init__(self, slow_reset=0.3):\n        super(UnittestSlowEnv, self).__init__()\n        self.slow_reset = slow_reset\n        self.observation_space = Box(low=0, high=255,\n            shape=(HEIGHT, WIDTH, 3), dtype=np.uint8)\n        self.action_space = Box(low=0., high=1., shape=(), dtype=np.float32)\n\n    def reset(self):\n        if self.slow_reset > 0:\n            time.sleep(self.slow_reset)\n        return self.observation_space.sample()\n\n    def step(self, action):\n        time.sleep(action)\n        observation = self.observation_space.sample()\n        reward, done = 0., False\n        return observation, reward, done, {}\n\ndef make_env(env_name, seed):\n    def _make():\n        env = gym.make(env_name)\n        env.seed(seed)\n        return env\n    return _make\n\ndef make_slow_env(slow_reset, seed):\n    def _make():\n        env = UnittestSlowEnv(slow_reset=slow_reset)\n        env.seed(seed)\n        return env\n    return _make",
        "import pytest\nimport numpy as np\n\nfrom gym.vector.tests.utils import make_env\n\nfrom gym.vector.async_vector_env import AsyncVectorEnv\nfrom gym.vector.sync_vector_env import SyncVectorEnv\n\n@pytest.mark.parametrize('shared_memory', [True, False])\ndef test_vector_env_equal(shared_memory):\n    env_fns = [make_env('CubeCrash-v0', i) for i in range(4)]\n    num_steps = 100\n    try:\n        async_env = AsyncVectorEnv(env_fns, shared_memory=shared_memory)\n        sync_env = SyncVectorEnv(env_fns)\n\n        async_env.seed(0)\n        sync_env.seed(0)\n\n        assert async_env.num_envs == sync_env.num_envs\n        assert async_env.observation_space == sync_env.observation_space\n        assert async_env.single_observation_space == sync_env.single_observation_space\n        assert async_env.action_space == sync_env.action_space\n        assert async_env.single_action_space == sync_env.single_action_space\n\n        async_observations = async_env.reset()\n        sync_observations = sync_env.reset()\n        assert np.all(async_observations == sync_observations)\n\n        for _ in range(num_steps):\n            actions = async_env.action_space.sample()\n            assert actions in sync_env.action_space\n\n            async_observations, async_rewards, async_dones, _ = async_env.step(actions)\n            sync_observations, sync_rewards, sync_dones, _ = sync_env.step(actions)\n\n            assert np.all(async_observations == sync_observations)\n            assert np.all(async_rewards == sync_rewards)\n            assert np.all(async_dones == sync_dones)\n\n    finally:\n        async_env.close()\n        sync_env.close()",
        "import pytest\nimport numpy as np\n\nfrom gym.spaces import Box, MultiDiscrete, Tuple, Dict\nfrom gym.vector.tests.utils import spaces\n\nfrom gym.vector.utils.spaces import _BaseGymSpaces, batch_space\n\nexpected_batch_spaces_4 = [\n    Box(low=-1., high=1., shape=(4,), dtype=np.float64),\n    Box(low=0., high=10., shape=(4, 1), dtype=np.float32),\n    Box(low=np.array([[-1., 0., 0.], [-1., 0., 0.], [-1., 0., 0.], [-1., 0., 0.]]),\n        high=np.array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]), dtype=np.float32),\n    Box(low=np.array([[[-1., 0.], [0., -1.]], [[-1., 0.], [0., -1.]], [[-1., 0.], [0., -1]],\n        [[-1., 0.], [0., -1.]]]), high=np.ones((4, 2, 2)), dtype=np.float32),\n    Box(low=0, high=255, shape=(4,), dtype=np.uint8),\n    Box(low=0, high=255, shape=(4, 32, 32, 3), dtype=np.uint8),\n    MultiDiscrete([2, 2, 2, 2]),\n    Tuple((MultiDiscrete([3, 3, 3, 3]), MultiDiscrete([5, 5, 5, 5]))),\n    Tuple((MultiDiscrete([7, 7, 7, 7]), Box(low=np.array([[0., -1.], [0., -1.], [0., -1.], [0., -1]]),\n        high=np.array([[1., 1.], [1., 1.], [1., 1.], [1., 1.]]), dtype=np.float32))),\n    Box(low=np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]),\n        high=np.array([[10, 12, 16], [10, 12, 16], [10, 12, 16], [10, 12, 16]]), dtype=np.int64),\n    Box(low=0, high=1, shape=(4, 19), dtype=np.int8),\n    Dict({\n        'position': MultiDiscrete([23, 23, 23, 23]),\n        'velocity': Box(low=0., high=1., shape=(4, 1), dtype=np.float32)\n    }),\n    Dict({\n        'position': Dict({'x': MultiDiscrete([29, 29, 29, 29]), 'y': MultiDiscrete([31, 31, 31, 31])}),\n        'velocity': Tuple((MultiDiscrete([37, 37, 37, 37]), Box(low=0, high=255, shape=(4,), dtype=np.uint8)))\n    })\n]\n\n@pytest.mark.parametrize('space,expected_batch_space_4', list(zip(spaces,\n    expected_batch_spaces_4)), ids=[space.__class__.__name__ for space in spaces])\ndef test_batch_space(space, expected_batch_space_4):\n    batch_space_4 = batch_space(space, n=4)\n    assert batch_space_4 == expected_batch_space_4",
        "try:\n    from collections.abc import Iterable\nexcept ImportError:\n    Iterable = (tuple, list)\n\nfrom gym.vector.async_vector_env import AsyncVectorEnv\nfrom gym.vector.sync_vector_env import SyncVectorEnv\nfrom gym.vector.vector_env import VectorEnv\n\n__all__ = ['AsyncVectorEnv', 'SyncVectorEnv', 'VectorEnv', 'make']\n\ndef make(id, num_envs=1, asynchronous=True, wrappers=None, **kwargs):\n    \"\"\"Create a vectorized environment from multiple copies of an environment,\n    from its id\n\n    Parameters\n    ----------\n    id : str\n        The environment ID. This must be a valid ID from the registry.\n\n    num_envs : int\n        Number of copies of the environment. \n\n    asynchronous : bool (default: `True`)\n        If `True`, wraps the environments in an `AsyncVectorEnv` (which uses \n        `multiprocessing` to run the environments in parallel). If `False`,\n        wraps the environments in a `SyncVectorEnv`.\n        \n    wrappers : Callable or Iterable of Callables (default: `None`)\n        If not `None`, then apply the wrappers to each internal \n        environment during creation. \n\n    Returns\n    -------\n    env : `gym.vector.VectorEnv` instance\n        The vectorized environment.\n\n    Example\n    -------\n    >>> import gym\n    >>> env = gym.vector.make('CartPole-v1', 3)\n    >>> env.reset()\n    array([[-0.04456399,  0.04653909,  0.01326909, -0.02099827],\n           [ 0.03073904,  0.00145001, -0.03088818, -0.03131252],\n           [ 0.03468829,  0.01500225,  0.01230312,  0.01825218]],\n          dtype=float32)\n    \"\"\"\n    from gym.envs import make as make_\n    def _make_env():\n        env = make_(id, **kwargs)\n        if wrappers is not None:\n            if callable(wrappers):\n                env = wrappers(env)\n            elif isinstance(wrappers, Iterable) and all([callable(w) for w in wrappers]):\n                for wrapper in wrappers:\n                    env = wrapper(env)\n            else:\n                raise NotImplementedError\n        return env\n    env_fns = [_make_env for _ in range(num_envs)]\n    return AsyncVectorEnv(env_fns) if asynchronous else SyncVectorEnv(env_fns)",
        "import contextlib\nimport os\n\n__all__ = ['CloudpickleWrapper', 'clear_mpi_env_vars']\n\nclass CloudpickleWrapper(object):\n    def __init__(self, fn):\n        self.fn = fn\n\n    def __getstate__(self):\n        import cloudpickle\n        return cloudpickle.dumps(self.fn)\n\n    def __setstate__(self, ob):\n        import pickle\n        self.fn = pickle.loads(ob)\n\n    def __call__(self):\n        return self.fn()\n\n@contextlib.contextmanager\ndef clear_mpi_env_vars():\n    \"\"\"\n    `from mpi4py import MPI` will call `MPI_Init` by default. If the child\n    process has MPI environment variables, MPI will think that the child process\n    is an MPI process just like the parent and do bad things such as hang.\n    \n    This context manager is a hacky way to clear those environment variables\n    temporarily such as when we are starting multiprocessing Processes.\n    \"\"\"\n    removed_environment = {}\n    for k, v in list(os.environ.items()):\n        for prefix in ['OMPI_', 'PMI_']:\n            if k.startswith(prefix):\n                removed_environment[k] = v\n                del os.environ[k]\n    try:\n        yield\n    finally:\n        os.environ.update(removed_environment)",
        "import numpy as np\nimport multiprocessing as mp\nfrom ctypes import c_bool\nfrom collections import OrderedDict\n\nfrom gym import logger\nfrom gym.spaces import Tuple, Dict\nfrom gym.vector.utils.spaces import _BaseGymSpaces\n\n__all__ = [\n    'create_shared_memory',\n    'read_from_shared_memory',\n    'write_to_shared_memory'\n]\n\ndef create_shared_memory(space, n=1, ctx=mp):\n    \"\"\"Create a shared memory object, to be shared across processes. This\n    eventually contains the observations from the vectorized environment.\n\n    Parameters\n    ----------\n    space : `gym.spaces.Space` instance\n        Observation space of a single environment in the vectorized environment.\n\n    n : int\n        Number of environments in the vectorized environment (i.e. the number\n        of processes).\n\n    ctx : `multiprocessing` context\n        Context for multiprocessing.\n\n    Returns\n    -------\n    shared_memory : dict, tuple, or `multiprocessing.Array` instance\n        Shared object across processes.\n    \"\"\"\n    if isinstance(space, _BaseGymSpaces):\n        return create_base_shared_memory(space, n=n, ctx=ctx)\n    elif isinstance(space, Tuple):\n        return create_tuple_shared_memory(space, n=n, ctx=ctx)\n    elif isinstance(space, Dict):\n        return create_dict_shared_memory(space, n=n, ctx=ctx)\n    else:\n        raise NotImplementedError()\n\ndef create_base_shared_memory(space, n=1, ctx=mp):\n    dtype = space.dtype.char\n    if dtype in '?':\n        dtype = c_bool\n    return ctx.Array(dtype, n * int(np.prod(space.shape)))\n\ndef create_tuple_shared_memory(space, n=1, ctx=mp):\n    return tuple(create_shared_memory(subspace, n=n, ctx=ctx)\n        for subspace in space.spaces)\n\ndef create_dict_shared_memory(space, n=1, ctx=mp):\n    return OrderedDict([(key, create_shared_memory(subspace, n=n, ctx=ctx))\n        for (key, subspace) in space.spaces.items()])\n\n\ndef read_from_shared_memory(shared_memory, space, n=1):\n    \"\"\"Read the batch of observations from shared memory as a numpy array.\n\n    Parameters\n    ----------\n    shared_memory : dict, tuple, or `multiprocessing.Array` instance\n        Shared object across processes. This contains the observations from the\n        vectorized environment. This object is created with `create_shared_memory`.\n\n    space : `gym.spaces.Space` instance\n        Observation space of a single environment in the vectorized environment.\n\n    n : int\n        Number of environments in the vectorized environment (i.e. the number\n        of processes).\n\n    Returns\n    -------\n    observations : dict, tuple or `np.ndarray` instance\n        Batch of observations as a (possibly nested) numpy array.\n\n    Notes\n    -----\n    The numpy array objects returned by `read_from_shared_memory` shares the\n    memory of `shared_memory`. Any changes to `shared_memory` are forwarded\n    to `observations`, and vice-versa. To avoid any side-effect, use `np.copy`.\n    \"\"\"\n    if isinstance(space, _BaseGymSpaces):\n        return read_base_from_shared_memory(shared_memory, space, n=n)\n    elif isinstance(space, Tuple):\n        return read_tuple_from_shared_memory(shared_memory, space, n=n)\n    elif isinstance(space, Dict):\n        return read_dict_from_shared_memory(shared_memory, space, n=n)\n    else:\n        raise NotImplementedError()\n\ndef read_base_from_shared_memory(shared_memory, space, n=1):\n    return np.frombuffer(shared_memory.get_obj(),\n        dtype=space.dtype).reshape((n,) + space.shape)\n\ndef read_tuple_from_shared_memory(shared_memory, space, n=1):\n    return tuple(read_from_shared_memory(memory, subspace, n=n)\n        for (memory, subspace) in zip(shared_memory, space.spaces))\n\ndef read_dict_from_shared_memory(shared_memory, space, n=1):\n    return OrderedDict([(key, read_from_shared_memory(shared_memory[key],\n        subspace, n=n)) for (key, subspace) in space.spaces.items()])\n\n\ndef write_to_shared_memory(index, value, shared_memory, space):\n    \"\"\"Write the observation of a single environment into shared memory.\n\n    Parameters\n    ----------\n    index : int\n        Index of the environment (must be in `[0, num_envs)`).\n\n    value : sample from `space`\n        Observation of the single environment to write to shared memory.\n\n    shared_memory : dict, tuple, or `multiprocessing.Array` instance\n        Shared object across processes. This contains the observations from the\n        vectorized environment. This object is created with `create_shared_memory`.\n\n    space : `gym.spaces.Space` instance\n        Observation space of a single environment in the vectorized environment.\n\n    Returns\n    -------\n    `None`\n    \"\"\"\n    if isinstance(space, _BaseGymSpaces):\n        write_base_to_shared_memory(index, value, shared_memory, space)\n    elif isinstance(space, Tuple):\n        write_tuple_to_shared_memory(index, value, shared_memory, space)\n    elif isinstance(space, Dict):\n        write_dict_to_shared_memory(index, value, shared_memory, space)\n    else:\n        raise NotImplementedError()\n\ndef write_base_to_shared_memory(index, value, shared_memory, space):\n    size = int(np.prod(space.shape))\n    destination = np.frombuffer(shared_memory.get_obj(), dtype=space.dtype)\n    np.copyto(destination[index * size:(index + 1) * size], np.asarray(\n        value, dtype=space.dtype).flatten())\n\ndef write_tuple_to_shared_memory(index, values, shared_memory, space):\n    for value, memory, subspace in zip(values, shared_memory, space.spaces):\n        write_to_shared_memory(index, value, memory, subspace)\n\ndef write_dict_to_shared_memory(index, values, shared_memory, space):\n    for key, subspace in space.spaces.items():\n        write_to_shared_memory(index, values[key], shared_memory[key], subspace)",
        "import numpy as np\n\nfrom gym.spaces import Tuple, Dict\nfrom gym.vector.utils.spaces import _BaseGymSpaces\nfrom collections import OrderedDict\n\n__all__ = ['concatenate', 'create_empty_array']\n\ndef concatenate(items, out, space):\n    \"\"\"Concatenate multiple samples from space into a single object.\n\n    Parameters\n    ----------\n    items : iterable of samples of `space`\n        Samples to be concatenated.\n\n    out : tuple, dict, or `np.ndarray`\n        The output object. This object is a (possibly nested) numpy array.\n\n    space : `gym.spaces.Space` instance\n        Observation space of a single environment in the vectorized environment.\n\n    Returns\n    -------\n    out : tuple, dict, or `np.ndarray`\n        The output object. This object is a (possibly nested) numpy array.\n\n    Example\n    -------\n    >>> from gym.spaces import Box\n    >>> space = Box(low=0, high=1, shape=(3,), dtype=np.float32)\n    >>> out = np.zeros((2, 3), dtype=np.float32)\n    >>> items = [space.sample() for _ in range(2)]\n    >>> concatenate(items, out, space)\n    array([[0.6348213 , 0.28607962, 0.60760117],\n           [0.87383074, 0.192658  , 0.2148103 ]], dtype=float32)\n    \"\"\"\n    assert isinstance(items, (list, tuple))\n    if isinstance(space, _BaseGymSpaces):\n        return concatenate_base(items, out, space)\n    elif isinstance(space, Tuple):\n        return concatenate_tuple(items, out, space)\n    elif isinstance(space, Dict):\n        return concatenate_dict(items, out, space)\n    else:\n        raise NotImplementedError()\n\ndef concatenate_base(items, out, space):\n    return np.stack(items, axis=0, out=out)\n\ndef concatenate_tuple(items, out, space):\n    return tuple(concatenate([item[i] for item in items],\n        out[i], subspace) for (i, subspace) in enumerate(space.spaces))\n\ndef concatenate_dict(items, out, space):\n    return OrderedDict([(key, concatenate([item[key] for item in items],\n        out[key], subspace)) for (key, subspace) in space.spaces.items()])\n\n\ndef create_empty_array(space, n=1, fn=np.zeros):\n    \"\"\"Create an empty (possibly nested) numpy array.\n\n    Parameters\n    ----------\n    space : `gym.spaces.Space` instance\n        Observation space of a single environment in the vectorized environment.\n\n    n : int\n        Number of environments in the vectorized environment. If `None`, creates\n        an empty sample from `space`.\n\n    fn : callable\n        Function to apply when creating the empty numpy array. Examples of such\n        functions are `np.empty` or `np.zeros`.\n\n    Returns\n    -------\n    out : tuple, dict, or `np.ndarray`\n        The output object. This object is a (possibly nested) numpy array.\n\n    Example\n    -------\n    >>> from gym.spaces import Box, Dict\n    >>> space = Dict({\n    ... 'position': Box(low=0, high=1, shape=(3,), dtype=np.float32),\n    ... 'velocity': Box(low=0, high=1, shape=(2,), dtype=np.float32)})\n    >>> create_empty_array(space, n=2, fn=np.zeros)\n    OrderedDict([('position', array([[0., 0., 0.],\n                                     [0., 0., 0.]], dtype=float32)),\n                 ('velocity', array([[0., 0.],\n                                     [0., 0.]], dtype=float32))])\n    \"\"\"\n    if isinstance(space, _BaseGymSpaces):\n        return create_empty_array_base(space, n=n, fn=fn)\n    elif isinstance(space, Tuple):\n        return create_empty_array_tuple(space, n=n, fn=fn)\n    elif isinstance(space, Dict):\n        return create_empty_array_dict(space, n=n, fn=fn)\n    else:\n        raise NotImplementedError()\n\ndef create_empty_array_base(space, n=1, fn=np.zeros):\n    shape = space.shape if (n is None) else (n,) + space.shape\n    return fn(shape, dtype=space.dtype)\n\ndef create_empty_array_tuple(space, n=1, fn=np.zeros):\n    return tuple(create_empty_array(subspace, n=n, fn=fn)\n        for subspace in space.spaces)\n\ndef create_empty_array_dict(space, n=1, fn=np.zeros):\n    return OrderedDict([(key, create_empty_array(subspace, n=n, fn=fn))\n        for (key, subspace) in space.spaces.items()])",
        "from gym.vector.utils.misc import CloudpickleWrapper, clear_mpi_env_vars\nfrom gym.vector.utils.numpy_utils import concatenate, create_empty_array\nfrom gym.vector.utils.shared_memory import create_shared_memory, read_from_shared_memory, write_to_shared_memory\nfrom gym.vector.utils.spaces import _BaseGymSpaces, batch_space\n\n__all__ = [\n    'CloudpickleWrapper',\n    'clear_mpi_env_vars',\n    'concatenate',\n    'create_empty_array',\n    'create_shared_memory',\n    'read_from_shared_memory',\n    'write_to_shared_memory',\n    '_BaseGymSpaces',\n    'batch_space'\n]",
        "import numpy as np\nfrom collections import OrderedDict\n\nfrom gym.spaces import Box, Discrete, MultiDiscrete, MultiBinary, Tuple, Dict\n\n_BaseGymSpaces = (Box, Discrete, MultiDiscrete, MultiBinary)\n__all__ = ['_BaseGymSpaces', 'batch_space']\n\ndef batch_space(space, n=1):\n    \"\"\"Create a (batched) space, containing multiple copies of a single space.\n\n    Parameters\n    ----------\n    space : `gym.spaces.Space` instance\n        Space (e.g. the observation space) for a single environment in the\n        vectorized environment.\n\n    n : int\n        Number of environments in the vectorized environment.\n\n    Returns\n    -------\n    batched_space : `gym.spaces.Space` instance\n        Space (e.g. the observation space) for a batch of environments in the\n        vectorized environment.\n\n    Example\n    -------\n    >>> from gym.spaces import Box, Dict\n    >>> space = Dict({\n    ... 'position': Box(low=0, high=1, shape=(3,), dtype=np.float32),\n    ... 'velocity': Box(low=0, high=1, shape=(2,), dtype=np.float32)})\n    >>> batch_space(space, n=5)\n    Dict(position:Box(5, 3), velocity:Box(5, 2))\n    \"\"\"\n    if isinstance(space, _BaseGymSpaces):\n        return batch_space_base(space, n=n)\n    elif isinstance(space, Tuple):\n        return batch_space_tuple(space, n=n)\n    elif isinstance(space, Dict):\n        return batch_space_dict(space, n=n)\n    else:\n        raise NotImplementedError()\n\ndef batch_space_base(space, n=1):\n    if isinstance(space, Box):\n        repeats = tuple([n] + [1] * space.low.ndim)\n        low, high = np.tile(space.low, repeats), np.tile(space.high, repeats)\n        return Box(low=low, high=high, dtype=space.dtype)\n\n    elif isinstance(space, Discrete):\n        return MultiDiscrete(np.full((n,), space.n, dtype=space.dtype))\n\n    elif isinstance(space, MultiDiscrete):\n        repeats = tuple([n] + [1] * space.nvec.ndim)\n        high = np.tile(space.nvec, repeats) - 1\n        return Box(low=np.zeros_like(high), high=high, dtype=space.dtype)\n\n    elif isinstance(space, MultiBinary):\n        return Box(low=0, high=1, shape=(n,) + space.shape, dtype=space.dtype)\n\n    else:\n        raise NotImplementedError()\n\ndef batch_space_tuple(space, n=1):\n    return Tuple(tuple(batch_space(subspace, n=n) for subspace in space.spaces))\n\ndef batch_space_dict(space, n=1):\n    return Dict(OrderedDict([(key, batch_space(subspace, n=n))\n        for (key, subspace) in space.spaces.items()]))",
        "import numpy as np\nimport multiprocessing as mp\nimport time\nimport sys\nfrom enum import Enum\nfrom copy import deepcopy\n\nfrom gym import logger\nfrom gym.vector.vector_env import VectorEnv\nfrom gym.error import (AlreadyPendingCallError, NoAsyncCallError,\n                       ClosedEnvironmentError)\nfrom gym.vector.utils import (create_shared_memory, create_empty_array,\n                              write_to_shared_memory, read_from_shared_memory,\n                              concatenate, CloudpickleWrapper, clear_mpi_env_vars)\n\n__all__ = ['AsyncVectorEnv']\n\n\nclass AsyncState(Enum):\n    DEFAULT = 'default'\n    WAITING_RESET = 'reset'\n    WAITING_STEP = 'step'\n\n\nclass AsyncVectorEnv(VectorEnv):\n    \"\"\"Vectorized environment that runs multiple environments in parallel. It\n    uses `multiprocessing` processes, and pipes for communication.\n\n    Parameters\n    ----------\n    env_fns : iterable of callable\n        Functions that create the environments.\n\n    observation_space : `gym.spaces.Space` instance, optional\n        Observation space of a single environment. If `None`, then the\n        observation space of the first environment is taken.\n\n    action_space : `gym.spaces.Space` instance, optional\n        Action space of a single environment. If `None`, then the action space\n        of the first environment is taken.\n\n    shared_memory : bool (default: `True`)\n        If `True`, then the observations from the worker processes are\n        communicated back through shared variables. This can improve the\n        efficiency if the observations are large (e.g. images).\n\n    copy : bool (default: `True`)\n        If `True`, then the `reset` and `step` methods return a copy of the\n        observations.\n\n    context : str, optional\n        Context for multiprocessing. If `None`, then the default context is used.\n        Only available in Python 3.\n\n    daemon : bool (default: `True`)\n        If `True`, then subprocesses have `daemon` flag turned on; that is, they\n        will quit if the head process quits. However, `daemon=True` prevents\n        subprocesses to spawn children, so for some environments you may want\n        to have it set to `False`\n\n    worker : function, optional\n        WARNING - advanced mode option! If set, then use that worker in a subprocess\n        instead of a default one. Can be useful to override some inner vector env\n        logic, for instance, how resets on done are handled. Provides high\n        degree of flexibility and a high chance to shoot yourself in the foot; thus,\n        if you are writing your own worker, it is recommended to start from the code\n        for `_worker` (or `_worker_shared_memory`) method below, and add changes\n    \"\"\"\n    def __init__(self, env_fns, observation_space=None, action_space=None,\n                 shared_memory=True, copy=True, context=None, daemon=True, worker=None):\n        try:\n            ctx = mp.get_context(context)\n        except AttributeError:\n            logger.warn('Context switching for `multiprocessing` is not '\n                'available in Python 2. Using the default context.')\n            ctx = mp\n        self.env_fns = env_fns\n        self.shared_memory = shared_memory\n        self.copy = copy\n\n        if (observation_space is None) or (action_space is None):\n            dummy_env = env_fns[0]()\n            observation_space = observation_space or dummy_env.observation_space\n            action_space = action_space or dummy_env.action_space\n            dummy_env.close()\n            del dummy_env\n        super(AsyncVectorEnv, self).__init__(num_envs=len(env_fns),\n            observation_space=observation_space, action_space=action_space)\n\n        if self.shared_memory:\n            _obs_buffer = create_shared_memory(self.single_observation_space,\n                n=self.num_envs, ctx=ctx)\n            self.observations = read_from_shared_memory(_obs_buffer,\n                self.single_observation_space, n=self.num_envs)\n        else:\n            _obs_buffer = None\n            self.observations = create_empty_array(\n            \tself.single_observation_space, n=self.num_envs, fn=np.zeros)\n\n        self.parent_pipes, self.processes = [], []\n        self.error_queue = ctx.Queue()\n        target = _worker_shared_memory if self.shared_memory else _worker\n        target = worker or target\n        with clear_mpi_env_vars():\n            for idx, env_fn in enumerate(self.env_fns):\n                parent_pipe, child_pipe = ctx.Pipe()\n                process = ctx.Process(target=target,\n                    name='Worker<{0}>-{1}'.format(type(self).__name__, idx),\n                    args=(idx, CloudpickleWrapper(env_fn), child_pipe,\n                    parent_pipe, _obs_buffer, self.error_queue))\n\n                self.parent_pipes.append(parent_pipe)\n                self.processes.append(process)\n\n                process.daemon = daemon\n                process.start()\n                child_pipe.close()\n\n        self._state = AsyncState.DEFAULT\n        self._check_observation_spaces()\n\n    def seed(self, seeds=None):\n        self._assert_is_running()\n        if seeds is None:\n            seeds = [None for _ in range(self.num_envs)]\n        if isinstance(seeds, int):\n            seeds = [seeds + i for i in range(self.num_envs)]\n        assert len(seeds) == self.num_envs\n\n        if self._state != AsyncState.DEFAULT:\n            raise AlreadyPendingCallError('Calling `seed` while waiting '\n                'for a pending call to `{0}` to complete.'.format(\n                self._state.value), self._state.value)\n\n        for pipe, seed in zip(self.parent_pipes, seeds):\n            pipe.send(('seed', seed))\n        _, successes = zip(*[pipe.recv() for pipe in self.parent_pipes])\n        self._raise_if_errors(successes)\n\n    def reset_async(self):\n        self._assert_is_running()\n        if self._state != AsyncState.DEFAULT:\n            raise AlreadyPendingCallError('Calling `reset_async` while waiting '\n                'for a pending call to `{0}` to complete'.format(\n                self._state.value), self._state.value)\n\n        for pipe in self.parent_pipes:\n            pipe.send(('reset', None))\n        self._state = AsyncState.WAITING_RESET\n\n    def reset_wait(self, timeout=None):\n        \"\"\"\n        Parameters\n        ----------\n        timeout : int or float, optional\n            Number of seconds before the call to `reset_wait` times out. If\n            `None`, the call to `reset_wait` never times out.\n\n        Returns\n        -------\n        observations : sample from `observation_space`\n            A batch of observations from the vectorized environment.\n        \"\"\"\n        self._assert_is_running()\n        if self._state != AsyncState.WAITING_RESET:\n            raise NoAsyncCallError('Calling `reset_wait` without any prior '\n                'call to `reset_async`.', AsyncState.WAITING_RESET.value)\n\n        if not self._poll(timeout):\n            self._state = AsyncState.DEFAULT\n            raise mp.TimeoutError('The call to `reset_wait` has timed out after '\n                '{0} second{1}.'.format(timeout, 's' if timeout > 1 else ''))\n\n        results, successes = zip(*[pipe.recv() for pipe in self.parent_pipes])\n        self._raise_if_errors(successes)\n        self._state = AsyncState.DEFAULT\n\n        if not self.shared_memory:\n            concatenate(results, self.observations, self.single_observation_space)\n\n        return deepcopy(self.observations) if self.copy else self.observations\n\n    def step_async(self, actions):\n        \"\"\"\n        Parameters\n        ----------\n        actions : iterable of samples from `action_space`\n            List of actions.\n        \"\"\"\n        self._assert_is_running()\n        if self._state != AsyncState.DEFAULT:\n            raise AlreadyPendingCallError('Calling `step_async` while waiting '\n                'for a pending call to `{0}` to complete.'.format(\n                self._state.value), self._state.value)\n\n        for pipe, action in zip(self.parent_pipes, actions):\n            pipe.send(('step', action))\n        self._state = AsyncState.WAITING_STEP\n\n    def step_wait(self, timeout=None):\n        \"\"\"\n        Parameters\n        ----------\n        timeout : int or float, optional\n            Number of seconds before the call to `step_wait` times out. If\n            `None`, the call to `step_wait` never times out.\n\n        Returns\n        -------\n        observations : sample from `observation_space`\n            A batch of observations from the vectorized environment.\n\n        rewards : `np.ndarray` instance (dtype `np.float_`)\n            A vector of rewards from the vectorized environment.\n\n        dones : `np.ndarray` instance (dtype `np.bool_`)\n            A vector whose entries indicate whether the episode has ended.\n\n        infos : list of dict\n            A list of auxiliary diagnostic information.\n        \"\"\"\n        self._assert_is_running()\n        if self._state != AsyncState.WAITING_STEP:\n            raise NoAsyncCallError('Calling `step_wait` without any prior call '\n                'to `step_async`.', AsyncState.WAITING_STEP.value)\n\n        if not self._poll(timeout):\n            self._state = AsyncState.DEFAULT\n            raise mp.TimeoutError('The call to `step_wait` has timed out after '\n                '{0} second{1}.'.format(timeout, 's' if timeout > 1 else ''))\n\n        results, successes = zip(*[pipe.recv() for pipe in self.parent_pipes])\n        self._raise_if_errors(successes)\n        self._state = AsyncState.DEFAULT\n        observations_list, rewards, dones, infos = zip(*results)\n\n        if not self.shared_memory:\n            concatenate(observations_list, self.observations,\n                self.single_observation_space)\n\n        return (deepcopy(self.observations) if self.copy else self.observations,\n                np.array(rewards), np.array(dones, dtype=np.bool_), infos)\n\n    def close_extras(self, timeout=None, terminate=False):\n        \"\"\"\n        Parameters\n        ----------\n        timeout : int or float, optional\n            Number of seconds before the call to `close` times out. If `None`,\n            the call to `close` never times out. If the call to `close` times\n            out, then all processes are terminated.\n\n        terminate : bool (default: `False`)\n            If `True`, then the `close` operation is forced and all processes\n            are terminated.\n        \"\"\"\n        timeout = 0 if terminate else timeout\n        try:\n            if self._state != AsyncState.DEFAULT:\n                logger.warn('Calling `close` while waiting for a pending '\n                    'call to `{0}` to complete.'.format(self._state.value))\n                function = getattr(self, '{0}_wait'.format(self._state.value))\n                function(timeout)\n        except mp.TimeoutError:\n            terminate = True\n\n        if terminate:\n            for process in self.processes:\n                if process.is_alive():\n                    process.terminate()\n        else:\n            for pipe in self.parent_pipes:\n                if (pipe is not None) and (not pipe.closed):\n                    pipe.send(('close', None))\n            for pipe in self.parent_pipes:\n                if (pipe is not None) and (not pipe.closed):\n                    pipe.recv()\n\n        for pipe in self.parent_pipes:\n            if pipe is not None:\n                pipe.close()\n        for process in self.processes:\n            process.join()\n\n    def _poll(self, timeout=None):\n        self._assert_is_running()\n        if timeout is None:\n            return True\n        end_time = time.time() + timeout\n        delta = None\n        for pipe in self.parent_pipes:\n            delta = max(end_time - time.time(), 0)\n            if pipe is None:\n                return False\n            if pipe.closed or (not pipe.poll(delta)):\n                return False\n        return True\n\n    def _check_observation_spaces(self):\n        self._assert_is_running()\n        for pipe in self.parent_pipes:\n            pipe.send(('_check_observation_space', self.single_observation_space))\n        same_spaces, successes = zip(*[pipe.recv() for pipe in self.parent_pipes])\n        self._raise_if_errors(successes)\n        if not all(same_spaces):\n            raise RuntimeError('Some environments have an observation space '\n                'different from `{0}`. In order to batch observations, the '\n                'observation spaces from all environments must be '\n                'equal.'.format(self.single_observation_space))\n\n    def _assert_is_running(self):\n        if self.closed:\n            raise ClosedEnvironmentError('Trying to operate on `{0}`, after a '\n                'call to `close()`.'.format(type(self).__name__))\n\n    def _raise_if_errors(self, successes):\n        if all(successes):\n            return\n\n        num_errors = self.num_envs - sum(successes)\n        assert num_errors > 0\n        for _ in range(num_errors):\n            index, exctype, value = self.error_queue.get()\n            logger.error('Received the following error from Worker-{0}: '\n                '{1}: {2}'.format(index, exctype.__name__, value))\n            logger.error('Shutting down Worker-{0}.'.format(index))\n            self.parent_pipes[index].close()\n            self.parent_pipes[index] = None\n\n        logger.error('Raising the last exception back to the main process.')\n        raise exctype(value)\n\n\ndef _worker(index, env_fn, pipe, parent_pipe, shared_memory, error_queue):\n    assert shared_memory is None\n    env = env_fn()\n    parent_pipe.close()\n    try:\n        while True:\n            command, data = pipe.recv()\n            if command == 'reset':\n                observation = env.reset()\n                pipe.send((observation, True))\n            elif command == 'step':\n                observation, reward, done, info = env.step(data)\n                if done:\n                    observation = env.reset()\n                pipe.send(((observation, reward, done, info), True))\n            elif command == 'seed':\n                env.seed(data)\n                pipe.send((None, True))\n            elif command == 'close':\n                pipe.send((None, True))\n                break\n            elif command == '_check_observation_space':\n                pipe.send((data == env.observation_space, True))\n            else:\n                raise RuntimeError('Received unknown command `{0}`. Must '\n                    'be one of {`reset`, `step`, `seed`, `close`, '\n                    '`_check_observation_space`}.'.format(command))\n    except (KeyboardInterrupt, Exception):\n        error_queue.put((index,) + sys.exc_info()[:2])\n        pipe.send((None, False))\n    finally:\n        env.close()\n\n\ndef _worker_shared_memory(index, env_fn, pipe, parent_pipe, shared_memory, error_queue):\n    assert shared_memory is not None\n    env = env_fn()\n    observation_space = env.observation_space\n    parent_pipe.close()\n    try:\n        while True:\n            command, data = pipe.recv()\n            if command == 'reset':\n                observation = env.reset()\n                write_to_shared_memory(index, observation, shared_memory,\n                                       observation_space)\n                pipe.send((None, True))\n            elif command == 'step':\n                observation, reward, done, info = env.step(data)\n                if done:\n                    observation = env.reset()\n                write_to_shared_memory(index, observation, shared_memory,\n                                       observation_space)\n                pipe.send(((None, reward, done, info), True))\n            elif command == 'seed':\n                env.seed(data)\n                pipe.send((None, True))\n            elif command == 'close':\n                pipe.send((None, True))\n                break\n            elif command == '_check_observation_space':\n                pipe.send((data == observation_space, True))\n            else:\n                raise RuntimeError('Received unknown command `{0}`. Must '\n                    'be one of {`reset`, `step`, `seed`, `close`, '\n                    '`_check_observation_space`}.'.format(command))\n    except (KeyboardInterrupt, Exception):\n        error_queue.put((index,) + sys.exc_info()[:2])\n        pipe.send((None, False))\n    finally:\n        env.close()"
    ]
};